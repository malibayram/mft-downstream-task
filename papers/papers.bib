@misc{turker_tabibert_2026,
  title      = {{TabiBERT}: {A} {Large}-{Scale} {ModernBERT} {Foundation} {Model} and {A} {Unified} {Benchmark} for {Turkish}},
  shorttitle = {{TabiBERT}},
  url        = {http://arxiv.org/abs/2512.23065},
  doi        = {10.48550/arXiv.2512.23065},
  abstract   = {Since the inception of BERT, encoder-only Transformers have evolved significantly in computational efficiency, training stability, and long-context modeling. ModernBERT consolidates these advances by integrating Rotary Positional Embeddings (RoPE), FlashAttention, and refined normalization. Despite these developments, Turkish NLP lacks a monolingual encoder trained from scratch, incorporating such modern architectural paradigms. This work introduces TabiBERT, a monolingual Turkish encoder based on ModernBERT architecture trained from scratch on a large, curated corpus. TabiBERT is pre-trained on one trillion tokens sampled from an 84.88B token multi-domain corpus: web text (73\%), scientific publications (20\%), source code (6\%), and mathematical content (0.3\%). It supports 8,192-token context length (16x original BERT), achieves up to 2.65x inference speedup, and reduces GPU memory consumption, enabling larger batch sizes. We introduce TabiBench with 28 datasets across eight task categories with standardized splits and protocols, evaluated using GLUE-style macro-averaging. TabiBERT attains 77.58 on TabiBench, outperforming BERTurk by 1.62 points and establishing state-of-the-art on five of eight categories, with particularly strong gains on question answering (+9.55 points), code retrieval (+2.41 points), and academic understanding (+0.66 points). Compared with task-specific prior best results, including specialized models like TurkishBERTweet, TabiBERT achieves +1.47 average improvement, indicating robust cross-domain generalization. We release model weights, training configurations, and evaluation code for transparent, reproducible Turkish encoder research.},
  urldate    = {2026-01-22},
  publisher  = {arXiv},
  author     = {Türker, Melikşah and Kızıloğlu, A. Ebrar and Güngör, Onur and Üsküdarlı, Susan},
  month      = jan,
  year       = {2026},
  note       = {arXiv:2512.23065 [cs]},
  keywords   = {Computer Science - Computation and Language}
}

@misc{asgari_morphbpe_2025,
  title      = {{MorphBPE}: {A} {Morpho}-{Aware} {Tokenizer} {Bridging} {Linguistic} {Complexity} for {Efficient} {LLM} {Training} {Across} {Morphologies}},
  shorttitle = {{MorphBPE}},
  url        = {http://arxiv.org/abs/2502.00894},
  doi        = {10.48550/arXiv.2502.00894},
  abstract   = {Tokenization is fundamental to Natural Language Processing (NLP), directly impacting model efficiency and linguistic fidelity. While Byte Pair Encoding (BPE) is widely used in Large Language Models (LLMs), it often disregards morpheme boundaries, leading to suboptimal segmentation, particularly in morphologically rich languages. We introduce MorphBPE, a morphology-aware extension of BPE that integrates linguistic structure into subword tokenization while preserving statistical efficiency. Additionally, we propose two morphology-based evaluation metrics: (i) Morphological Consistency F1-Score, which quantifies the consistency between morpheme sharing and token sharing, contributing to LLM training convergence, and (ii) Morphological Edit Distance, which measures alignment between morphemes and tokens concerning interpretability. Experiments on English, Russian, Hungarian, and Arabic across 300M and 1B parameter LLMs demonstrate that MorphBPE consistently reduces cross-entropy loss, accelerates convergence, and improves morphological alignment scores. Fully compatible with existing LLM pipelines, MorphBPE requires minimal modifications for integration. The MorphBPE codebase and tokenizer playground will be available at: https://github.com/llm-lab-org/MorphBPE and https://tokenizer.llm-lab.org},
  urldate    = {2026-01-26},
  publisher  = {arXiv},
  author     = {Asgari, Ehsaneddin and Kheir, Yassine El and Javaheri, Mohammad Ali Sadraei},
  month      = feb,
  year       = {2025},
  note       = {arXiv:2502.00894 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence}
}

@misc{rahimov_milli_2025,
  address    = {Rochester, NY},
  type       = {{SSRN} {Scholarly} {Paper}},
  title      = {{miLLi}: {Model} {Integrating} {Local} {Linguistic} {Insights} for {Morphologically} {Robust} {Tokenization}},
  shorttitle = {{miLLi}},
  url        = {https://papers.ssrn.com/abstract=5997834},
  doi        = {10.2139/ssrn.5997834},
  abstract   = {Standard statistical tokenization algorithms often struggle to preserve the morphological boundaries of agglutinative languages such as Azerbaijani. This study introduces miLLi 1.0, a hybrid tokenizer that integrates a rule-based root dictionary with a statistical Byte-Pair Encoding (BPE) approach. The model's distinguishing feature is a dynamic phonological restoration algorithm designed to map allomorphic variations back to their canonical root forms. Empirical evaluations on the Tatoeba corpus demonstrate that miLLi 1.0 (1.955 T/W) outperforms global standards such as GPT-4o and mBERT in terms of representation efficiency. While exhibiting lower token density compared to local statistical models, miLLi 1.0 demonstrates superior linguistic robustness, achieving 53.0\% in Morphological Boundary Accuracy (MBA) and 78.0\% in Root Consistency Rate (RCR). The findings suggest that the integration of a linguistic filtration layer establishes an optimal balance between statistical compression and semantic integrity.},
  language   = {en},
  urldate    = {2026-01-26},
  publisher  = {Social Science Research Network},
  author     = {Rahimov, Elshad},
  month      = dec,
  year       = {2025},
  keywords   = {Tokenization, NLP, Agglutinative Languages, Azerbaijani Language, Morphological Segmentation, Phonological Restoration}
}

@inproceedings{beken_fikri_semantic_2021,
  address   = {Online},
  title     = {Semantic {Similarity} {Based} {Evaluation} for {Abstractive} {News} {Summarization}},
  url       = {https://aclanthology.org/2021.gem-1.3/},
  doi       = {10.18653/v1/2021.gem-1.3},
  abstract  = {ROUGE is a widely used evaluation metric in text summarization. However, it is not suitable for the evaluation of abstractive summarization systems as it relies on lexical overlap between the gold standard and the generated summaries. This limitation becomes more apparent for agglutinative languages with very large vocabularies and high type/token ratios. In this paper, we present semantic similarity models for Turkish and apply them as evaluation metrics for an abstractive summarization task. To achieve this, we translated the English STSb dataset into Turkish and presented the first semantic textual similarity dataset for Turkish as well. We showed that our best similarity models have better alignment with average human judgments compared to ROUGE in both Pearson and Spearman correlations.},
  urldate   = {2026-01-27},
  booktitle = {Proceedings of the {First} {Workshop} on {Natural} {Language} {Generation}, {Evaluation}, and {Metrics} ({GEM})},
  publisher = {Association for Computational Linguistics},
  author    = {Beken Fikri, Figen and Oflazer, Kemal and Yanikoglu, Berrin},
  editor    = {Bosselut, Antoine and Durmus, Esin and Gangal, Varun Prashant and Gehrmann, Sebastian and Jernite, Yacine and Perez-Beltrachini, Laura and Shaikh, Samira and Xu, Wei},
  month     = aug,
  year      = {2021},
  pages     = {24--33}
}


@inproceedings{basar_turblimp_2025,
  address    = {Suzhou, China},
  title      = {{TurBLiMP}: {A} {Turkish} {Benchmark} of {Linguistic} {Minimal} {Pairs}},
  isbn       = {979-8-89176-332-6},
  shorttitle = {{TurBLiMP}},
  url        = {https://aclanthology.org/2025.emnlp-main.834/},
  doi        = {10.18653/v1/2025.emnlp-main.834},
  abstract   = {We introduce TurBLiMP, the first Turkish benchmark of linguistic minimal pairs, designed to evaluate the linguistic abilities of monolingual and multilingual language models (LMs). Covering 16 linguistic phenomena with 1000 minimal pairs each, TurBLiMP fills an important gap in linguistic evaluation resources for Turkish. In designing the benchmark, we give extra attention to two properties of Turkish that remain understudied in current syntactic evaluations of LMs, namely word order flexibility and subordination through morphological processes. Our experiments on a wide range of LMs and a newly collected set of human acceptability judgments reveal that even cutting-edge Large LMs still struggle with grammatical phenomena that are not challenging for humans, and may also exhibit different sensitivities to word order and morphological complexity compared to humans.},
  urldate    = {2026-01-27},
  booktitle  = {Proceedings of the 2025 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  publisher  = {Association for Computational Linguistics},
  author     = {Başar, Ezgi and Padovani, Francesca and Jumelet, Jaap and Bisazza, Arianna},
  editor     = {Christodoulopoulos, Christos and Chakraborty, Tanmoy and Rose, Carolyn and Peng, Violet},
  month      = nov,
  year       = {2025},
  pages      = {16495--16510}
}


