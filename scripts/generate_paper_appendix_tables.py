#!/usr/bin/env python3
from __future__ import annotations

import json
import re
from pathlib import Path


def parse_markdown_table(lines: list[str]) -> tuple[list[str], list[list[str]]]:
    """
    Parse a simple GitHub-flavored Markdown table.
    Returns (headers, rows) with raw cell strings.
    """
    # Find header separator row like | --- | --- |
    sep_idx = None
    for i, line in enumerate(lines):
        if "|" in line and re.search(r"\|\s*:?-{3,}", line):
            sep_idx = i
            break
    if sep_idx is None or sep_idx == 0:
        raise ValueError("Could not find markdown table separator row")

    header_line = lines[sep_idx - 1].strip()
    header_cells = [c.strip() for c in header_line.strip("|").split("|")]

    rows: list[list[str]] = []
    for line in lines[sep_idx + 1 :]:
        line = line.strip()
        if not line.startswith("|"):
            break
        cells = [c.strip() for c in line.strip("|").split("|")]
        if len(cells) != len(header_cells):
            continue
        rows.append(cells)
    return header_cells, rows


def latex_escape(s: str) -> str:
    s = s.replace("\\", "\\textbackslash ")
    s = s.replace("&", "\\&").replace("%", "\\%").replace("_", "\\_")
    s = s.replace("#", "\\#").replace("{", "\\{").replace("}", "\\}")
    s = s.replace("^", "\\textasciicircum ")
    s = s.replace("~", "\\textasciitilde ")
    return s


def strip_md_bold(s: str) -> str:
    # remove **...** while keeping inner text
    return re.sub(r"\*\*(.*?)\*\*", r"\1", s)


def write_sts_tables(root: Path, out: Path) -> None:
    text = (root / "STS_BENCHMARK_RESULTS.md").read_text(encoding="utf-8").splitlines()
    # Grab "All Runs - Test Split" table block
    def extract_block(title: str) -> list[str]:
        start = None
        for i, line in enumerate(text):
            if line.strip() == title:
                start = i
                break
        if start is None:
            raise ValueError(f"Missing section: {title}")
        # table begins after a blank line and header row
        block: list[str] = []
        for line in text[start + 1 :]:
            if line.strip().startswith("|"):
                block.append(line)
            elif block:
                break
        return block

    test_block = extract_block("## All Runs - Test Split")
    train_block = extract_block("## All Runs - Train Split")
    h_test, r_test = parse_markdown_table(test_block)
    h_train, r_train = parse_markdown_table(train_block)

    def rows_to_latex(rows: list[list[str]]) -> list[list[str]]:
        out_rows = []
        for row in rows:
            cleaned = [latex_escape(strip_md_bold(c)) for c in row]
            out_rows.append(cleaned)
        return out_rows

    r_test = rows_to_latex(r_test)
    r_train = rows_to_latex(r_train)
    h_test = [latex_escape(strip_md_bold(c)) for c in h_test]
    h_train = [latex_escape(strip_md_bold(c)) for c in h_train]

    buf: list[str] = []
    buf.append("% Auto-generated by scripts/generate_paper_appendix_tables.py")
    buf.append("\\subsection{STS Benchmark: Full Run Log}")
    buf.append("\\label{sec:appendix_sts_full}")
    buf.append("")
    buf.append("\\small")
    buf.append("\\begin{longtable}{p{2.6cm}p{6.2cm}r rr}")
    buf.append("\\caption{All STS runs on the test split (from \\texttt{STS\\_BENCHMARK\\_RESULTS.md}).}\\\\")
    buf.append("\\toprule")
    buf.append(" & ".join(h_test) + " \\\\")
    buf.append("\\midrule")
    buf.append("\\endfirsthead")
    buf.append("\\toprule")
    buf.append(" & ".join(h_test) + " \\\\")
    buf.append("\\midrule")
    buf.append("\\endhead")
    for row in r_test:
        buf.append(" & ".join(row) + " \\\\")
    buf.append("\\bottomrule")
    buf.append("\\end{longtable}")
    buf.append("\\normalsize")
    buf.append("")
    buf.append("\\small")
    buf.append("\\begin{longtable}{p{2.6cm}p{6.2cm}r rr}")
    buf.append("\\caption{All STS runs on the train split (from \\texttt{STS\\_BENCHMARK\\_RESULTS.md}).}\\\\")
    buf.append("\\toprule")
    buf.append(" & ".join(h_train) + " \\\\")
    buf.append("\\midrule")
    buf.append("\\endfirsthead")
    buf.append("\\toprule")
    buf.append(" & ".join(h_train) + " \\\\")
    buf.append("\\midrule")
    buf.append("\\endhead")
    for row in r_train:
        buf.append(" & ".join(row) + " \\\\")
    buf.append("\\bottomrule")
    buf.append("\\end{longtable}")
    buf.append("\\normalsize")
    buf.append("")
    out.write_text("\n".join(buf) + "\n", encoding="utf-8")


def write_mteb_table(root: Path, out: Path) -> None:
    text = (root / "MTEB_BENCHMARK_RESULTS.md").read_text(encoding="utf-8").splitlines()
    # Extract "Detailed Task Results" first table (starts at header row with | Task | Category | ...)
    start = None
    for i, line in enumerate(text):
        if line.strip().startswith("| Task") and "Category" in line:
            start = i
            break
    if start is None:
        raise ValueError("Could not find MTEB detailed task table")
    block: list[str] = []
    for line in text[start:]:
        if line.strip().startswith("|"):
            block.append(line)
        elif block:
            break
    headers, rows = parse_markdown_table(block)

    # Select a tokenizer-comparison subset (MFT/Tabi + random-init), drop large baseline columns
    keep = {
        "Task": None,
        "Category": None,
        "mft-downstream-task-embeddinggemma": None,
        "mft-downstream-task-embeddingmagibu": None,
        "mft-random-init": None,
        "tabi-downstream-task-embeddinggemma": None,
        "tabi-downstream-task-embeddingmagibu": None,
        "tabi-random-init": None,
    }
    idx_map = {h: i for i, h in enumerate(headers)}
    keep_headers = [h for h in headers if h in keep]
    keep_idxs = [idx_map[h] for h in keep_headers]

    cleaned_rows: list[list[str]] = []
    for row in rows:
        kept = [row[i] for i in keep_idxs]
        kept = [latex_escape(strip_md_bold(c)) for c in kept]
        cleaned_rows.append(kept)

    keep_headers = [latex_escape(strip_md_bold(h)) for h in keep_headers]

    buf: list[str] = []
    buf.append("% Auto-generated by scripts/generate_paper_appendix_tables.py")
    buf.append("\\subsection{MTEB-TR: Per-Task Results}")
    buf.append("\\label{sec:appendix_mteb_tasks}")
    buf.append("")
    buf.append("\\begin{landscape}")
    buf.append("\\scriptsize")
    buf.append("\\begin{longtable}{p{5.2cm}p{2.8cm}rrrrrr}")
    buf.append("\\caption{Per-task MTEB-TR results for tokenizer comparison models (from \\texttt{MTEB\\_BENCHMARK\\_RESULTS.md}).}\\\\")
    buf.append("\\toprule")
    buf.append(" & ".join(keep_headers) + " \\\\")
    buf.append("\\midrule")
    buf.append("\\endfirsthead")
    buf.append("\\toprule")
    buf.append(" & ".join(keep_headers) + " \\\\")
    buf.append("\\midrule")
    buf.append("\\endhead")
    for row in cleaned_rows:
        buf.append(" & ".join(row) + " \\\\")
    buf.append("\\bottomrule")
    buf.append("\\end{longtable}")
    buf.append("\\normalsize")
    buf.append("\\end{landscape}")
    buf.append("")
    out.write_text("\n".join(buf) + "\n", encoding="utf-8")


def write_version_tables(root: Path, out: Path) -> None:
    text = (root / "VERSION_BENCHMARK_RESULTS.md").read_text(encoding="utf-8").splitlines()

    def extract_table_after(header: str) -> tuple[list[str], list[list[str]]]:
        start = None
        for i, line in enumerate(text):
            if line.strip() == header:
                start = i
                break
        if start is None:
            raise ValueError(f"Missing header: {header}")
        block: list[str] = []
        for line in text[start + 1 :]:
            if line.strip().startswith("|"):
                block.append(line)
            elif block:
                break
        return parse_markdown_table(block)

    # Best results per model is a good journal appendix table
    h_best, r_best = extract_table_after("## ðŸŒŸ Best Results per Model")
    h_best = [latex_escape(strip_md_bold(x)) for x in h_best]
    r_best = [[latex_escape(strip_md_bold(c)) for c in row] for row in r_best]

    # Also include detailed per-model histories that are present in the markdown
    model_sections = []
    for i, line in enumerate(text):
        if line.startswith("## ") and "mft-" in line or "## " and "tabi-" in line:
            pass

    def extract_model_history(model_header: str) -> tuple[list[str], list[list[str]]]:
        # model header is like "## mft-downstream-task-embeddinggemma"
        start = None
        for i, line in enumerate(text):
            if line.strip() == model_header:
                start = i
                break
        if start is None:
            raise ValueError(f"Missing model section: {model_header}")
        # first table after it
        block: list[str] = []
        for line in text[start + 1 :]:
            if line.strip().startswith("|"):
                block.append(line)
            elif block:
                break
        return parse_markdown_table(block)

    model_headers = [
        "## mft-downstream-task-embeddinggemma",
        "## mft-downstream-task-embeddingmagibu",
        "## mft-random-init",
        "## tabi-downstream-task-embeddinggemma",
        "## tabi-downstream-task-embeddingmagibu",
        "## tabi-random-init",
    ]
    histories = []
    for mh in model_headers:
        if any(line.strip() == mh for line in text):
            headers, rows = extract_model_history(mh)
            histories.append((mh[3:], headers, rows))

    buf: list[str] = []
    buf.append("% Auto-generated by scripts/generate_paper_appendix_tables.py")
    buf.append("\\subsection{Version History: Detailed Tables}")
    buf.append("\\label{sec:appendix_versions}")
    buf.append("")
    buf.append("\\small")
    buf.append("\\begin{longtable}{p{2.6cm}p{5.6cm}p{2.0cm}rrr}")
    buf.append("\\caption{Best STS results per model across tracked revisions (from \\texttt{VERSION\\_BENCHMARK\\_RESULTS.md}).}\\\\")
    buf.append("\\toprule")
    buf.append(" & ".join(h_best) + " \\\\")
    buf.append("\\midrule")
    buf.append("\\endfirsthead")
    buf.append("\\toprule")
    buf.append(" & ".join(h_best) + " \\\\")
    buf.append("\\midrule")
    buf.append("\\endhead")
    for row in r_best:
        buf.append(" & ".join(row) + " \\\\")
    buf.append("\\bottomrule")
    buf.append("\\end{longtable}")
    buf.append("\\normalsize")
    buf.append("")

    for model_name, headers, rows in histories:
        headers = [latex_escape(strip_md_bold(x)) for x in headers]
        rows = [[latex_escape(strip_md_bold(c)) for c in row] for row in rows]
        buf.append("\\paragraph{%s.}" % latex_escape(model_name))
        buf.append("\\small")
        buf.append("\\begin{longtable}{p{2.6cm}p{2.0cm}rrp{4.6cm}}")
        buf.append("\\toprule")
        buf.append(" & ".join(headers) + " \\\\")
        buf.append("\\midrule")
        buf.append("\\endfirsthead")
        buf.append("\\toprule")
        buf.append(" & ".join(headers) + " \\\\")
        buf.append("\\midrule")
        buf.append("\\endhead")
        for row in rows:
            buf.append(" & ".join(row) + " \\\\")
        buf.append("\\bottomrule")
        buf.append("\\end{longtable}")
        buf.append("\\normalsize")
        buf.append("")

    out.write_text("\n".join(buf) + "\n", encoding="utf-8")


def write_affix_table(root: Path, out: Path) -> None:
    ekler = json.loads((root / "ekler.json").read_text(encoding="utf-8"))
    by_id: dict[int, list[str]] = {}
    for s, i in ekler.items():
        by_id.setdefault(int(i), []).append(s)
    for i in by_id:
        by_id[i] = sorted(by_id[i], key=lambda x: (len(x), x))

    buf: list[str] = []
    buf.append("% Auto-generated by scripts/generate_paper_appendix_tables.py")
    buf.append("\\subsection{Affix Equivalence Classes}")
    buf.append("\\label{sec:appendix_affixes}")
    buf.append("Each affix identifier corresponds to one or more surface allomorphs. Table~\\ref{tab:affix_classes} lists all affix ids and their surface forms from \\texttt{ekler.json}.")
    buf.append("")
    buf.append("\\small")
    buf.append("\\begin{longtable}{rp{12.2cm}}")
    buf.append("\\caption{Affix identifiers and surface allomorph sets (from \\texttt{ekler.json}).}\\\\")
    buf.append("\\label{tab:affix_classes}\\\\")
    buf.append("\\toprule")
    buf.append("Affix ID & Surface forms \\\\")
    buf.append("\\midrule")
    buf.append("\\endfirsthead")
    buf.append("\\toprule")
    buf.append("Affix ID & Surface forms \\\\")
    buf.append("\\midrule")
    buf.append("\\endhead")
    for i in sorted(by_id):
        forms = ", ".join(latex_escape(f) for f in by_id[i])
        buf.append(f"{i} & {forms} \\\\")
    buf.append("\\bottomrule")
    buf.append("\\end{longtable}")
    buf.append("\\normalsize")
    buf.append("")
    out.write_text("\n".join(buf) + "\n", encoding="utf-8")


def write_root_variant_tables(root: Path, out: Path) -> None:
    kokler = json.loads((root / "kokler.json").read_text(encoding="utf-8"))
    by_id: dict[int, list[str]] = {}
    for s, i in kokler.items():
        by_id.setdefault(int(i), []).append(s)

    # distribution of variant set sizes
    dist: dict[int, int] = {}
    multi: list[tuple[int, list[str]]] = []
    for i, forms in by_id.items():
        n = len(forms)
        dist[n] = dist.get(n, 0) + 1
        if n > 1:
            multi.append((i, sorted(forms, key=lambda x: (len(x), x))))

    multi.sort(key=lambda x: (-len(x[1]), x[0]))

    buf: list[str] = []
    buf.append("% Auto-generated by scripts/generate_paper_appendix_tables.py")
    buf.append("\\subsection{Root Variant Sets}")
    buf.append("\\label{sec:appendix_root_variants}")
    buf.append("Some root identifiers correspond to multiple surface strings (e.g., to unify common alternations). This appendix reports the distribution of root variant set sizes and provides representative examples.")
    buf.append("")
    buf.append("\\begin{table}[h]")
    buf.append("\\centering")
    buf.append("\\caption{Distribution of root variant set sizes in \\texttt{kokler.json}.}")
    buf.append("\\label{tab:root_variant_dist}")
    buf.append("\\begin{tabular}{rr}")
    buf.append("\\toprule")
    buf.append("Variant set size & \\# Root IDs \\\\")
    buf.append("\\midrule")
    for n in sorted(dist):
        buf.append(f"{n} & {dist[n]} \\\\")
    buf.append("\\bottomrule")
    buf.append("\\end{tabular}")
    buf.append("\\end{table}")
    buf.append("")
    buf.append("\\small")
    buf.append("\\begin{longtable}{rp{12.2cm}}")
    buf.append("\\caption{Examples of root identifiers with multiple surface strings (top 150 by variant count).}\\\\")
    buf.append("\\label{tab:root_variant_examples}\\\\")
    buf.append("\\toprule")
    buf.append("Root ID & Surface strings \\\\")
    buf.append("\\midrule")
    buf.append("\\endfirsthead")
    buf.append("\\toprule")
    buf.append("Root ID & Surface strings \\\\")
    buf.append("\\midrule")
    buf.append("\\endhead")
    for i, forms in multi[:150]:
        buf.append(f"{i} & {', '.join(latex_escape(f) for f in forms)} \\\\")
    buf.append("\\bottomrule")
    buf.append("\\end{longtable}")
    buf.append("\\normalsize")
    buf.append("")
    out.write_text("\n".join(buf) + "\n", encoding="utf-8")


def write_tokenization_examples(root: Path, out: Path) -> None:
    # Use local tokenizer implementation to generate tokens/ids for illustrative examples.
    import sys

    repo_root = root
    sys.path.insert(0, str(repo_root))
    from turkish_tokenizer import TurkishTokenizer  # type: ignore

    tok = TurkishTokenizer()

    examples = [
        ("Example A", "Kitap okumayÄ± seviyorum.", "book read-NMLZ-ACC like-PROG-1SG", "I like reading books."),
        ("Example B", "Anlayabildiklerimizden misin?", "understand-ABIL-NMLZ-PL-1PL.POSS-ABL Q-2SG", "Are you among those we could understand?"),
        ("Example C", "HTTPServer yapÄ±landÄ±rmasÄ± zor.", "HTTP Server configuration difficult", "The HTTPServer configuration is difficult."),
        ("Example D", "GÃ¶zlÃ¼ÄŸÃ¼mÃ¼n sapÄ± kÄ±rÄ±ldÄ±.", "glasses-1SG.POSS-GEN handle-3SG.POSS break-PST", "The arm of my glasses broke."),
        ("Example E", "OkullarÄ±mÄ±zdan ayrÄ±ldÄ±k.", "school-PL-1PL.POSS-ABL leave-PST-1PL", "We left from our schools."),
        ("Example F", "BayraÄŸÄ± asÄ±yoruz.", "flag-ACC hang-PROG-1PL", "We are hanging the flag."),
        ("Example G", "Ã‡amaÅŸÄ±rhanede Ã§alÄ±ÅŸÄ±yor.", "laundryhouse-LOC work-PROG-3SG", "He/she works in the laundry room."),
        ("Example H", "Kedilerimizden birini gÃ¶rdÃ¼n mÃ¼?", "cat-PL-1PL.POSS-ABL one-ACC see-PST-2SG Q", "Did you see one of our cats?"),
        ("Example I", "Ä°stanbul'dayÄ±z.", "Istanbul-LOC-1PL", "We are in Istanbul."),
        ("Example J", "GÃ¼nÃ¼mÃ¼zde teknolojiler hÄ±zla deÄŸiÅŸiyor.", "today-LOC technology-PL quickly change-PROG", "Today, technologies change rapidly."),
    ]

    def fmt_token_list(tokens: list[str]) -> str:
        # keep it compact; escape TeX
        return latex_escape("[" + ", ".join(tokens) + "]")

    buf: list[str] = []
    buf.append("% Auto-generated by scripts/generate_paper_appendix_tables.py")
    buf.append("\\subsection{Additional Tokenization Examples}")
    buf.append("\\label{sec:appendix_examples}")
    buf.append("Table~\\ref{tab:tok_examples} provides additional qualitative examples, including casing and mixed-script tokens. Gloss lines are approximate and intended for readability.")
    buf.append("")
    buf.append("\\begin{table*}[t]")
    buf.append("\\centering")
    buf.append("\\scriptsize")
    buf.append("\\setlength{\\tabcolsep}{2pt}")
    buf.append("\\renewcommand{\\arraystretch}{1.1}")
    buf.append("\\caption{Additional tokenization examples produced by \\texttt{turkish\\_tokenizer} (tokens shown).}")
    buf.append("\\label{tab:tok_examples}")
    buf.append("\\begin{tabular}{@{}p{1.7cm}p{5.4cm}p{6.2cm}@{}}")
    buf.append("\\toprule")
    buf.append("ID & Input / Gloss & Token output \\\\")
    buf.append("\\midrule")
    for ex_id, sent, gloss, trans in examples:
        tokens = tok.tokenize(sent)
        left = (
            f"\\textit{{{latex_escape(sent)}}}"
            f"\\newline ``{latex_escape(trans)}``"
            f"\\newline \\textsc{{{latex_escape(gloss)}}}"
        )
        buf.append(f"{latex_escape(ex_id)} & {left} & {fmt_token_list(tokens)} \\\\")
    buf.append("\\bottomrule")
    buf.append("\\end{tabular}")
    buf.append("\\end{table*}")
    buf.append("\\normalsize")
    buf.append("")
    out.write_text("\n".join(buf) + "\n", encoding="utf-8")


def main() -> None:
    repo_root = Path(__file__).resolve().parents[1]
    paper_dir = repo_root / "Tokens_with_Meaning__A_Hybrid_Tokenization_Approach_for_NLP"

    write_sts_tables(repo_root, paper_dir / "appendix_sts_full.tex")
    write_mteb_table(repo_root, paper_dir / "appendix_mteb_tasks.tex")
    write_version_tables(repo_root, paper_dir / "appendix_versions.tex")
    write_affix_table(repo_root, paper_dir / "appendix_affixes.tex")
    write_root_variant_tables(repo_root, paper_dir / "appendix_root_variants.tex")
    write_tokenization_examples(repo_root, paper_dir / "appendix_examples.tex")


if __name__ == "__main__":
    main()
