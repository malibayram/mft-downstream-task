{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d992b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gemma3/tokenizer_config.json',\n",
       " 'gemma3/chat_template.jinja',\n",
       " 'gemma3/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "org_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "org_tokenizer.save_pretrained(\"gemma3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1925ba81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4313c97dae44acf9cefbb73a7af312f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_model = AutoModelForCausalLM.from_pretrained(\"google/embeddinggemma-300m\")\n",
    "org_model.save_pretrained(\"gemma3\")\n",
    "source_embeddings = org_model.model.embed_tokens.weight.to(org_model.device)\n",
    "source_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "949caf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c46a32d9da40d4a01668f845171970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32768, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_model = AutoModelForCausalLM.from_pretrained(\"gemma3_cloned\")\n",
    "source_embeddings = org_model.model.embed_tokens.weight.to(org_model.device)\n",
    "source_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e850e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8386d6a346404b25ba16e79f864f1c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3616, 0.2702, 0.3002]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"alibayram/tabi-downstream-task-embeddinggemma\")\n",
    "\n",
    "query = \"Yapay zeka gelecekte önemli olacak\"\n",
    "documents = [\n",
    "    \"Makine öğrenmesi geleceğin teknolojisidir\",\n",
    "    \"Bugün hava çok güzel\",\n",
    "    \"Türkiye'de turizm sektörü büyüyor\"\n",
    "]\n",
    "\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(documents)\n",
    "\n",
    "similarities = model.similarity(query_embedding, doc_embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a7bbe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"org_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(org_tokenizer.get_vocab(), f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b4e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<uppercase>\", \" atasöz\", \"ler\", \"i\", \" geçmiş\", \"ten\", \" günü\", \"m\", \"üz\", \"e\", \" kadar\", \" ulaş\", \"an\", \" anlam\", \"ı\", \" bakım\", \"ın\", \"dan\", \" mecaz\", \"lı\", \" bir\", \" mana\", \" kazan\", \"an\", \" kalıp\", \"laş\", \"mış\", \" sözle\", \"r\", \"dir\", \".\"]\n"
     ]
    }
   ],
   "source": [
    "str_ss = \"['<uppercase>', ' atasöz', 'ler', 'i', ' geçmiş', 'ten', ' günü', 'm', 'üz', 'e', ' kadar', ' ulaş', 'an', ' anlam', 'ı', ' bakım', 'ın', 'dan', ' mecaz', 'lı', ' bir', ' mana', ' kazan', 'an', ' kalıp', 'laş', 'mış', ' sözle', 'r', 'dir', '.']\"\n",
    "\n",
    "print(str_ss.replace(\"'\", '\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797b140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<uppercase>', ' atasöz', 'ler', 'i', ' geçmiş', 'ten', ' günü', 'm', 'üz', 'e', ' kadar', ' ulaş', 'an', ' anlam', 'ı', ' bakım', 'ın', 'dan', ' mecaz', 'lı', ' bir', ' mana', ' kazan', 'an', ' kalıp', 'laş', 'mış', ' sözle', 'r', 'dir', '.']\n",
      "[0, 11237, 20000, 20033, 2961, 20025, 19826, 20039, 20014, 20038, 19483, 2598, 20012, 2558, 20034, 2779, 20021, 20025, 10477, 20018, 2501, 4497, 2584, 20012, 414, 20004, 20016, 2940, 20046, 20028, 20072]\n",
      " atasözlari geçmişden günümüze kadar ulaşen anlamı bakımından mecazlı bir mana kazanan kalıplaşmış sözlerdür.\n"
     ]
    }
   ],
   "source": [
    "# Import the module directly\n",
    "import turkish_tokenizer as tt\n",
    "\n",
    "tokenizer = tt.TurkishTokenizer()\n",
    "\n",
    "text = \"Atasözleri geçmişten günümüze kadar ulaşan anlamı bakımından mecazlı bir mana kazanan kalıplaşmış sözlerdir.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(tokens)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "367c26d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<uppercase>', ' yapay', ' zeka', ' ve', ' makine', ' öğren', 'me', 'si']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Yapay zeka ve makine öğrenmesi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eafc8207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8939cd6457c5473f96eef1146c59634d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3636, 0.3003, 0.1316]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"gemma3_cloned\", custom_tokenizer=tokenizer)\n",
    "\n",
    "query = \"Yapay zeka gelecekte önemli olacak\"\n",
    "documents = [\n",
    "    \"Makine öğrenmesi geleceğin teknolojisidir\",\n",
    "    \"Bugün hava çok güzel\",\n",
    "    \"Türkiye'de turizm sektörü büyüyor\"\n",
    "]\n",
    "\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(documents)\n",
    "\n",
    "similarities = model.similarity(query_embedding, doc_embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4564b921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afbe4b665cd841f5a2252f71da627ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4850, 0.2780, 0.2800]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"magibu_cloned\", custom_tokenizer=tokenizer)\n",
    "\n",
    "query = \"Yapay zeka gelecekte önemli olacak\"\n",
    "documents = [\n",
    "    \"Makine öğrenmesi geleceğin teknolojisidir\",\n",
    "    \"Bugün hava çok güzel\",\n",
    "    \"Türkiye'de turizm sektörü büyüyor\"\n",
    "]\n",
    "\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(documents)\n",
    "\n",
    "similarities = model.similarity(query_embedding, doc_embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7018568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21761, 177)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"kokler.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    kokler = json.load(f)\n",
    "\n",
    "with open(\"ekler.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ekler = json.load(f)\n",
    "\n",
    "len(kokler), len(ekler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5f0b444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88885"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"sorted_freq_cosmos.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sorted_freq_cosmos = json.load(f)\n",
    "\n",
    "len(sorted_freq_cosmos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09efe23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' token', 19529)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_key = list(kokler.keys())[-1]\n",
    "last_id = kokler[last_key]\n",
    "last_key, last_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2918252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, _ in sorted_freq_cosmos.items():\n",
    "  if token.startswith(\" \") and token.islower() and token not in kokler:\n",
    "    last_id += 1\n",
    "    kokler[token] = last_id\n",
    "    \n",
    "    if last_id > 19998:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "305f9817",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"yeni_kokler.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(kokler, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d439053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"1\".islower(), \"1\".isdigit(), \"!\".isidentifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "37ac3a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lır', 20071)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_key = list(ekler.keys())[-1]\n",
    "last_id = ekler[last_key]\n",
    "last_key, last_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98deb365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94647989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12697"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokens = {}\n",
    "for token, _ in sorted_freq_cosmos.items():\n",
    "  token = token.lower()\n",
    "  if len(token) < 4 and token not in kokler and token not in ekler and token not in bpe_tokens:\n",
    "    last_id += 1\n",
    "    bpe_tokens[token] = last_id\n",
    "\n",
    "for token, _ in sorted_freq_cosmos.items():\n",
    "  token = token.lower()\n",
    "  if len(token) < 5 and token not in kokler and token not in ekler and token not in bpe_tokens:\n",
    "    last_id += 1\n",
    "    bpe_tokens[token] = last_id\n",
    "\n",
    "    if last_id >= 2 ** 15:\n",
    "      break\n",
    "\n",
    "\n",
    "len(bpe_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215121f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"yeni_bpe_tokenler.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(bpe_tokens, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfdea16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"At\", \"asöz\", \"leri\", \" geçmişten\", \" günümüze\", \" kadar\", \" ulaşan\", \" anlamı\", \" bakımından\", \" mec\", \"azlı\", \" bir\", \" man\", \"a\", \" kazanan\", \" kalıp\", \"laşmış\", \" söz\", \"lerdir\", \".\"]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alibayram/newmindaiMursit-random-init\")\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tr_chars = {\"Ã¶\": \"ö\", \"Ġ\": \" \", \"Ã¼\": \"ü\", \"'\": '\"', \"Ã§\": \"ç\", \"ÅŁ\": \"ş\", \"Ä±\": \"ı\"}\n",
    "\n",
    "output = str(tokens)\n",
    "for k, v in tr_chars.items():\n",
    "    output = output.replace(k, v)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af520936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"At\", \"as\", \"öz\", \"leri\", \" geçmişten\", \" günümüze\", \" kadar\", \" ulaşan\", \" anlamı\", \" bakımından\", \" mec\", \"az\", \"lı\", \" bir\", \" man\", \"a\", \" kazanan\", \" kalıp\", \"laşmış\", \" söz\", \"lerdir\", \".\"]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"alibayram/cosmosGPT2-random-init\")\n",
    "tokens = tokenizer.tokenize(text)\n",
    "output = str(tokens)\n",
    "for k, v in tr_chars.items():\n",
    "    output = output.replace(k, v)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71f8d4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"A\", \"tasöz\", \"leri \", \"geçmişten \", \"günümüze kadar \", \"ulaşan \", \"anlamı \", \"bakımından \", \"mec\", \"az\", \"lı bir \", \"mana \", \"kazanan \", \"kalıp\", \"laşmış\", \" söz\", \"lerdir\", \".\"]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"alibayram/tabi-random-init\")\n",
    "tokens = tokenizer.tokenize(text)\n",
    "output = str(tokens)\n",
    "for k, v in tr_chars.items():\n",
    "    output = output.replace(k, v)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb73a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"At\", \"asöz\", \"leri\", \" geçmişten\", \" günümüze\", \" kadar\", \" ulaşan\", \" anlamı\", \" bakımından\", \" mec\", \"azlı\", \" bir\", \" man\", \"a\", \" kazanan\", \" kalıp\", \"laşmış\", \" söz\", \"lerdir\", \".\"]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb52d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<uppercase>', ' atasöz', 'ler', 'i', ' geçmiş', 'ten', ' günü', 'm', 'üz', 'e', ' kadar', ' ulaş', 'an', ' anlam', 'ı', ' bakım', 'ın', 'dan', ' mecaz', 'lı', ' bir', ' mana', ' kazan', 'an', ' kalıp', 'laş', 'mış', ' sözle', 'r', 'dir', '.']\n",
      "[0, 11237, 20000, 20033, 2961, 20025, 19826, 20039, 20014, 20038, 19483, 2598, 20012, 2558, 20034, 2779, 20021, 20025, 10477, 20018, 2501, 4497, 2584, 20012, 414, 20004, 20016, 2940, 20046, 20028, 20072]\n",
      " atasözlari geçmişden günümüze kadar ulaşen anlamı bakımından mecazlı bir mana kazanan kalıplaşmış sözlerdür.\n"
     ]
    }
   ],
   "source": [
    "# Import the module directly\n",
    "import turkish_tokenizer as tt\n",
    "\n",
    "tokenizer = tt.TurkishTokenizer()\n",
    "\n",
    "text = \"Atasözleri geçmişten günümüze kadar ulaşan anlamı bakımından mecazlı bir mana kazanan kalıplaşmış sözlerdir.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(tokens)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24f33174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/alibayram/TabiBERT-tokenizer-32k/commit/f18e474d7f6f6f686a0a7dda314cfb64c1d83d28', commit_message='Upload tokenizer', commit_description='', oid='f18e474d7f6f6f686a0a7dda314cfb64c1d83d28', pr_url=None, repo_url=RepoUrl('https://huggingface.co/alibayram/TabiBERT-tokenizer-32k', endpoint='https://huggingface.co', repo_type='model', repo_id='alibayram/TabiBERT-tokenizer-32k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"alibayram/TabiBERT-tokenizer-32k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0acd2ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TabiBERT/tokenizer_config.json', 'TabiBERT/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"TabiBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ea37970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 13576,\n",
       " 1682,\n",
       " 14817,\n",
       " 686,\n",
       " 1355,\n",
       " 25239,\n",
       " 1784,\n",
       " 2519,\n",
       " 22607,\n",
       " 1563,\n",
       " 2357,\n",
       " 599,\n",
       " 8956,\n",
       " 268,\n",
       " 13867,\n",
       " 1811,\n",
       " 7676,\n",
       " 6003,\n",
       " 2007,\n",
       " 268,\n",
       " 1013,\n",
       " 2007,\n",
       " 268,\n",
       " 1013,\n",
       " 2007,\n",
       " 268,\n",
       " 548,\n",
       " 5]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b12ef44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8f7f8592f640cf99c32cb0f60cab36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/537 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d13b80e2384ca18aaaf9c135b9d0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e25b7b91044470b4c4f1002ccdaf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca9f06023f2402ba499f61bb49a7965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3a58b32a8d47f390658abe84adc75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Merhaba', ',', 'ĠnasÄ±l', 'sÄ±nÄ±z', '?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "cosmos_tokenizer = AutoTokenizer.from_pretrained(\"ytu-ce-cosmos/turkish-gpt2-large\")\n",
    "\n",
    "cosmos_tokenizer.tokenize(\"Merhaba, nasılsınız?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2db1bdef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cosmos_tokenizer/tokenizer_config.json', 'cosmos_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosmos_tokenizer.save_pretrained(\"cosmos_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1f17f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./cosmos_tokenizer/tokenizer.json\", \"r\") as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "print(len(tokenizer_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1553b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257, 50000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets remove all tokens from the tokenizer.json file that with id bigger than 2**15 and their respected merges from the merges list.\n",
    "\n",
    "vocab = tokenizer_json[\"model\"][\"vocab\"]\n",
    "merges = tokenizer_json[\"model\"][\"merges\"]\n",
    "\n",
    "len(vocab), len(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a112f493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ar'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(merges[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "050fb752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|> 0\n",
      "orm 1000\n",
      "ana 2000\n",
      "ĠÃ¶deme 3000\n",
      "ĠkapÄ 4000\n",
      "yip 5000\n",
      "ĠoranÄ±nda 6000\n",
      "Ġker 7000\n",
      "ĠÃĸrgÃ¼t 8000\n",
      "ĠDel 9000\n",
      "Ãĸncelikle 10000\n",
      "ĠyapÄ±ya 11000\n",
      "Ġkaydedildi 12000\n",
      "ĠFiyatÄ± 13000\n",
      "wood 14000\n",
      "ĠSekreter 15000\n",
      "ĠboÅŁan 16000\n",
      "ĠsoÄŁutma 17000\n",
      "Ġsuik 18000\n",
      "iyordum 19000\n",
      "ĠgÃ¶rerek 20000\n",
      "ĠaÄŁzÄ± 21000\n",
      "Ġisteniyor 22000\n",
      "Ġimtihan 23000\n",
      "ĠborsasÄ± 24000\n",
      "Ġbilinmiyor 25000\n",
      "ĠhoÅŁuna 26000\n",
      "Ġserum 27000\n",
      "Ġbiliyorsun 28000\n",
      "Ġbitiminde 29000\n",
      "Ġokunur 30000\n",
      "ĠsalgÄ±nÄ±nÄ±n 31000\n",
      "YouTube 32000\n",
      "ĠSantr 33000\n",
      "Alex 34000\n",
      "Ġemirler 35000\n",
      "ĠmeydanÄ 36000\n",
      "Ġcephesi 37000\n",
      "ĠGenetik 38000\n",
      "Ted 39000\n",
      "ĠANLAT 40000\n",
      "ĠYayÄ±nlandÄ± 41000\n",
      "aile 42000\n",
      "Ġdavetliler 43000\n",
      "ĠÅŀekilde 44000\n",
      "Bot 45000\n",
      "ĠkÃ¼mesi 46000\n",
      "ĠRiyad 47000\n",
      "General 48000\n",
      "Ġdershane 49000\n",
      "Ġnispet 50000\n"
     ]
    }
   ],
   "source": [
    "new_vocab = {}\n",
    "for token, id in vocab.items():\n",
    "    if id >= 2 ** 15:\n",
    "        for merge in merges:\n",
    "            merged = \"\".join(merge)\n",
    "            if merged == token:\n",
    "                merges.remove(merge)\n",
    "        if id % 1000 == 0:\n",
    "            print(token, id)\n",
    "        continue\n",
    "    new_vocab[token] = id\n",
    "    if id % 1000 == 0:\n",
    "        print(token, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a878f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json[\"model\"][\"vocab\"] = new_vocab\n",
    "with open(\"./cosmos_tokenizer/tokenizer_n.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tokenizer_json, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0190750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/alibayram/cosmosGPT2-tokenizer-32k/commit/19804c7091b340a69879b9dad326321460914cc9', commit_message='Upload tokenizer', commit_description='', oid='19804c7091b340a69879b9dad326321460914cc9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/alibayram/cosmosGPT2-tokenizer-32k', endpoint='https://huggingface.co', repo_type='model', repo_id='alibayram/cosmosGPT2-tokenizer-32k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cosmos_tokenizer = AutoTokenizer.from_pretrained(\"cosmos_tokenizer\")\n",
    "print(new_cosmos_tokenizer.vocab_size)\n",
    "new_cosmos_tokenizer.push_to_hub(\"alibayram/cosmosGPT2-tokenizer-32k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8196898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d1738fe2a247819e7f2051c399bb54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680c98ed5ff44cf8bd4b4df691108681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdf6409a202426a927f5bbceacf1a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59008\n",
      "['Merhaba', ',', 'ĠnasÄ±l', 'sÄ±nÄ±z', '?']\n",
      "9\n",
      "59008 58747\n",
      "[PAD] 0\n",
      "25 1000\n",
      "ite 2000\n",
      "uyu 3000\n",
      "ĠnumaralÄ± 4000\n",
      "ĠkapÄ± 5000\n",
      "Ġgetiren 6000\n",
      "droid 7000\n",
      "Ġgetirmek 8000\n",
      "prÃ¼ 9000\n",
      ")} 10000\n",
      "ution 11000\n",
      "ĠonarÄ±m 12000\n",
      "ware 13000\n",
      "ĠteÅŁhis 14000\n",
      "ĠbaÅŁlamasÄ± 15000\n",
      "mazon 16000\n",
      "amda 17000\n",
      "Ġbilgisini 18000\n",
      "koz 19000\n",
      "trik 20000\n",
      "ĠolmasÄ±yla 21000\n",
      "ĠkayÄ±tlar 22000\n",
      "Ġrespons 23000\n",
      "tional 24000\n",
      "Ã¼lf 25000\n",
      "ĠfÄ±kranÄ±n 26000\n",
      "ĠgÃ¶rkem 27000\n",
      "ĠMÃ¼cadel 28000\n",
      "ĠhavalandÄ±rma 29000\n",
      "Ġprojesine 30000\n",
      "ĠÅŀirketler 31000\n",
      "istanbul 32000\n",
      "Ä±lap 33000\n",
      ">Ãĸn 34000\n",
      "ĠbozmanÄ±n 35000\n",
      "ĠsÃ¼rebilir 36000\n",
      "Ġelbiseler 37000\n",
      "ĠÄ°shak 38000\n",
      "Ġintibak 39000\n",
      "ĠNaf 40000\n",
      "encefil 41000\n",
      "Ġdecrease 42000\n",
      "omu 43000\n",
      "Ġjest 44000\n",
      "Ġahlaka 45000\n",
      "ĠzararsÄ±z 46000\n",
      "lÃ¼y 47000\n",
      "ĠLite 48000\n",
      "ebilmiÅŁtir 49000\n",
      "ĠApol 50000\n",
      "ĠHukukÃ® 51000\n",
      "Ġcayma 52000\n",
      "Ġedep 53000\n",
      "Ġmahalleler 54000\n",
      "ivot 55000\n",
      "ĠderinliÄŁinde 56000\n",
      "ĠdÃ¼ÅŁÃ¼ndÃ¼ÄŁÃ¼nÃ¼zÃ¼ 57000\n",
      "[/ 58000\n",
      "ĠfÄ±rsattÄ±r 59000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "mursit_tokenizer = AutoTokenizer.from_pretrained(\"newmindai/Mursit-Base-TR-Retrieval\")\n",
    "print(mursit_tokenizer.vocab_size)\n",
    "print(mursit_tokenizer.tokenize(\"Merhaba, nasılsınız?\"))\n",
    "\n",
    "mursit_tokenizer.save_pretrained(\"mursit_tokenizer\")\n",
    "with open(\"./mursit_tokenizer/tokenizer.json\", \"r\") as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "print(len(tokenizer_json))\n",
    "\n",
    "vocab = tokenizer_json[\"model\"][\"vocab\"]\n",
    "merges = tokenizer_json[\"model\"][\"merges\"]\n",
    "\n",
    "print(len(vocab), len(merges))\n",
    "\n",
    "new_vocab = {}\n",
    "for token, id in vocab.items():\n",
    "    if id >= 2**15:\n",
    "        for merge in merges:\n",
    "            merged = \"\".join(merge)\n",
    "            if merged == token:\n",
    "                merges.remove(merge)\n",
    "        if id % 1000 == 0:\n",
    "            print(token, id)\n",
    "        continue\n",
    "    new_vocab[token] = id\n",
    "    if id % 1000 == 0:\n",
    "        print(token, id)\n",
    "\n",
    "tokenizer_json[\"model\"][\"vocab\"] = new_vocab\n",
    "with open(\"./mursit_tokenizer/tokenizer_n.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tokenizer_json, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1946b918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/alibayram/newmindaiMursit-tokenizer-32k/commit/8eecd7036571962c3d818225fcd567c018ccf567', commit_message='Upload tokenizer', commit_description='', oid='8eecd7036571962c3d818225fcd567c018ccf567', pr_url=None, repo_url=RepoUrl('https://huggingface.co/alibayram/newmindaiMursit-tokenizer-32k', endpoint='https://huggingface.co', repo_type='model', repo_id='alibayram/newmindaiMursit-tokenizer-32k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cosmos_tokenizer = AutoTokenizer.from_pretrained(\"mursit_tokenizer\")\n",
    "print(new_cosmos_tokenizer.vocab_size)\n",
    "new_cosmos_tokenizer.push_to_hub(\"alibayram/newmindaiMursit-tokenizer-32k\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
