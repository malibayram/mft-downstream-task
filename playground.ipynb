{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d992b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gemma3/tokenizer_config.json',\n",
       " 'gemma3/chat_template.jinja',\n",
       " 'gemma3/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "org_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "org_tokenizer.save_pretrained(\"gemma3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1925ba81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4313c97dae44acf9cefbb73a7af312f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_model = AutoModelForCausalLM.from_pretrained(\"google/embeddinggemma-300m\")\n",
    "org_model.save_pretrained(\"gemma3\")\n",
    "source_embeddings = org_model.model.embed_tokens.weight.to(org_model.device)\n",
    "source_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "949caf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c46a32d9da40d4a01668f845171970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32768, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_model = AutoModelForCausalLM.from_pretrained(\"gemma3_cloned\")\n",
    "source_embeddings = org_model.model.embed_tokens.weight.to(org_model.device)\n",
    "source_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e850e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8386d6a346404b25ba16e79f864f1c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3616, 0.2702, 0.3002]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"alibayram/tabi-downstream-task-embeddinggemma\")\n",
    "\n",
    "query = \"Yapay zeka gelecekte önemli olacak\"\n",
    "documents = [\n",
    "    \"Makine öğrenmesi geleceğin teknolojisidir\",\n",
    "    \"Bugün hava çok güzel\",\n",
    "    \"Türkiye'de turizm sektörü büyüyor\"\n",
    "]\n",
    "\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(documents)\n",
    "\n",
    "similarities = model.similarity(query_embedding, doc_embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a7bbe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"org_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(org_tokenizer.get_vocab(), f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6797b140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' yapılır', 'ken', ' kır', 'ı', 'lır', 'ken', ' oku', 'yor', ' söylü', 'yor', 'du', ' başla', 'n', 'mış', 'tır', ' bilin', 'di', 'k', '<uppercase>', ' ankara', \"'\", 'yla', ' reng', 'a', ' renk', '<uppercase>', ' türkiye', \"'\", 'nin', '<uppercase>', ' türkiye', \"'\", 'nin', '<uppercase>', ' türkiye', \"'\", 'de']\n",
      "[19767, 20007, 3766, 20034, 20071, 20007, 2656, 20041, 2306, 20041, 20026, 2206, 20040, 20016, 20028, 19736, 20026, 20108, 0, 3399, 20078, 20023, 245, 20037, 245, 0, 4563, 20078, 20022, 0, 4563, 20078, 20022, 0, 4563, 20078, 20024]\n",
      " yapılırkan kırılırkan okuyor söylüyordu başlanmüştür bilindük ankara'la renga renk türkiye'nın türkiye'nın türkiye'da\n"
     ]
    }
   ],
   "source": [
    "# Import the module directly\n",
    "import turkish_tokenizer as tt\n",
    "\n",
    "tokenizer = tt.TurkishTokenizer()\n",
    "\n",
    "text = \"yapılırken kırılırken okuyor söylüyordu başlanmıştır bilindik Ankara'yla renga renk Türkiye'nin Türkiye'nin Türkiye'de\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(tokens)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eafc8207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8939cd6457c5473f96eef1146c59634d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3636, 0.3003, 0.1316]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"gemma3_cloned\", custom_tokenizer=tokenizer)\n",
    "\n",
    "query = \"Yapay zeka gelecekte önemli olacak\"\n",
    "documents = [\n",
    "    \"Makine öğrenmesi geleceğin teknolojisidir\",\n",
    "    \"Bugün hava çok güzel\",\n",
    "    \"Türkiye'de turizm sektörü büyüyor\"\n",
    "]\n",
    "\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(documents)\n",
    "\n",
    "similarities = model.similarity(query_embedding, doc_embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4564b921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afbe4b665cd841f5a2252f71da627ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4850, 0.2780, 0.2800]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"magibu_cloned\", custom_tokenizer=tokenizer)\n",
    "\n",
    "query = \"Yapay zeka gelecekte önemli olacak\"\n",
    "documents = [\n",
    "    \"Makine öğrenmesi geleceğin teknolojisidir\",\n",
    "    \"Bugün hava çok güzel\",\n",
    "    \"Türkiye'de turizm sektörü büyüyor\"\n",
    "]\n",
    "\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(documents)\n",
    "\n",
    "similarities = model.similarity(query_embedding, doc_embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7018568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21761, 177)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"kokler.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    kokler = json.load(f)\n",
    "\n",
    "with open(\"ekler.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ekler = json.load(f)\n",
    "\n",
    "len(kokler), len(ekler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5f0b444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88885"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"sorted_freq_cosmos.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sorted_freq_cosmos = json.load(f)\n",
    "\n",
    "len(sorted_freq_cosmos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09efe23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' token', 19529)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_key = list(kokler.keys())[-1]\n",
    "last_id = kokler[last_key]\n",
    "last_key, last_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2918252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, _ in sorted_freq_cosmos.items():\n",
    "  if token.startswith(\" \") and token.islower() and token not in kokler:\n",
    "    last_id += 1\n",
    "    kokler[token] = last_id\n",
    "    \n",
    "    if last_id > 19998:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "305f9817",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"yeni_kokler.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(kokler, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d439053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"1\".islower(), \"1\".isdigit(), \"!\".isidentifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "37ac3a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lır', 20071)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_key = list(ekler.keys())[-1]\n",
    "last_id = ekler[last_key]\n",
    "last_key, last_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98deb365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94647989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12697"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokens = {}\n",
    "for token, _ in sorted_freq_cosmos.items():\n",
    "  token = token.lower()\n",
    "  if len(token) < 4 and token not in kokler and token not in ekler and token not in bpe_tokens:\n",
    "    last_id += 1\n",
    "    bpe_tokens[token] = last_id\n",
    "\n",
    "for token, _ in sorted_freq_cosmos.items():\n",
    "  token = token.lower()\n",
    "  if len(token) < 5 and token not in kokler and token not in ekler and token not in bpe_tokens:\n",
    "    last_id += 1\n",
    "    bpe_tokens[token] = last_id\n",
    "\n",
    "    if last_id >= 2 ** 15:\n",
    "      break\n",
    "\n",
    "\n",
    "len(bpe_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215121f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"yeni_bpe_tokenler.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(bpe_tokens, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfdea16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TabiBERT\")\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24f33174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/alibayram/TabiBERT-tokenizer-32k/commit/f18e474d7f6f6f686a0a7dda314cfb64c1d83d28', commit_message='Upload tokenizer', commit_description='', oid='f18e474d7f6f6f686a0a7dda314cfb64c1d83d28', pr_url=None, repo_url=RepoUrl('https://huggingface.co/alibayram/TabiBERT-tokenizer-32k', endpoint='https://huggingface.co', repo_type='model', repo_id='alibayram/TabiBERT-tokenizer-32k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"alibayram/TabiBERT-tokenizer-32k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0acd2ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TabiBERT/tokenizer_config.json', 'TabiBERT/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"TabiBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ea37970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 13576,\n",
       " 1682,\n",
       " 14817,\n",
       " 686,\n",
       " 1355,\n",
       " 25239,\n",
       " 1784,\n",
       " 2519,\n",
       " 22607,\n",
       " 1563,\n",
       " 2357,\n",
       " 599,\n",
       " 8956,\n",
       " 268,\n",
       " 13867,\n",
       " 1811,\n",
       " 7676,\n",
       " 6003,\n",
       " 2007,\n",
       " 268,\n",
       " 1013,\n",
       " 2007,\n",
       " 268,\n",
       " 1013,\n",
       " 2007,\n",
       " 268,\n",
       " 548,\n",
       " 5]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
