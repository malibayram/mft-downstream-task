{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d992b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gemma3/tokenizer_config.json',\n",
       " 'gemma3/chat_template.jinja',\n",
       " 'gemma3/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "org_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "org_tokenizer.save_pretrained(\"gemma3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1925ba81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4313c97dae44acf9cefbb73a7af312f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_model = AutoModelForCausalLM.from_pretrained(\"google/embeddinggemma-300m\")\n",
    "org_model.save_pretrained(\"gemma3\")\n",
    "source_embeddings = org_model.model.embed_tokens.weight.to(org_model.device)\n",
    "source_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "949caf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c46a32d9da40d4a01668f845171970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32768, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_model = AutoModelForCausalLM.from_pretrained(\"gemma3_cloned\")\n",
    "source_embeddings = org_model.model.embed_tokens.weight.to(org_model.device)\n",
    "source_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e850e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8386d6a346404b25ba16e79f864f1c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3616, 0.2702, 0.3002]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"alibayram/tabi-downstream-task-embeddinggemma\")\n",
    "\n",
    "query = \"Yapay zeka gelecekte önemli olacak\"\n",
    "documents = [\n",
    "    \"Makine öğrenmesi geleceğin teknolojisidir\",\n",
    "    \"Bugün hava çok güzel\",\n",
    "    \"Türkiye'de turizm sektörü büyüyor\"\n",
    "]\n",
    "\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(documents)\n",
    "\n",
    "similarities = model.similarity(query_embedding, doc_embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a7bbe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"org_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(org_tokenizer.get_vocab(), f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6797b140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' yapılır', 'ken', ' kır', 'ı', 'lır', 'ken', ' oku', 'yor', ' söylü', 'yor', 'du', ' başla', 'n', 'mış', 'tır', ' bilin', 'di', 'k', '<uppercase>', ' ankara', \"'\", 'yla', ' reng', 'a', ' renk', '<uppercase>', ' türkiye', \"'\", 'nin', '<uppercase>', ' türkiye', \"'\", 'nin', '<uppercase>', ' türkiye', \"'\", 'de']\n",
      "[19767, 20007, 3766, 20034, 20071, 20007, 2656, 20041, 2306, 20041, 20026, 2206, 20040, 20016, 20028, 19736, 20026, 20108, 0, 3399, 20078, 20023, 245, 20037, 245, 0, 4563, 20078, 20022, 0, 4563, 20078, 20022, 0, 4563, 20078, 20024]\n",
      " yapılırkan kırılırkan okuyor söylüyordu başlanmüştür bilindük ankara'la renga renk türkiye'nın türkiye'nın türkiye'da\n"
     ]
    }
   ],
   "source": [
    "# Import the module directly\n",
    "import turkish_tokenizer as tt\n",
    "\n",
    "tokenizer = tt.TurkishTokenizer()\n",
    "\n",
    "text = \"yapılırken kırılırken okuyor söylüyordu başlanmıştır bilindik Ankara'yla renga renk Türkiye'nin Türkiye'nin Türkiye'de\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "print(tokens)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "367c26d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<uppercase>', ' yapay', ' zeka', ' ve', ' makine', ' öğren', 'me', 'si']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Yapay zeka ve makine öğrenmesi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eafc8207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8939cd6457c5473f96eef1146c59634d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3636, 0.3003, 0.1316]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"gemma3_cloned\", custom_tokenizer=tokenizer)\n",
    "\n",
    "query = \"Yapay zeka gelecekte önemli olacak\"\n",
    "documents = [\n",
    "    \"Makine öğrenmesi geleceğin teknolojisidir\",\n",
    "    \"Bugün hava çok güzel\",\n",
    "    \"Türkiye'de turizm sektörü büyüyor\"\n",
    "]\n",
    "\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(documents)\n",
    "\n",
    "similarities = model.similarity(query_embedding, doc_embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4564b921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afbe4b665cd841f5a2252f71da627ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4850, 0.2780, 0.2800]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"magibu_cloned\", custom_tokenizer=tokenizer)\n",
    "\n",
    "query = \"Yapay zeka gelecekte önemli olacak\"\n",
    "documents = [\n",
    "    \"Makine öğrenmesi geleceğin teknolojisidir\",\n",
    "    \"Bugün hava çok güzel\",\n",
    "    \"Türkiye'de turizm sektörü büyüyor\"\n",
    "]\n",
    "\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(documents)\n",
    "\n",
    "similarities = model.similarity(query_embedding, doc_embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7018568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21761, 177)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"kokler.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    kokler = json.load(f)\n",
    "\n",
    "with open(\"ekler.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    ekler = json.load(f)\n",
    "\n",
    "len(kokler), len(ekler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5f0b444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88885"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"sorted_freq_cosmos.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sorted_freq_cosmos = json.load(f)\n",
    "\n",
    "len(sorted_freq_cosmos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09efe23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' token', 19529)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_key = list(kokler.keys())[-1]\n",
    "last_id = kokler[last_key]\n",
    "last_key, last_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2918252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, _ in sorted_freq_cosmos.items():\n",
    "  if token.startswith(\" \") and token.islower() and token not in kokler:\n",
    "    last_id += 1\n",
    "    kokler[token] = last_id\n",
    "    \n",
    "    if last_id > 19998:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "305f9817",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"yeni_kokler.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(kokler, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d439053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"1\".islower(), \"1\".isdigit(), \"!\".isidentifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "37ac3a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lır', 20071)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_key = list(ekler.keys())[-1]\n",
    "last_id = ekler[last_key]\n",
    "last_key, last_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98deb365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94647989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12697"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokens = {}\n",
    "for token, _ in sorted_freq_cosmos.items():\n",
    "  token = token.lower()\n",
    "  if len(token) < 4 and token not in kokler and token not in ekler and token not in bpe_tokens:\n",
    "    last_id += 1\n",
    "    bpe_tokens[token] = last_id\n",
    "\n",
    "for token, _ in sorted_freq_cosmos.items():\n",
    "  token = token.lower()\n",
    "  if len(token) < 5 and token not in kokler and token not in ekler and token not in bpe_tokens:\n",
    "    last_id += 1\n",
    "    bpe_tokens[token] = last_id\n",
    "\n",
    "    if last_id >= 2 ** 15:\n",
    "      break\n",
    "\n",
    "\n",
    "len(bpe_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215121f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"yeni_bpe_tokenler.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(bpe_tokens, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfdea16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yap', 'ay', 'Ġzek', 'aĠveĠ', 'makineĠ', 'Ã¶ÄŁren', 'mesi']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alibayram/TabiBERT-tokenizer-32k\")\n",
    "tokenizer.tokenize(\"Yapay zeka ve makine öğrenmesi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24f33174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/alibayram/TabiBERT-tokenizer-32k/commit/f18e474d7f6f6f686a0a7dda314cfb64c1d83d28', commit_message='Upload tokenizer', commit_description='', oid='f18e474d7f6f6f686a0a7dda314cfb64c1d83d28', pr_url=None, repo_url=RepoUrl('https://huggingface.co/alibayram/TabiBERT-tokenizer-32k', endpoint='https://huggingface.co', repo_type='model', repo_id='alibayram/TabiBERT-tokenizer-32k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"alibayram/TabiBERT-tokenizer-32k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0acd2ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TabiBERT/tokenizer_config.json', 'TabiBERT/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"TabiBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ea37970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 13576,\n",
       " 1682,\n",
       " 14817,\n",
       " 686,\n",
       " 1355,\n",
       " 25239,\n",
       " 1784,\n",
       " 2519,\n",
       " 22607,\n",
       " 1563,\n",
       " 2357,\n",
       " 599,\n",
       " 8956,\n",
       " 268,\n",
       " 13867,\n",
       " 1811,\n",
       " 7676,\n",
       " 6003,\n",
       " 2007,\n",
       " 268,\n",
       " 1013,\n",
       " 2007,\n",
       " 268,\n",
       " 1013,\n",
       " 2007,\n",
       " 268,\n",
       " 548,\n",
       " 5]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b12ef44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8f7f8592f640cf99c32cb0f60cab36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/537 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d13b80e2384ca18aaaf9c135b9d0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e25b7b91044470b4c4f1002ccdaf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca9f06023f2402ba499f61bb49a7965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3a58b32a8d47f390658abe84adc75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Merhaba', ',', 'ĠnasÄ±l', 'sÄ±nÄ±z', '?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "cosmos_tokenizer = AutoTokenizer.from_pretrained(\"ytu-ce-cosmos/turkish-gpt2-large\")\n",
    "\n",
    "cosmos_tokenizer.tokenize(\"Merhaba, nasılsınız?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2db1bdef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cosmos_tokenizer/tokenizer_config.json', 'cosmos_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosmos_tokenizer.save_pretrained(\"cosmos_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1f17f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./cosmos_tokenizer/tokenizer.json\", \"r\") as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "print(len(tokenizer_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1553b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257, 50000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets remove all tokens from the tokenizer.json file that with id bigger than 2**15 and their respected merges from the merges list.\n",
    "\n",
    "vocab = tokenizer_json[\"model\"][\"vocab\"]\n",
    "merges = tokenizer_json[\"model\"][\"merges\"]\n",
    "\n",
    "len(vocab), len(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a112f493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ar'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(merges[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "050fb752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|> 0\n",
      "orm 1000\n",
      "ana 2000\n",
      "ĠÃ¶deme 3000\n",
      "ĠkapÄ 4000\n",
      "yip 5000\n",
      "ĠoranÄ±nda 6000\n",
      "Ġker 7000\n",
      "ĠÃĸrgÃ¼t 8000\n",
      "ĠDel 9000\n",
      "Ãĸncelikle 10000\n",
      "ĠyapÄ±ya 11000\n",
      "Ġkaydedildi 12000\n",
      "ĠFiyatÄ± 13000\n",
      "wood 14000\n",
      "ĠSekreter 15000\n",
      "ĠboÅŁan 16000\n",
      "ĠsoÄŁutma 17000\n",
      "Ġsuik 18000\n",
      "iyordum 19000\n",
      "ĠgÃ¶rerek 20000\n",
      "ĠaÄŁzÄ± 21000\n",
      "Ġisteniyor 22000\n",
      "Ġimtihan 23000\n",
      "ĠborsasÄ± 24000\n",
      "Ġbilinmiyor 25000\n",
      "ĠhoÅŁuna 26000\n",
      "Ġserum 27000\n",
      "Ġbiliyorsun 28000\n",
      "Ġbitiminde 29000\n",
      "Ġokunur 30000\n",
      "ĠsalgÄ±nÄ±nÄ±n 31000\n",
      "YouTube 32000\n",
      "ĠSantr 33000\n",
      "Alex 34000\n",
      "Ġemirler 35000\n",
      "ĠmeydanÄ 36000\n",
      "Ġcephesi 37000\n",
      "ĠGenetik 38000\n",
      "Ted 39000\n",
      "ĠANLAT 40000\n",
      "ĠYayÄ±nlandÄ± 41000\n",
      "aile 42000\n",
      "Ġdavetliler 43000\n",
      "ĠÅŀekilde 44000\n",
      "Bot 45000\n",
      "ĠkÃ¼mesi 46000\n",
      "ĠRiyad 47000\n",
      "General 48000\n",
      "Ġdershane 49000\n",
      "Ġnispet 50000\n"
     ]
    }
   ],
   "source": [
    "new_vocab = {}\n",
    "for token, id in vocab.items():\n",
    "    if id >= 2 ** 15:\n",
    "        for merge in merges:\n",
    "            merged = \"\".join(merge)\n",
    "            if merged == token:\n",
    "                merges.remove(merge)\n",
    "        if id % 1000 == 0:\n",
    "            print(token, id)\n",
    "        continue\n",
    "    new_vocab[token] = id\n",
    "    if id % 1000 == 0:\n",
    "        print(token, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a878f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json[\"model\"][\"vocab\"] = new_vocab\n",
    "with open(\"./cosmos_tokenizer/tokenizer_n.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tokenizer_json, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0190750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/alibayram/cosmosGPT2-tokenizer-32k/commit/19804c7091b340a69879b9dad326321460914cc9', commit_message='Upload tokenizer', commit_description='', oid='19804c7091b340a69879b9dad326321460914cc9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/alibayram/cosmosGPT2-tokenizer-32k', endpoint='https://huggingface.co', repo_type='model', repo_id='alibayram/cosmosGPT2-tokenizer-32k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cosmos_tokenizer = AutoTokenizer.from_pretrained(\"cosmos_tokenizer\")\n",
    "print(new_cosmos_tokenizer.vocab_size)\n",
    "new_cosmos_tokenizer.push_to_hub(\"alibayram/cosmosGPT2-tokenizer-32k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8196898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"newmindai/Mursit-Base-TR-Retrieval\"\n",
    "mursit_tokenizer = AutoTokenizer.from_pretrained(\"newmindai/Mursit-Base-TR-Retrieval\")\n",
    "\n",
    "print(mursit_tokenizer.tokenize(\"Merhaba, nasılsınız?\"))\n",
    "\n",
    "mursit_tokenizer.save_pretrained(\"mursit_tokenizer\")\n",
    "with open(\"./mursit_tokenizer/tokenizer.json\", \"r\") as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "print(len(tokenizer_json))\n",
    "\n",
    "vocab = tokenizer_json[\"model\"][\"vocab\"]\n",
    "merges = tokenizer_json[\"model\"][\"merges\"]\n",
    "\n",
    "print(len(vocab), len(merges))\n",
    "\n",
    "new_vocab = {}\n",
    "for token, id in vocab.items():\n",
    "    if id >= 2**15:\n",
    "        for merge in merges:\n",
    "            merged = \"\".join(merge)\n",
    "            if merged == token:\n",
    "                merges.remove(merge)\n",
    "        if id % 1000 == 0:\n",
    "            print(token, id)\n",
    "        continue\n",
    "    new_vocab[token] = id\n",
    "    if id % 1000 == 0:\n",
    "        print(token, id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
