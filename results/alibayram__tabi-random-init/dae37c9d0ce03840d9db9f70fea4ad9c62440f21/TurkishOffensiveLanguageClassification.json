{
  "dataset_revision": "main",
  "task_name": "TurkishOffensiveLanguageClassification",
  "mteb_version": "1.31.3",
  "scores": {
    "test": [
      {
        "accuracy": 0.482162,
        "f1": 0.445252,
        "f1_weighted": 0.526562,
        "ap": 0.208041,
        "ap_weighted": 0.208041,
        "scores_per_experiment": [
          {
            "accuracy": 0.508677,
            "f1": 0.459933,
            "f1_weighted": 0.556545,
            "ap": 0.205934,
            "ap_weighted": 0.205934
          },
          {
            "accuracy": 0.419346,
            "f1": 0.404989,
            "f1_weighted": 0.460023,
            "ap": 0.204374,
            "ap_weighted": 0.204374
          },
          {
            "accuracy": 0.395733,
            "f1": 0.392173,
            "f1_weighted": 0.419871,
            "ap": 0.217288,
            "ap_weighted": 0.217288
          },
          {
            "accuracy": 0.502134,
            "f1": 0.453863,
            "f1_weighted": 0.550543,
            "ap": 0.203484,
            "ap_weighted": 0.203484
          },
          {
            "accuracy": 0.526031,
            "f1": 0.476318,
            "f1_weighted": 0.572394,
            "ap": 0.213194,
            "ap_weighted": 0.213194
          },
          {
            "accuracy": 0.448649,
            "f1": 0.42415,
            "f1_weighted": 0.494874,
            "ap": 0.203852,
            "ap_weighted": 0.203852
          },
          {
            "accuracy": 0.493883,
            "f1": 0.453272,
            "f1_weighted": 0.541999,
            "ap": 0.206899,
            "ap_weighted": 0.206899
          },
          {
            "accuracy": 0.502703,
            "f1": 0.457074,
            "f1_weighted": 0.550794,
            "ap": 0.206088,
            "ap_weighted": 0.206088
          },
          {
            "accuracy": 0.523186,
            "f1": 0.468769,
            "f1_weighted": 0.57001,
            "ap": 0.207473,
            "ap_weighted": 0.207473
          },
          {
            "accuracy": 0.50128,
            "f1": 0.46198,
            "f1_weighted": 0.548565,
            "ap": 0.211818,
            "ap_weighted": 0.211818
          }
        ],
        "main_score": 0.482162,
        "hf_subset": "default",
        "languages": [
          "tur-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 25.035621881484985,
  "kg_co2_emissions": null
}