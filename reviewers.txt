Lacking the downstream task performance is a big missing aspect.
Official Reviewby Reviewer Wx9w21 Jan 2026, 20:29 (modified: 24 Jan 2026, 23:39)Program Chairs, Reviewer Wx9w, AuthorsRevisions
Review:
This paper proposes a linguistically motivated tokenizer for Turkish. They tried to chop a given Turkish word into its morphemes and at the end they reach 85% purity in the tokenizer, i.e., alignment of tokens with the morphemes or roots. It is a big disappointment to see that the downstream task performance is left as future work due to computational constraints. Also, the authors acknowledge that most modern LLMs cover a large number of languages, so it is not clear how the proposed approach would perform on non-Turkish benchmarks.

Rating: 4: Ok but not good enough - rejection
Confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature
Tokens with Meaning: A Hybrid Tokenization Approach for NLP
Official Reviewby Reviewer jLKM14 Jan 2026, 13:10 (modified: 24 Jan 2026, 23:39)Program Chairs, Reviewer jLKM, AuthorsRevisions
Review:
This paper presents a tokenization approach for handling languages like Turkish, with the claim that it is also applicable to other agglutinative languages. The approach involves a linguistically informed hybrid tokenization framework that integrates rule-based morphological analysis with statistical subword segmentation.

The paper is written very clearly and addresses the well-known research gap in the tokenization of morphologically rich languages. It has very few, rather minor issues I list below, and it is a solid contribution to the workshop.

Here are some minor points I would like to encourage the authors to consider, to do justice to language in general and particularly the language they are dealing with.

Firstly, the authors may want to update their title to include the language(s) (or the typological features of the languages) they are dealing with.

Now, let me list other issues with reference to line numbers:

103: The comparison with computer vision is not well-motivated; could the authors expand this, please.

207: Please give morpheme-by-morpheme glosses as well as the meaning of the entire word. Note that there are many cases like this one throughout the rest of the paper. In introducing each example, the authors should consider a linguistically motivated technique to do full justice to their claim that this is a linguistically motivated paper.

The authors may also want to check the Leipzig glossing rules (a standard approach in linguistically motivated papers) while updating the format of their linguistic examples.

226: What is the source of this Turkish web text? Please clarify for transparency.

260, 279: I have the same suggestion as that in line 207 for introducing the example here.

326: One might want to add the expression "vowel or a semivowel" to handle "y" here.

333: I have the same suggestion as that in line 207 for introducing the example here (and possibly the entire section).

376: The "special category" you mention is not clear; please make the necessary extensions. And add glosses for these morphemes. The same comment holds for the rest of the section.

433: The distinct surface forms are not very clear in this example, please consider updating the explanation.

Nice paper!

Rating: 9: Top 15% of accepted papers, strong accept
Confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature

Reviewer Comments:

Reviewer 1
The topic of linguistically informed tokenization has gained considerable attention in recent years,
particularly as large language models are increasingly applied to morphologically rich and low-resource
languages. This paper addresses an important and timely problem by proposing a hybrid tokenization
framework that integrates rule-based morphological analysis with BPE, using Turkish as the primary case
study. The proposed method is conceptually interesting and shows promise, especially in improving
morphological coherence and token-level linguistic alignment. However, the current version of the
manuscript suffers from substantial structural, methodological, and empirical limitations that prevent it
from meeting the standards of a publishable contribution. For these reasons, I recommend rejection in
its present form:
1. The Introduction currently presents detailed quantitative findings, which belong in the Results
section. The opening should instead set up the problem, research questions, and high-level
contributions.
2. Although the text claim support for morphologically rich languages, the study’s experiments and
analysis are confined only to agglutinative systems (primarily Turkish) so the scope must be
adjusted or additional language types (analytical, inflectional) included.
3. The Related Work survey focuses heavily on agglutinative contexts (Turkish, Finnish, Hungarian)
while citing languages such as English and Chinese without specifying their morphological
typology. The manuscript would benefit from explicitly identifying the typological classification of
each mentioned language and incorporating tokenization research on other morphologically rich,
low-resource inflectional languages. For example, a recent study introduces the Slovak
Morphological Tokenizer [https://doi.org/10.7717/peerj-cs.2465], which, similarly to the
approach proposed in this manuscript, preserves the integrity of root morphemes in individual
tokens through a morphology-aware BPE framework.
4. The final paragraph of the Related Work section is problematic, as it shifts away from surveying
prior literature and instead offers a summary and speculative suggestions for future research
directions. Such content is not appropriate in a literature review and would be more suitably
placed in the Conclusion or a dedicated Discussion section.
5. Section 3 (Methodology) begins with background on standard tokenizers rather than immediately
describing the novel pipeline, and lacks clear sub-sectioning (e.g. Dictionary Construction,
Encoding Algorithm, Decoding Process) to guide the reader through each component.
6. The Methodology section states that the root dictionary is constructed from high-frequency
words extracted from large-scale Turkish corpora, yet no specific corpus names or sources are
cited at this point in the text: „The root dictionary is constructed from high-frequency words
extracted from large-scale Turkish corpora“. For transparency and reproducibility, the exact
datasets used for root extraction should be clearly referenced where first mentioned.
7. The structure of the Methodology section is confusing and undermines clarity. The authors begin
by describing the proposed morphological tokenization approach in overly general terms,
including evaluative statements such as “the tokenizer achieves improved performance” and
“the methodology is adaptable to other morphologically complex languages,” despite no results
having yet been presented at that point. These claims are premature and should be reserved for
the Results or Conclusion sections. Following this, the authors return to a more detailed
explanation of the proposed system; however, at the point where the root dictionary is
introduced, the source of the “large-scale corpora” used for extracting high-frequency Turkish 
words is again omitted. Additionally, the process by which high-frequency words were extracted
from these corpora is not described, leaving a significant methodological gap.
8. Without pseudocode or an algorithmic box detailing the hybrid tokenization procedure, the
reader cannot fully understand or reproduce the proposed method; a precise step-by-step
description is essential.
9. The handling of acronyms or runs of uppercase letters (for example word: „HTTPServer“) is not
specified, despite the introduction of an <uppercase> token for single capital letters – this edge
case should be addressed explicitly.
10. The Methodology is presented as one dense narrative; breaking it into titled subsections and
supplementing with a diagram or table of core parameters would greatly improve readability.
11. The manuscript repeatedly uses the full term „Byte Pair Encoding“ even after introducing the
abbreviation „BPE“. Once the abbreviation is defined, the full phrase should be avoided in
subsequent occurrences to maintain clarity and conciseness.
12. The Methodology section inappropriately includes remarks about potential future applications of
the proposed framework, which should be reserved for the Conclusion or Discussion section.
13. Table 1 reports metrics only for the proposed tokenizer; a consolidated comparison table
including all baseline tokenizers is needed to transparently demonstrate relative performance.
Furthermore, it is unclear whether the selected tokenizers are fully comparable, as many of them
were not primarily trained on Turkish data. The authors should justify their choice of baselines
and consider including tokenizers from Turkish-specific language models to ensure a fair and
meaningful comparison.
14. The manuscript relies on reported correlations between TR %/Pure % and MMLU performance
but does not include any direct evaluation on downstream tasks. To demonstrate the practical
value of the proposed tokenization approach, the authors should pretrain a language model using
their tokenizer and fine-tune it on at least one real-world downstream task. Without such
evaluation, the claimed improvements remain purely theoretical.
15. The proposed tokenizer generates substantially more tokens (707 k vs. 434 k for aya-expanse).
The impact on model inference speed, memory usage, and overall system efficiency is not
analyzed. This trade-off should be characterized, or at least noted as a limitation.
16. All experiments target Turkish, yet the paper claims language independence; the authors should
either present preliminary results on another language or clearly frame cross-linguistic
applicability as future work and acknowledge it as a limitation.
17. The approach relies on a manually curated root dictionary (app. 22 000 Turkish roots). The
authors should discuss the practical challenges of obtaining comparable resources for other
languages and include this in the study’s limitations.
Reviewer 2
This paper proposes a tokenization approach for morphologically
complex languages such as  Finnish, Hungarian, Turkish and the like,
in which  instead of being based on e.g. BPE or WordPieces methods,
tokens are based on morphological structure with additional
morphophonological abstractions to further reduce vocabulary size.
Intuitively the motivation is compelling. The authors right at the
outset claim that  "Evaluation on the TR-MMLU
benchmark-a large-scale, Turkish-specific NLP benchmark-demonstrates that
the proposed tokenizer achieves the highest Turkish Token Percentage (90.29%)
and Pure Token Percentage (85.8%) among all tested models." which
leads to some excitement and a heightened expectations -- which
unfortunately are never to come!.

Maybe I am missing something but when one suggests a such a tokenization algorithm, one should show its
efficacy by showing how/if it improves the actual results in the tasks
of the benchmark. If the tasks show statistically significant gains
compared to competing tokenization methods while keeping other
variations (e.g. training data, training protocol etc.), one would
then believe that the tokenization proposed is superior.  But this is
a rather hard task.  One would need to pretrain LLMs using vocabulary
selected by the alternate tokenizer, compute embeddings for the
entries in the tokenization dictionary, and then run through the tasks
and compare.   As far as I can tell, the %'s provided and mentioned
above do not necessarily point to improvements in "performance of downstream tasks such as question answering, sentiment analysis,
and machine translation" although the authors indicate they
correlate.  Well, I would like to see the causal relationship for
this.

To be fair, the authors actually propose a so-called hybrid algorithm
"combining dictionary-based morphological analysis with Byte
Pair Encoding. While morphological segmentation ensures alignment with linguistic
units, BPE provides fallback coverage for unknown words, maintaining efficiency and
scalability in large corpora." So the hybrid component is actually a fall-back
unit.

There are numerous other issues with the proposed approach:

1) These days LLMs are almost all multi-lingual, being trained on
data  literally  from 10s, if not hundreds of languages. While I
sympathize with the authors' motivation that linguistically informed
tokenization may be better for a single language, it is not clear how
that can be applied in such a massive-multilingual case even with some
hybrid support. Now you can
certainly train a monolingual LLM for a specific language and use
tokenizer like the one proposed, but you will be doing away with all
the other useful functionality brought in by multilinguality --
e.g. machine translation, and the like.

2) The Turkish-specific tokenization approach again seems to be a
reasonably method to use until you realize that it is a bit
haphazard.  First there is the issue of phonological abstractions:
Once the method identifies morphemes with
 open vowels that differ in the front-back dimension {a, e}, then
 the morphemes (e.g. -ler/-lar)  can be abstracted into  e.g., -lAr contributing to
 the reduction in vocabulary! There is also some discussion how
 consonant assimilation cases can be handled. But I do not see any
 discussion of the vowels {ı, i, u, ü}  and there are many morphemes
 with such alternations.  In fact,  the example tokenizations on page 13
 are NOT displaying these abstract tokens so that is very confusing.

3) Further, there are many other consonant phenomena that are not even
mentioned;  e.g., k/ğ alternations, consonant assimilations across
morpheme boundaries, consonant gemination in words borrowed from
mostly Arabic (e.g. ,  üssü, tıbbı). and the like. There are also many
exceptions to harmony based on *written* forms dues to mostly
palatalization of preceding consonants.

4) What is also not clear is whether  the morpheme segmentations are on
the surface form of the morphological structure (which I suspect the
authors are doing) or over the lexical morpheme structure. Surface
segmentations which are the same may actually correspond to different
lexical segmentations (e.g., evinde would have two different lexical
segmentations when used in "onun evinde" vs "senin evinde").
Furthermore, the assumption of the longest matching root is also iffy
if you are going to be aligned to the linguistic structure. For
example, on page 13 why is okuma a token as clearly it is a derived
nominal from a verb with a potential structure oku+mA

At this point I am nitpicking but the point is  if you are going to be  linguistically
informed you should go most of the way in.  I am also not going to
comment too much on the generation side of things, where things can
get rather tricky.


5) While the presentation is quite clear, there is one very bothersome
convention that the authors use, citations as if they are sentence
constituents; e.g.,

--"The morphological tokenizer introduced by [8] outperformed"   -- should
be something like "The morphological tokenizer introduced by First-Author et al., [8], outperformed.."

-- " [17] demonstrated that morphology-aware.." -- should be like
  " First-Author et al., [17],  demonstrated that morphology-aware..."

This is really bad style.

6) In the references, please find the actually refereed and published
versions of arxiv entries.  You should use consistent word cases in
paper titles; sometime all content words are capital initial (e.g.,
[19]) sometime not (e.g., [16])

Reviwer 3
Key Results: 
The manuscript proposes a 3-stage rule-based tokenization system to improve morphological segmentation for the Turkish language. The authors benchmark their system on the TR-MMLU dataset and compare with existing tokenizers of gemma-3, llama-3.2, qwen-2.5, phi-4, gpt-4o and aya-expense. The system reports significant improvements in Turkish Token Percentage (TR%) and Pure Token Percentage (Pure%) metrics, and includes qualitative examples illustrating over-segmentation and morphological boundary violations in baseline tokenizers.
Clarity and Context:
The abstract and introduction are well-written, clearly contextualizing the challenges of tokenization in agglutinative languages and motivating the need for morphological awareness. The paper effectively highlights limitations in standard BPE/WordPiece approaches, particularly their tendency to ignore morphological boundaries, and justifies the inclusion of whitespace and special character handling in tokenization.
Validity: 
While the proposed approach is relevant and the TR% and Pure% results are promising, several issues affect validity:
	•	Incomplete metrics: The reported results omit processing time and downstream task performance. Claims of Rust-based speed improvements are unsupported, and there is no evidence of impact on tasks such as MMLU performance, despite prior related work (arXiv:2502.07057) by the same authors including such measures.
	•	Limited benchmarks: Comparisons are restricted to general-purpose tokenizers trained largely on English-heavy corpora. No evaluation against specialized open-source Turkish tokenizers (e.g., Orbina, TurkCell) from the OpenTurkishLLM Leaderboard is presented.
	•	Metric dependency: TR% and Pure% may reflect properties of Turkish morphology and the TR-MMLU dataset rather than true tokenizer quality. For languages where many words are single morphemes, these metrics would be inflated.
These gaps weaken claims about generalizability and real-world applicability.
Originality and Significance: 
The paper's overall segmentation algorithm, while original, doesn't present any unique idea or approach. For instance, space, punctuation, case, and unknown‑token handling are presented as contributions, but these are common in modern tokenizers. The addition of "uppercase token" to differentiate capitalized words is also explored previously here: https://ceur-ws.org/Vol-2829/paper2.pdf. Similarly, the idea of adding suffixes is also explored here: https://arxiv.org/abs/2307.07262v2 
Furthermore, some design choices raise concerns:
	•	Treating frequent compounds (e.g., akarsu, çamaşırhane) as single tokens contradicts the paper’s stated goal of morpheme preservation and likely inflates Pure%.
	•	The reported improvements could be artifacts of dataset composition rather than true algorithmic advancement.
Data and Methodology: 
The TR-MMLU benchmark is a strong dataset contribution, but more examples demonstrating qualitative tokenization results would help illustrate improvements. I encourage the authors to share more examples from the data highlighting the qualitative results.
The methodology is verbose and repetitive. Tokenization and decoding processes are described multiple times, diluting clarity. For instance, the paper first lists down the method from word segmentation to affix identification to BPE and root word analysis. Then the paper lists down decoding strategy and then follows it up with root word analysis, affix identification and BPE integration followed by encoding again. It may be suitable to break down the paper into major sections for readability. 
The decoding pipeline is underspecified: how affixes recombine, and the rules for vowel deletion, consonant softening, and contraction, are left ambiguous. Parameter choices (e.g., 230 affixes, 10k BPE vocab size) are not justified or analyzed through ablations.
Conclusions: 
While results on TR% and Pure% are robust, claims about downstream NLP benefits are speculative without supporting metrics (accuracy, perplexity, or MMLU).
Furthermore, the Turkish Token Percentage (TR%) and Pure Token Percentage (Pure%) metrics could be language dependent. The previous works by the same authors have explored them only in Turkish. Testing on other agglutinative languages (e.g., Finnish) or including downstream evaluations would strengthen generalizability and credibility. 
References: 
The authors provide ample references to existing works, and are well aware of the problems in existing methodologies. The paper omits comparison to current open-source Turkish tokenizers. Including these comparisons would provide stronger empirical grounding.
Suggested Improvements:
	•	Report efficiency metrics: Include processing time (s) for all compared tokenizers to validate Rust speed claims.
	•	Add downstream evaluation: Report MMLU or other accuracy-based metrics, as done in prior related work (arXiv:2502.07057), to demonstrate real-world performance benefits.
	•	Benchmark against open-source Turkish tokenizers: Compare to Orbina, TurkCell, and other models from the OpenTurkishLLM Leaderboard.
	•	Clarify TR%/Pure% dependence: Test on another agglutinative language (e.g., Finnish) or multiple Turkish corpora to validate metric robustness.
	•	Revisit compound treatment: Justify or remove compounds as single tokens and re-run TR%/Pure% without this design choice.
	•	Streamline methodology: Consolidate into one algorithm box with encode/decode pseudocode and worked examples.
	•	Detail decoding strategy: Explicitly define rules for affix recombination, vowel deletion, consonantization, and contraction.
	•	Provide pipeline examples: Include tokenization outputs for sample Turkish words showing segmentation, affix mapping, and decoding.
	•	Expand result tables: Add token counts, vocab size, TR%, Pure%, processing time, and error breakdowns for each tokenizer.
	•	Contextualize novelty: Explicitly compare your contributions to related work (uppercase token, affix handling) and highlight distinct differences.
Reviewer 4:
Review: Tokens with Meaning: A Hybrid Tokenization Approach for NLP
This paper tackles an important issue: how tokenizers may be poorly suited to less represented
languages in NLP. This paper proposes a novel tokenization method that the authors argue
better represents Turkish. However, I raise issues with the paper itself and the details of the
work. I think overall the work needs significant revision and expansion of the experiments in
order to be published. Based on the scope established in the paper, I believe a revised and
significantly shortened version of the work would be more appropriately published as a short
paper (4 pages) at an ACL venue. Therefore, I recommend a reject decision for this manuscript.
This paper is written in a way that makes it difficult to evaluate the work. The introduction is too
long. The introduction is unclear, contains some inaccuracies, and is repetitive. Material that
should be in the introduction or related work sections is in the methodology section.
● Inaccurate representation of the literature:
○ In the related work section, the authors motivate the work with a small number of
papers that argue that tokenization is important for morphologically rich
languages (top of p. 5), but fail to cite any of the work that argues the opposite.
See [1, 2, 3] and citations within.
○ Similarly, the authors claim that “morphologically rich languages such as Turkish,
Finnish, and Hungarian are frequently segmented in ways that violate morphemic
boundaries”. There is work, e.g. [1], which empirically shows the exact opposite.
The authors should better represent the work that contradicts the claims in the
paper.
○ Again, for the claim “While experimental results consistently show that
morphology-aware tokenization improves efficiency and accuracy, most
large-scale language models still rely on traditional subword segmentation
methods.”, this is not definitively true. See discussion in [2].
○ Citation 25 is a LinkedIn post, which I don’t think represents rigorous enough
scientific evidence to support the claim in the third to last paragraph on page 6.
○ The authors should also better introduce and summarize related work. For
example, in “As shown in [23], even state-of-the-art LLMs struggle with
compositional morphology”, the authors should explain what the paper tested
before summarizing the findings. Otherwise, you can only follow paragraphs like
this by reading the paper cited here.
● Repetition:
○ The point about 2.5 times token length is repeated on pages 5 and 6 in nearly
identical contexts.
○ The second-to-last paragraph on p. 9 is repetitive with respect to UNK tokens.
● Important terms are not defined:
○ On p. 3 when you introduce TR% and Pure%, you need to define them. Similarly,
if you’re going to use these terms in the abstract, they need to be introduced and
defined, if only briefly.
○ Token purity is not defined at any point in the paper.
○ In “Their analysis found that performance declines sharply as morphological
complexity increases”, what is morphological complexity (within/across
languages)? What is performance?
○ Compound words are introduced on p. 12, which have not been mentioned or
defined up to that point. Do you treat them as root-root? How do they fit into the
rest of the work?
○ Insufficient and possibly misleading explanations of final devoicing/haplology, and
vowel hiatus. Final devoicing implies that the underlying form is voiced and when
the form occurs without a vowel it is de-voiced, which is the opposite direction to
what is shown.
● Structure and organization:
○ In the methodology section, the authors make claims about the performance of
the tokenizer, but at that point in the paper, the results have not been introduced.
Methodologically, I highlight some concerns here. In general, there is insufficient empirical
evidence to support the authors’ claims and the evaluation methodology is poorly motivated.
● Token granularity (fertility) shouldn’t be compared across languages. Turkish words are
longer, therefore, it is not surprising that there would be more tokens given a fixed vocab.
Instead, the authors should use the method from [4] or similar.
● The authors also mention that this approach might improve summarization quality, but
then don’t check that? Also, NER and sentiment analysis. Not mentioned is that some of
these models were small bidirectional models. Do we expect all of those results to
generalize to (larger) autoregressive models?
● I think if you’re going to make an argument about linguistic representations, you should
evaluate on TurBLiMP [5] (or at least MultiBLiMP [6] which was released at the time of
submission). In order to make this claim, the authors need to evaluate models on this
benchmark, not tokenizers.
● “The results presented in this section provide strong empirical support”. There is no
empirical analysis. The authors provide only a qualitative analysis of a small number of
examples. This analysis is vaguely described and does not sufficiently support the
claims.
● “The proposed framework provides a balance between linguistic integrity and
computational efficiency.” The authors do not evaluate either of these.
Clarification questions:
● Is the proposed tokenization method lossless?
● What is the internal composition of ”akarsu” and ”¸cama¸sırhane”. What do these words
mean?
○ These examples use closing quotation marks at the beginning. Also, the other
Turkish words are italicized, and these are inequities. You should use consistent
formatting for clarity.
● The SentencePiece library supports Unigram and BPE tokenization. Which did you use?
● What does “incorporated into the tokenizer” mean. Is this vocabulary expansion.
● Does this multi-step process slow things down?
[1] https://aclanthology.org/2025.coling-main.441/
[2] https://openreview.net/forum?id=XYRri1s6pP&noteId=XYRri1s6pP
[3] https://aclanthology.org/2024.sigmorphon-1.4/
[4]
https://proceedings.neurips.cc/paper_files/paper/2023/file/74bb24dca8334adce292883b4b651e
da-Paper-Conference.pdf
[5] https://arxiv.org/pdf/2506.13487
[6] https://arxiv.org/pdf/2504.02768?
