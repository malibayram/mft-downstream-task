# Review of "Tokens with Meaning: A Hybrid Tokenization Approach for Turkish" from NeurIPS 2026

## Summary

This paper proposes a linguistically informed, morphology-first tokenizer for Turkish (MFT) that combines dictionary-driven root/affix segmentation with phonological normalization and a BPE fallback. The authors evaluate tokenization quality using TR % and Pure % on TR-MMLU and conduct controlled downstream experiments with sentence-embedding models trained from random initialization to isolate tokenizer inductive bias, reporting substantial gains on STSb-TR, MTEB-TR, and a TurBLiMP-inspired proxy. The work argues that morpheme-aligned segmentation yields more coherent representations for morphologically rich languages and provides released resources and a decoder for lossless reconstruction.

## Strengths

### Technical novelty and innovation

- A practical hybrid pipeline that integrates linguistic resources (root/affix lexicons, phonological normalization) with a BPE fallback, aiming for lossless decoding.
- Explicit handling of capitalization and whitespace via special tokens without duplicating vocabulary entries.
- A decoding module that applies phonological rules to reconstruct surface forms, addressing a common weakness in morphology-aware tokenizers.

### Experimental rigor and validation

- Use of language-aware tokenization metrics (TR %, Pure %) derived from prior work to quantify morpheme alignment on a vetted Turkish benchmark (TR-MMLU).
- A controlled downstream setup where encoder architectures and training objectives are held constant and only tokenization differs, helping attribute differences to tokenizer inductive bias.
- Reported improvements on multiple embedding benchmarks (STS, MTEB-TR) and a linguistic minimal-pair proxy (TurBLiMP) under random initialization.

### Clarity of presentation

- Clear high-level algorithmic description (preprocessing, morphology-first segmentation, BPE fallback) and illustrative examples with complex Turkish morphology.
- Reproducibility signals via HF dataset/model identifiers and mention of released code/resources.

### Significance of contributions

- Addresses a well-motivated and impactful problem: tokenization for agglutinative, morphologically rich languages where mainstream subword tokenizers underperform linguistically.
- The reported downstream effects suggest practical utility beyond diagnostic tokenization metrics, potentially influencing tokenizer design for other MRLs.

## Weaknesses

### Technical limitations or concerns

- Ambiguities/inconsistencies in reported resource sizes (e.g., 20k vs. 22k roots; 72 affix identifiers vs. ~230 morphemes), and limited detail on the morphological analyzer’s accuracy and ambiguity resolution.
- Prompt to AI: Get details from files and fix numbers inside the paper. Count of roots is 20K and for affixes and BPE tokens check their json files.
- No quantitative round-trip evaluation of the “lossless” decoder (exact-match reconstruction rate, error classes), nor a segmentation accuracy benchmark against gold morphological annotations.
- Prompt to AI: Check our files and prepare a detailed answer for this. But do not change the paper. Just prepare the answer as a markdown file.
- Greedy longest-prefix matching is plausible but may mis-segment in ambiguous cases; no analysis of failure modes or disambiguation strategies.
- Prompt to AI: Check our files and prepare a detailed answer for this. But do not change the paper. Just prepare the answer as a markdown file.

### Experimental gaps or methodological issues

- TR % and Pure % appear to be computed “under our lexical resources” in places; it is unclear whether an external, independent morphological validator (e.g., ITU/Kalbur as in [2502.07057]) was consistently used for fair comparisons across tokenizers.
- Prompt to AI: TR and Pure are detailed in \citet{bayram_tokenization_2025}. (https://arxiv.org/abs/2502.07057 you may get more details and put in this paper)
- Downstream comparisons use a single architecture and a relatively small/short training schedule; results are reported mainly for one seed and do not quantify training-run variance.
- Answer: Since 3 different models compared with the MFT show similar trends, we believe that the results are not due to chance.
- The teacher model for embedding distillation is not clearly specified, making reproducibility and interpretation of ceiling effects harder.
- Answer: The teacher model is google/embeddinggemma-300m.

### Clarity or presentation issues

- Some formatting artifacts and numerical inconsistencies (root/affix counts; subword vocabulary sizes) reduce precision and raise questions about exact configurations used.
- Prompt to AI: Fix those numbers in the paper. Get details from files and fix numbers inside the paper. Count of roots is 20K and for affixes and BPE tokens check their json files.
- Limited reporting on computational trade-offs (tokens-per-word, average sequence length, training speed/memory vs. baselines), despite acknowledging efficiency importance.
- Prompt to AI: Check our files and prepare a detailed answer for this. But do not change the paper. Just prepare the answer as a markdown file.

### Missing related work or comparisons

- No direct empirical comparisons with morphology-aware tokenizers such as MorphBPE, MorphPiece, or miLLi, which are close in spirit to the proposed approach.
- Prompt to AI: Since they are not Turkish that makes them not directly comparable. We can add a small note about them.
- Lack of ablations isolating contributions of components (phonological normalization, merged affix IDs, uppercase token, BPE vocabulary size), making it difficult to attribute gains.
- Prompt to AI: Check our files and prepare a detailed answer for this. You can get answers by counting the number of tokens in the files. And the last id that is 32768.

## Detailed Comments

### Technical soundness evaluation

- The hybrid design is sensible and aligns with linguistic theory for Turkish; mapping allomorphs to shared IDs and restoring them via decoding is a pragmatic approach. However, the paper would benefit from:
  - A round-trip reconstruction evaluation (original text → tokens → decoder → text) to substantiate “lossless” claims.
  - A morphological segmentation quality evaluation against annotated data or a trusted analyzer to quantify precision/recall on morpheme boundaries.
  - A failure analysis for ambiguous roots and derivational chains, including backoff heuristics and error categories where the greedy strategy fails.

### Experimental evaluation assessment

- Tokenization metrics: Using TR % and Pure % is appropriate and grounded in prior work. But the paper should clarify whether validation used independent analyzers (as in [2502.07057]) versus the tokenizer’s own dictionaries; the former is preferable to avoid circularity.
- Downstream setup: Holding architecture and context fixed is a good control. Nonetheless:
  - Training budget seems modest; more epochs/seeds and reporting of mean±std would strengthen claims about robustness and effect sizes.
  - The distillation teacher should be documented (model name, domain), and sensitivity to teacher choice considered.
  - Efficiency metrics (tokens per sentence, training speed, memory) should be reported to contextualize the higher total token count and its computational cost.
  - The TurBLiMP proxy (similarity invariance between minimal pairs) is an interesting diagnostic but does not directly measure grammaticality; this is appropriately acknowledged. A complementary generative or classification-based probe would be valuable.

### Comparison with related work (using the summaries provided)

- The work builds on [2502.07057], which introduced TR % and Pure % and showed correlations with downstream Turkish performance; this paper applies those diagnostics and adds controlled downstream evaluations, strengthening the practical case.
- The approach is closely related to the “Tokens with Meaning” preprint [2508.14292]; relative to that, this submission adds downstream benchmarks (STS, MTEB-TR, TurBLiMP proxy) and claims of lossless decoding. However, the methodology appears substantially overlapping; the paper should clarify what is new beyond the preprint (e.g., improved decoder, expanded resources, ablations).
- Beyond general-purpose tokenizers, omission of direct comparisons to morphology-aware tokenizers (MorphBPE, MorphPiece, miLLi) is a notable gap given their conceptual proximity. Including these would position the work more precisely.
- Insights from Kurdish tokenization research [2511.14696] reinforce the coverage and evaluation caveats; adopting coverage-aware protocols and reporting coverage/sequence-length impacts would strengthen the evaluation.
- The Turkish LLM study [2405.04685] underscores the importance of tokenizer effects on compression and training efficiency; adding bits-per-character or tokens-per-char statistics would help contextualize MFT’s compute implications.

### Discussion of broader impact and significance

- The paper targets an important, persistent pain point for morphologically rich languages. If the reported gains generalize, the approach could inform tokenizers for other agglutinative languages. The released resources are valuable for the Turkish NLP community.
- However, increases in token counts and potential compute overhead deserve a clearer efficiency discussion to guide adoption in large-scale training. Broader multilingual applicability requires demonstrating transfer to at least one additional MRL.

## Questions for Authors

1. How exactly were TR % and Pure % computed—did you use external analyzers (e.g., ITU/Kalbur as in [2502.07057]) or your own lexical resources? If the latter, how do you mitigate bias toward your tokenizer?
2. Can you report a quantitative round-trip reconstruction accuracy for the decoder (exact string match on a held-out corpus) and provide an error breakdown?
3. What is the teacher model used for embedding distillation, and how sensitive are results to teacher choice and training budget (epochs, batch size)?
4. Can you provide tokens-per-word (or per character) and speed/memory metrics during training/inference to quantify the compute trade-offs relative to the baselines?
5. How does MFT compare empirically against morphology-aware baselines like MorphBPE, MorphPiece, or miLLi in TR %, Pure %, and downstream tasks?
6. Please clarify the apparent inconsistencies in counts (e.g., 20k vs. 22k roots; 72 affix IDs vs. ~230 morphemes; 12,696 vs. ~10k subwords) and specify the final configuration used in each experiment.
7. Did you run multiple seeds for the downstream training to estimate variance? If not, can you add at least 3 seeds and report mean±std for the main metrics?
