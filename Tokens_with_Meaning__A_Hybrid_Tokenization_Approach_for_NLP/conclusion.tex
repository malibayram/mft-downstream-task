\section{Conclusion}
\label{sec:conclusion}

We presented a linguistically informed, morphology-first hybrid tokenizer designed for Turkish and similar agglutinative languages. The tokenizer combines curated root and affix lexicons with phonological normalization (mapping surface allomorphs to shared identifiers) and a controlled subword fallback for coverage. This design aims to produce token sequences that more closely align with morpheme boundaries while remaining practical for large-scale NLP pipelines.

On TR-MMLU, the proposed tokenizer achieves 90.29\% Turkish Token Percentage (TR~\%) and 85.80\% Pure Token Percentage (Pure~\%), indicating substantially stronger morpheme-level alignment than several general-purpose tokenizers. We additionally report downstream sentence embedding evaluation on Turkish STS and MTEB-TR using \textbf{randomly initialized} models to isolate tokenizer effects from pretrained knowledge. The MFT-based model reaches \textbf{50.37\%} Pearson correlation on STSb-TR, compared to 33.58\% for the Tabi baseline---a gain of \textbf{+16.79 percentage points}. On MTEB-TR, MFT achieves 38.99\% overall average compared to 33.33\% for Tabi (+5.66 points). These substantial gaps demonstrate that morphology-first tokenization provides a stronger inductive bias for learning Turkish semantic representations from scratch.

We emphasize that empirical claims in this paper are Turkish-focused. We outline concrete next steps---improved morphophonological handling, better capitalization edge cases, and standardized efficiency measurements---in Section~\ref{sec:future_work}.
