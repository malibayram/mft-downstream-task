\section{Conclusion}
\label{sec:conclusion}

We presented a linguistically informed, morphology-first hybrid tokenizer designed for Turkish and similar agglutinative languages. The tokenizer combines curated root and affix lexicons with phonological normalization (mapping surface allomorphs to shared identifiers) and a controlled subword fallback for coverage. This design aims to produce token sequences that more closely align with morpheme boundaries while remaining practical for large-scale NLP pipelines.

On TR-MMLU, the proposed tokenizer achieves 90.29\% Turkish Token Percentage (TR~\%) and 85.80\% Pure Token Percentage (Pure~\%), indicating substantially stronger morpheme-level alignment than several general-purpose tokenizers. We additionally report downstream sentence embedding evaluation on Turkish STS and MTEB-TR. Under the same training budget, MFT-based models consistently outperform the Turkish subword baseline on STS and remain competitive across MTEB-TR task categories, with random-initialized baselines reinforcing that the observed gaps are not explained by training noise alone.

We emphasize that empirical claims in this paper are Turkish-focused. The framework structure is transferable, but cross-linguistic performance depends on the availability and quality of language-specific lexical resources and decoding rules. We outline concrete next steps---broader language coverage, improved morphophonological handling, better capitalization edge cases, and standardized efficiency measurements---in Section~\ref{sec:future_work}.
