\section{Discussion}
\label{sec:discussion}

\subsection{Why morphology-first tokenization helps semantic similarity}

The most consistent downstream gains we observe appear on semantic textual similarity (STS), where sentence-level meaning equivalence is evaluated via correlation with human judgments. A morphology-first tokenizer reduces two specific sources of representational noise in Turkish:
(i) \emph{root fragmentation}, where a frequent lexical stem is split into unrelated subpieces, and
(ii) \emph{allomorph dispersion}, where surface variants of the same morpheme (or root alternation) map to unrelated token identities.
Both effects can increase the burden on the model to relearn compositional meaning across many surface forms. By making roots and productive suffixes explicit and unifying common allomorphic variants, MFT encourages repeated exposure to stable meaning-bearing units.

\subsection{Trade-offs: token count, purity, and efficiency}

An important practical question is whether better morpheme alignment necessarily increases sequence length. In our TR-MMLU analysis, the proposed tokenizer produces more tokens overall than some baselines but achieves markedly higher TR~\% and Pure~\%. This reflects a deliberate bias toward compositionality: the tokenizer prefers to isolate affixes and preserve stable roots rather than ``compress'' meaning into opaque subword merges. The resulting trade-off can be summarized as:
\begin{itemize}
  \item \textbf{Higher alignment:} higher TR~\% and Pure~\% indicate tokens correspond to more interpretable units.
  \item \textbf{Potentially higher fertility:} explicit segmentation can increase token counts on morphologically complex word forms.
  \item \textbf{Compute implications:} longer sequences increase attention cost in transformer models, so the net effect on training/inference depends on how much alignment improves sample efficiency and representation quality.
\end{itemize}

Our current study reports tokenization-time measurements for the proposed tokenizer on TR-MMLU and provides downstream quality comparisons under a fixed training budget. A complete efficiency characterization would additionally compare tokenization throughput and end-to-end model latency for all baseline tokenizers under standardized hardware and batching.

Beyond runtime at inference, a practical ``adaptation cost'' matters for rapidly evolving domains. In a BPE-style pipeline, incorporating new morphological patterns or domain terms typically requires retraining merges or extending the subword vocabulary and re-learning embeddings for the resulting new units. In contrast, the morphology-first design isolates most linguistic updates to explicit resources (root and affix inventories, plus decoder rules), while keeping the statistical fallback fixed, making incremental updates conceptually simpler even when the downstream model still requires fine-tuning.

\subsection{Error modes and linguistic edge cases}

Despite improved alignment on many constructions, several edge cases remain important for a journal-ready characterization:
\begin{itemize}
  \item \textbf{Surface-vs-lexical ambiguity:} a deterministic longest-match strategy over surface strings can disagree with underlying morphological analyses in ambiguous cases (e.g., derivations where multiple segmentations are plausible).
  \item \textbf{Loanwords and irregular alternations:} Turkish includes morphophonological behaviors not fully captured by lightweight decoder rules (e.g., additional consonant alternations, gemination patterns, and harmony exceptions).
  \item \textbf{Compounds:} treating some lexicalized compounds as atomic entries may improve interpretability for common units but can blur morpheme-level preservation. A systematic ablation (compounds atomic vs.\ decomposed) is a natural next step.
  \item \textbf{Capitalization:} the \texttt{<uppercase>} marker preserves case without duplicating vocabulary, but sequences of all-caps letters and mixed-case identifiers (e.g., acronyms, code) remain imperfectly represented without richer casing rules.
\end{itemize}

\subsection{Interpretation of MTEB-TR results}

On the broader MTEB-TR suite, results are closer between tokenizers than on STS. This is expected because the benchmark spans heterogeneous task types (retrieval, classification, clustering, STS), and performance depends on factors beyond tokenization (training recipe, data distribution, and model architecture). Two takeaways are robust in our runs:
(i) MFT does not degrade performance across the suite and remains competitive, and
(ii) MFT yields the clearest improvements on semantic similarity, consistent with the hypothesis that morpheme-level clarity supports stable sentence representations.

\subsection{From embeddings to pretraining and fine-tuning}

This paper focuses on tokenizer design and evaluation. While we validate downstream utility through sentence embedding benchmarks, the same tokenization principles are relevant to pretraining and task-specific fine-tuning: stable roots and explicit affixes can increase reuse of meaning-bearing units, potentially improving sample efficiency and interpretability in morphologically rich languages. Establishing the causal impact in full language model pretraining remains an important direction, requiring controlled pretraining runs with matched compute and data.
