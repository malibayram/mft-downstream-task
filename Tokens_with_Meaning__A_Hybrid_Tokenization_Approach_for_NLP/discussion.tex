\section{Discussion}
\label{sec:discussion}

\textbf{Why morphology-first tokenization helps semantic similarity.}

The most consistent downstream gains we observe appear on STS, where sentence-level meaning equivalence is evaluated via correlation with human judgments. A morphology-first tokenizer reduces two specific sources of representational noise in Turkish:
(i) \emph{root fragmentation}, where a frequent lexical stem is split into unrelated subpieces, and
(ii) \emph{allomorph dispersion}, where surface variants of the same morpheme (or root alternation) map to unrelated token identities.
Both effects can increase the burden on the model to relearn compositional meaning across many surface forms. By making roots and productive suffixes explicit and unifying common allomorphic variants, MFT encourages repeated exposure to stable meaning-bearing units.

\textbf{Trade-offs: token count, purity, and efficiency.}

An important practical question is whether better morpheme alignment necessarily increases sequence length. In our TR-MMLU analysis, the proposed tokenizer produces more tokens overall than some baselines but achieves markedly higher TR~\% and Pure~\%. This reflects a deliberate bias toward compositionality: the tokenizer prefers to isolate affixes and preserve stable roots rather than ``compress'' meaning into opaque subword merges. On representative Turkish sentences, MFT averages approximately 3.2 tokens per word, compared to roughly 2.0--2.5 for BPE-based baselines; the additional tokens correspond to explicit suffix boundaries rather than arbitrary substrings. The resulting trade-off can be summarized as:
The resulting trade-off involves three factors. First, higher alignment (TR~\% and Pure~\%) indicates tokens correspond to more interpretable units. Second, this explicit segmentation can increase token counts on morphologically complex word forms (higher fertility). Finally, longer sequences increase attention cost in transformer models, so the net effect on training and inference depends on how much alignment improves sample efficiency and representation quality.

Our current study reports tokenization-time measurements for the proposed tokenizer on TR-MMLU and provides downstream quality comparisons under a fixed training budget. A complete efficiency characterization would additionally compare tokenization throughput and end-to-end model latency for all baseline tokenizers under standardized hardware and batching.

Beyond runtime at inference, a practical ``adaptation cost'' matters for rapidly evolving domains. In a BPE-style pipeline, incorporating new morphological patterns or domain terms typically requires retraining merges or extending the subword vocabulary and re-learning embeddings for the resulting new units. In contrast, the morphology-first design isolates most linguistic updates to explicit resources (root and affix inventories, plus decoder rules), while keeping the statistical fallback fixed, making incremental updates conceptually simpler even when the downstream model still requires fine-tuning.

\textbf{Error modes and linguistic edge cases.}

Despite improved alignment on many constructions, several edge cases remain important for a journal-ready characterization:
Despite improved alignment on many constructions, several edge cases remain important for a journal-ready characterization. First, a deterministic longest-match strategy over surface strings can disagree with underlying morphological analyses in ambiguous cases (e.g., derivations where multiple segmentations are plausible). Second, Turkish includes morphophonological behaviors not fully captured by lightweight decoder rules, such as additional consonant alternations, gemination patterns, and harmony exceptions. Third, treating some lexicalized compounds as atomic entries may improve interpretability for common units but can blur morpheme-level preservation; a systematic ablation is a natural next step. Finally, while the \texttt{<uppercase>} marker preserves case without duplicating vocabulary, sequences of all-caps letters and mixed-case identifiers (e.g., acronyms, code) remain imperfectly represented without richer casing rules.

\textbf{Interpretation of MTEB-TR results.}

On the broader MTEB-TR suite, results are closer between tokenizers than on STS. This is expected because the benchmark spans heterogeneous task types (retrieval, classification, clustering, STS), and performance depends on factors beyond tokenization (training recipe, data distribution, and model architecture). Two takeaways are robust in our runs:
(i) MFT does not degrade performance across the suite and remains competitive, and
(ii) MFT yields the clearest improvements on semantic similarity, consistent with the hypothesis that morpheme-level clarity supports stable sentence representations.

\textbf{From embeddings to pretraining and fine-tuning.}

This paper focuses on tokenizer design and evaluation. While we validate downstream utility through sentence embedding benchmarks, the same tokenization principles are relevant to pretraining and task-specific fine-tuning: stable roots and explicit affixes can increase reuse of meaning-bearing units, potentially improving sample efficiency and interpretability in morphologically rich languages. Establishing the causal impact in full language model pretraining remains an important direction, requiring controlled pretraining runs with matched compute and data.
