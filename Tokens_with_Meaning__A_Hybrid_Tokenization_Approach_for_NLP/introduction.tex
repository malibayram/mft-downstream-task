\section{Introduction}
\label{sec:introduction}

Tokenization---the mapping from text to discrete symbols---is a foundational design choice in modern NLP systems, influencing vocabulary size, sequence length, and how reliably models can represent morphology and meaning \cite{liu_roberta_2019}. Subword tokenizers such as Byte Pair Encoding (BPE) \cite{sennrich_neural_2016} and WordPiece \cite{schuster_japanese_2012} have become the default for transformer-based models \cite{devlin_bert_2019, radford_language_2019} because they mitigate out-of-vocabulary issues and scale to large corpora. However, frequency-driven segmentation can conflict with linguistic structure, particularly in morphologically rich and agglutinative languages where words encode many grammatical features through productive affixation.

Turkish is a canonical case: a single lemma can generate many surface forms through suffix sequences, while morphophonological alternations (e.g., vowel harmony, consonant alternations) create additional surface variability. In such settings, subword splits can fragment roots, merge partial morphemes, and treat allomorphic variants as unrelated symbols, reducing interpretability and potentially weakening generalization \cite{toraman_impact_2023, kaya_effect_2024}.

This paper investigates a simple guiding hypothesis: \emph{token boundaries that better align with morpheme boundaries provide cleaner input structure for learning}. Following \cite{bayram_tokenization_2025}, we quantify linguistic alignment using (i) Turkish Token Percentage (TR~\%), the proportion of tokens that correspond to Turkish lexical/morphemic units, and (ii) Pure Token Percentage (Pure~\%), the proportion of tokens that align with unambiguous root/affix boundaries. These metrics are not a substitute for downstream evaluation, but they provide an interpretable diagnostic for segmentation quality.

We propose a hybrid tokenizer that is \emph{morphology-first}: it segments via curated root and affix dictionaries with phonological normalization (mapping surface allomorphs to shared identifiers), and applies a controlled subword fallback to handle out-of-vocabulary material. While our implementation is Turkish-specific, the framework is designed to transfer to other morphologically rich languages given comparable lexical resources.

\paragraph{Contributions.} This work makes four contributions:
\begin{itemize}
  \item A linguistically informed hybrid tokenization pipeline combining dictionary-based morphological segmentation, phonological normalization, and a subword fallback.
  \item A transparent Turkish implementation with open dictionaries (\texttt{kokler.json}, \texttt{ekler.json}) and decoding rules (\texttt{turkish\_decoder.py}).
  \item An evaluation on TR-MMLU using TR~\% and Pure~\% to quantify token--morpheme alignment.
  \item A downstream sentence embedding evaluation on Turkish STS and MTEB-TR, showing that under the same training budget the proposed tokenizer yields stronger STS performance and remains competitive across MTEB-TR categories.
\end{itemize}

\paragraph{Scope and limitations.} Our experiments are Turkish-focused; we therefore frame cross-linguistic generality as a \emph{framework claim} (the pipeline structure) rather than an empirical claim across typologically diverse languages. We discuss transfer requirements and limitations in Section~\ref{sec:future_work}.

\paragraph{Paper organization.}
Section~\ref{sec:related_work} reviews subword and morphology-aware tokenization approaches and Turkish-specific baselines. Section~\ref{sec:methodology} details our vocabulary design, encoding/decoding procedures, and edge-case handling. Section~\ref{sec:results} reports tokenization alignment on TR-MMLU and downstream sentence embedding results on STSb-TR and MTEB-TR.
