\section{Related Work}
\label{sec:related_work}

Tokenization is a fundamental step in NLP, significantly impacting model performance, memory efficiency, and downstream task effectiveness. Tokenization strategies range from character-level segmentation to subword-based methods such as BPE \cite{sennrich_neural_2016}, WordPiece \cite{schuster_japanese_2012}, and Unigram \cite{kudo_subword_2018}. The choice of tokenization directly influences the ability of models to capture syntactic, semantic, and morphological structures, especially in morphologically rich languages such as Turkish, Finnish, and Hungarian \cite{baykara_abstractive_2022, toraman_impact_2023}.

Recent research has explored alternative tokenization strategies tailored to morphologically rich languages. Toraman et al.~\cite{toraman_impact_2023} analyze the impact of tokenization on Turkish language modeling, and report that morphology-aware tokenization can recover much of the performance of larger baselines under certain settings. Kaya and Tantuğ~\cite{kaya_effect_2024} examine tokenization granularity for Turkish language models, and highlight that Turkish can require substantially more subword splits per word than English under common subword tokenizers, underscoring the importance of vocabulary design and sequence length control.

Tokenization strategies also play a crucial role in machine translation and text generation tasks. Pan et al.~\cite{pan_morphological_2020} demonstrate that morphology-aware segmentation can reduce sparsity in neural machine translation, and Huck et al.~\cite{huck_target_2017} study target-side segmentation strategies that improve translation quality by maintaining linguistic consistency between source and target languages. Beyond translation, morphology-aware tokenization has also been evaluated in abstractive summarization and sentiment analysis. Baykara and Güngör~\cite{baykara_abstractive_2022} discuss summarization for agglutinative languages, and Kayalı and Omurca~\cite{kayali_hybrid_2024} propose a hybrid tokenization strategy for Turkish summarization. Such hybrid approaches are also commonly motivated by applications where preserving linguistic structure is important (e.g., named entity recognition (NER)).

Tokenization quality is also discussed in the context of modern LLM tokenizers, where differences in segmentation can affect non-English text processing and evaluation outcomes. Bayram et al.~\cite{bayram_tokenization_2025} compare several widely used tokenizers on Turkish and highlight how tokenizer-specific segmentation artifacts can influence downstream benchmarking.

Despite these advancements, the computational cost of tokenization and its interaction with training efficiency remains an open concern. Larger vocabularies can increase model size and memory footprint \cite{devlin_bert_2019, liu_roberta_2019}, and the energy and carbon footprint of training large models has motivated more careful reporting and efficiency analysis \cite{henderson_towards_2022}. From this perspective, tokenization is not only a linguistic design choice, but also a practical lever that affects sequence length and compute; inefficient vocabulary utilization and redundant segmentation can translate into longer sequences and higher training cost \cite{henderson_towards_2022}.

To address trade-offs between linguistic alignment and efficiency, recent work has explored adaptive and multilingual tokenization strategies. Martins et al.~\cite{martins_eurollm_2024} describe multilingual language models and tokenization choices across European languages, and Lin et al.~\cite{lin_not_2025} study token selection strategies that question whether all tokens contribute equally during pretraining. Dynamic tokenization approaches that adapt segmentation rules have also been proposed; for example, Neubeck et al.~\cite{neubeck_bpe_2024} explore a more flexible BPE-style tokenizer.

Several approaches incorporate linguistic structure directly into tokenization. Hofmann et al.~\cite{hofmann_superbizarre_2021} show that derivationally informed segmentation can improve model interpretation of complex word forms. MorphPiece~\cite{jabbar_morphpiece_2024} segments by morphemes before applying a subword encoding step, aiming to preserve compositional meaning while remaining compatible with standard training pipelines. Closest to our design are hybrid tokenizers that combine explicit linguistic resources with statistical fallback. miLLi~\cite{rahimov_milli_2025} is a tokenizer for Azerbaijani that uses a root dictionary, BPE fallback, and a phonological restoration mechanism to increase root consistency across surface variants. Another line of work modifies the subword algorithm itself to better respect morphological structure: MorphBPE~\cite{asgari_morphbpe_2025} extends BPE with morphology-aware constraints and introduces morphology-based evaluation metrics, reporting improved morphological alignment and training behavior across multiple languages. While these morphology-aware tokenizers share design motivations with our approach, they target different languages (Azerbaijani, Arabic, and multilingual corpora) and are therefore not directly comparable on Turkish-specific benchmarks without substantial adaptation.

Tokenization strategies play a critical role in pretraining large language models (LLMs), influencing model efficiency, generalization, and performance across downstream tasks. Transformer-based architectures such as BERT \cite{devlin_bert_2019}, RoBERTa \cite{liu_roberta_2019}, and GPT \cite{radford_language_2019} rely on effective tokenization to balance vocabulary size, sequence length, and computational cost. Studies have shown that tokenization choices can interact with morphological compositionality and generalization, particularly for morphologically rich languages \cite{ismayilzada_evaluating_2025}.

Benchmark evaluations such as Massive Multitask Language Understanding (MMLU) \cite{hendrycks_measuring_2021} and TR-MMLU \cite{bayram_setting_2025} have highlighted the need for language-aware evaluation. Bayram et al.~\cite{bayram_tokenization_2025} propose a linguistic integrity framework for evaluating Turkish tokenization, introducing metrics such as token purity and Turkish Token Percentage (TR~\%). TR~\% measures the proportion of produced tokens that correspond to valid Turkish lexical or morphemic units under curated lexical resources, while Pure~\% further requires that tokens align with unambiguous morpheme boundaries rather than arbitrary substrings. These metrics are computed using an external morphological validator (a curated root/affix inventory independent of the tokenizer under evaluation), which ensures fair cross-tokenizer comparison and avoids circularity. Their results suggest that higher TR~\% and purity correlate with stronger performance on MMLU-style Turkish benchmarks, motivating our focus on morpheme-level alignment.

Turkish-specific benchmarks and evaluation suites have expanded rapidly. TR-MMLU~\cite{bayram_setting_2025} provides a large-scale Turkish evaluation set for language model assessment, and TurBLiMP~\cite{basar_turblimp_2025} offers a controlled benchmark of linguistic minimal pairs covering diverse phenomena. In parallel, Turkish-focused model and tokenizer ecosystems continue to grow. For example, TabiBERT~\cite{turker_tabibert_2026} provides a modern Turkish encoder and a unified evaluation suite, reinforcing the value of language-specific baselines when assessing tokenizer behavior and downstream impact.

Finally, tokenization considerations extend beyond language modeling into applied pipelines such as optical character recognition and document parsing. Rashad et al.~\cite{rashad_arabic_nougat_2024} demonstrate that tokenizer choices can affect structure reconstruction and recognition accuracy in Arabic document processing, and Rosa et al.~\cite{rosa_tokenizer_benchmark_2024} provide a tokenizer benchmark in a multilingual setting, illustrating that tokenizer behavior can vary widely across languages and domains.
