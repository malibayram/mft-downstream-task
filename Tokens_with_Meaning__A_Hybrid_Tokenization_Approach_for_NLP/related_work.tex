\section{Related Work}
\label{sec:related_work}

Tokenization for transformer models is commonly implemented with subword algorithms such as BPE \cite{sennrich_neural_2016}, WordPiece \cite{schuster_japanese_2012}, and Unigram \cite{kudo_subword_2018}. These methods provide open-vocabulary coverage and compact vocabularies, but they may split across morpheme boundaries and treat predictable allomorphs as unrelated units, which can be problematic in morphologically rich languages \cite{toraman_impact_2023, kaya_effect_2024}.

For Turkish, prior work has analyzed how tokenization granularity affects downstream behavior. Toraman et al.~\cite{toraman_impact_2023} study the impact of tokenization choices on Turkish language modeling, and Kaya and TantuÄŸ~\cite{kaya_effect_2024} discuss how common subword tokenizers interact with Turkish morphology and sequence length. In machine translation, morphology-aware segmentation has also been investigated as a way to reduce sparsity and improve generalization \cite{pan_morphological_2020, huck_target_2017}.

Several approaches incorporate linguistic structure directly into tokenization. Hofmann et al.~\cite{hofmann_superbizarre_2021} show that derivationally informed segmentation can improve model interpretation of complex word forms. MorphPiece~\cite{jabbar_morphpiece_2024} segments by morphemes before applying a subword encoding step, aiming to preserve compositional meaning while retaining compatibility with standard training pipelines. These methods motivate our focus on morpheme-level alignment, but the literature also suggests that the benefits can be task- and model-dependent, emphasizing the importance of reporting both tokenization metrics and downstream outcomes.

Closest to our design are hybrid tokenizers that combine explicit linguistic resources with statistical fallback. miLLi~\cite{rahimov_milli_2025} is a tokenizer for Azerbaijani that uses a root dictionary, BPE fallback, and a phonological restoration mechanism to increase root consistency across surface variants. Our approach similarly emphasizes stable root and affix identifiers, but targets Turkish morphology and includes explicit formatting and casing tokens to support lossless decoding.

Another line of work modifies the subword algorithm itself to better respect morphological structure. MorphBPE~\cite{asgari_morphbpe_2025} extends BPE with morphology-aware constraints and introduces morphology-based evaluation metrics, reporting improved morphological alignment and training behavior across multiple languages. In contrast, our work treats morphology as a first-class layer through dictionary-driven segmentation and normalization, and uses subword units primarily as a controlled fallback for uncovered segments.

Finally, Turkish-specific benchmarks and evaluation suites have expanded rapidly in recent years. TR-MMLU~\cite{bayram_setting_2025} provides a large-scale Turkish evaluation set for language model assessment, and TurBLiMP~\cite{basar_turblimp_2025} offers a controlled benchmark of linguistic minimal pairs covering diverse phenomena. These resources enable more transparent comparisons for Turkish, and we use them to ground both our tokenization analysis and downstream evaluation.

In parallel, Turkish-focused model and tokenizer ecosystems continue to grow. For example, TabiBERT~\cite{turker_tabibert_2026} provides a modern Turkish encoder and a unified evaluation suite, reinforcing the value of language-specific baselines when assessing tokenizer behavior and downstream impact.
