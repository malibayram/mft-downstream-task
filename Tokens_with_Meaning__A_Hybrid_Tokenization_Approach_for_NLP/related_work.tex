\section{Related Work}
\label{sec:related_work}

\paragraph{Subword tokenization.}
Subword methods such as BPE \cite{sennrich_neural_2016}, WordPiece \cite{schuster_japanese_2012}, and Unigram-based approaches \cite{kudo_sentencepiece_2018} are widely used because they balance vocabulary size and coverage in neural models \cite{devlin_bert_2019, liu_roberta_2019}. For morphologically rich languages, however, segmentation granularity and vocabulary allocation become more consequential, often leading to higher token ``fertility'' and longer effective sequences \cite{kaya_effect_2024}.

\paragraph{Morphology-aware tokenization for Turkish and beyond.}
Prior work on Turkish shows that tokenizer choice can substantially affect model quality and efficiency. Toraman et al.~\cite{toraman_impact_2023} study tokenization impacts for Turkish language models, while Bayram et al.~\cite{bayram_tokenization_2025} propose linguistic alignment metrics (including TR~\% and Pure~\%) as diagnostics for Turkish tokenization quality. More broadly, morphology-aware segmentation has been explored for tasks such as machine translation \cite{pan_morphological_2020, huck_target_2017} and abstractive summarization \cite{baykara_abstractive_2022}.

\paragraph{Hybrid approaches combining morphology and subwords.}
Hybrid designs that retain linguistic structure while keeping statistical coverage are increasingly common. Jabbar~\cite{jabbar_morphpiece_2024} proposes MorphPiece, which applies morpheme segmentation prior to subword encoding. Asgari et al.~\cite{asgari_morphbpe_2025} propose MorphBPE, a morphology-aware extension of BPE that constrains merges and introduces morphology-based evaluation metrics across multiple languages. Rahimov~\cite{rahimov_milli_2025} introduces miLLi, a dictionary+BPE hybrid for Azerbaijani with a phonological restoration layer that maps allomorphic variants back to canonical roots. Our work aligns with this line but adopts a morphology-first pipeline with explicit allomorph unification and reversible decoding rules.

\paragraph{Turkish-specific baselines and benchmarks.}
Recent Turkish-focused foundation models and benchmarks emphasize the importance of language-specific evaluation and documentation. TÃ¼rker et al.~\cite{turker_tabibert_2026} introduce TabiBERT and a unified Turkish benchmark, motivating fairer Turkish baselines beyond English-centric tokenizers.
Complementary work on Turkish sentence embeddings highlights practical tokenizer adaptation and evaluation on STS/retrieval benchmarks; Bayram~\cite{bayram_adapting_embeddings_2026} describes token remapping and embedding-alignment techniques for adapting pretrained embedding models to Turkish.

\paragraph{Downstream semantic similarity evaluation.}
Correlation-based evaluation (Pearson/Spearman) is standard for semantic textual similarity, and for agglutinative languages semantic similarity metrics can capture quality beyond surface lexical overlap. Beken Fikri et al.~\cite{beken_fikri_semantic_2021} translate STS resources into Turkish and motivate semantic similarity as an evaluation signal for Turkish text generation and summarization.

\paragraph{Empirical variability.}
Across languages and training setups, the effect of morphology-aware tokenization on downstream model performance can vary. We therefore treat linguistic alignment metrics as interpretable diagnostics for segmentation quality rather than as proof of downstream gains, and we complement them with downstream evaluation (Section~\ref{sec:downstream}).
