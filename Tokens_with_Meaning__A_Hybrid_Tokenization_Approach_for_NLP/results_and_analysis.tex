\section{Results and Analysis}
\label{sec:results}

The performance of the proposed morphological tokenizer was evaluated using the TR-MMLU benchmark dataset, which comprises over 1.6 million characters and approximately 200,000 words curated specifically for Turkish \cite{bayram_setting_2025}. This dataset is designed to reflect the linguistic complexity of Turkish, including its rich morphology, agglutinative structures, and diverse syntactic constructions. As such, it provides a rigorous basis for assessing tokenization quality in morphologically complex languages.

The evaluation compared five different tokenizers: \texttt{google/gemma-2-9b}, \texttt{meta-llama/Llama-3.2-3B}, \texttt{Qwen/Qwen2.5-7B-Instruct}, \texttt{CohereForAI/aya-expanse-8b}, and the proposed \texttt{turkish\_tokenizer}. Each tokenizer was assessed using a consistent set of linguistic and computational metrics introduced in \cite{bayram_tokenization_2025}. These metrics include total token count, vocabulary size, number of unique tokens, Turkish Token Percentage (TR~\%), and Pure Token Percentage (Pure~\%). TR~\% quantifies the proportion of tokens that correspond to valid Turkish words or morphemes, while Pure~\% measures the proportion of tokens that fully align with unambiguous root or affix boundaries, thus reflecting morphological integrity.

\begin{table}[htbp]
\centering
\caption{Performance of the proposed \texttt{turkish\_tokenizer} on the TR-MMLU dataset.}
\label{tab:turkish_tokenizer_results}
\begin{tabular}{|l|c|}
\hline
\rowcolor[HTML]{DDDDDD} 
\textbf{Metric} & \textbf{Value} \\ \hline
Vocabulary Size & 32,768 \\ \hline
Total Token Count & 707,727 \\ \hline
Processing Time (s) & 0.6714 \\ \hline
Unique Token Count & 11,144 \\ \hline
Turkish Token Count & 10,062 \\ \hline
Turkish Token Percentage (TR \%) & 90.29\% \\ \hline
Pure Token Count & 9,562 \\ \hline
Pure Token Percentage (Pure \%) & 85.80\% \\ \hline
\end{tabular}
\end{table}

The proposed \texttt{turkish\_tokenizer} demonstrated the highest linguistic alignment across all evaluated metrics. It achieved a TR~\% of 90.29\% and a Pure~\% of 85.80\%, substantially outperforming all competing tokenizers. In comparison, \texttt{google/gemma-2-9b} reached a TR~\% of only 40.96\% and a Pure~\% of 28.49\%, indicating that the majority of its tokens do not represent full morphemes. Similarly, \texttt{meta-llama/Llama-3.2-3B} produced a TR~\% of 45.77\% and a Pure~\% of 31.45\%, while \texttt{Qwen2.5} and \texttt{aya-expanse} achieved TR~\% values of 40.39\% and 53.48\%, respectively.

Despite employing significantly smaller vocabulary sizes, the proposed tokenizer demonstrated better linguistic segmentation. With a vocabulary of 32,768 tokens and 11,144 unique tokens used during evaluation, it balanced generalization and expressiveness more effectively than models such as \texttt{gemma-2-9b} and \texttt{aya-expanse}, which rely on vocabularies of over 255,000 tokens. These large-vocabulary tokenizers, rooted in frequency-based subword segmentation, tend to fragment morphologically rich expressions and introduce ambiguity in downstream tasks. In contrast, the morphological awareness of the \texttt{turkish\_tokenizer} enables semantically coherent token formation and more consistent syntactic parsing.

Although the total token count generated by the proposed tokenizer (707,727) exceeds those of the other models-for instance, \texttt{aya-expanse} produced 434,526 tokens-this increase is offset by gains in interpretability and linguistic fidelity. High TR~\% and Pure~\% scores suggest reduced reliance on spurious subword splits and improved preservation of morphosyntactic structure. This is particularly beneficial for tasks such as syntactic parsing, translation, summarization, and question answering, where semantic consistency across tokens is essential.

These findings support the hypothesis introduced in \cite{bayram_tokenization_2025}, which argues that high linguistic alignment in tokenization correlates strongly with downstream model performance in morphologically rich and low-resource languages. While conventional subword tokenizers may suffice for high-resource languages like English, they exhibit clear limitations in Turkish unless informed by morphological structure. The results presented here highlight the effectiveness of combining rule-based linguistic analysis with subword strategies to produce tokenizers that are both accurate and efficient in morphologically complex settings.

To illustrate the linguistic fidelity of different tokenization strategies, we present a qualitative comparison using the Turkish sentence:

\textit{"Atasözleri geçmişten günümüze kadar ulaşan anlamı bakımından mecazlı bir mana kazanan kalıplaşmış sözlerdir."} \\
(“Proverbs are fixed expressions passed down from the past to the present that acquire a metaphorical meaning in terms of their significance.”)

This sentence contains a wide range of morphological features, including compound words, multiple derivational and inflectional suffixes, and root forms that undergo phonological alternations. These properties make it an ideal test case for evaluating the morphological sensitivity of different tokenizers.

\vspace{1em}

\textbf{Proposed Hybrid Tokenizer:} \\
The hybrid morphological tokenizer segments the sentence into linguistically meaningful units with high fidelity. It produces: \\
\texttt{["<uppercase>", "atasöz", "ler", "i", "<space>", "geçmiş", "ten", "<space>", "gün", "üm", "üz", "e", "<space>", "kadar", "<space>", "ulaş", "an", "<space>", "anlam", "ı", "<space>", "bakım", "ın", "dan", "<space>", "mecaz", "lı", "<space>", "bir", "<space>", "mana", "<space>", "kazan", "an", "<space>", "kalıp", "laş", "mış", "<space>", "sözle", "r", "dir", "."]} \\
It correctly separates suffixes (\texttt{"ler", "i", "ın", "dan", "lı", "an", "mış", "dir"}), extracts root forms such as \texttt{"atasöz", "gün", "mana"}, and employs special tokens like \texttt{"<uppercase>"} and \texttt{"<space>"} to preserve orthographic structure.

\vspace{1em}

\textbf{Gemma-3:} \\
The tokenizer \texttt{google/gemma-3} segments the sentence as: \\
\texttt{["<bos>", "At", "as", "öz", "leri", " geçmiş", "ten", " gün", "ümü", "ze", " kadar", " ulaş", "an", " anlam", "ı", " bakım", "ından", " mec", "az", "lı", " bir", " mana", " kaz", "anan", " kal", "ı", "pla", "ş", "mış", " söz", "lerdir", "."]} \\
Although it captures some suffixes like \texttt{"ten"} and \texttt{"ından"}, it fragments common roots (\texttt{"At", "as", "öz"} instead of \texttt{"atasöz"}) and fails to isolate inner morphemes in forms such as \texttt{"lerdir"} and \texttt{"kazanan"}, limiting morphological interpretability.

\vspace{1em}

\textbf{LLaMA-3.2:} \\
The tokenizer \texttt{meta-llama/Llama-3.2-3B} yields: \\
\texttt{["<|begin\_of\_text|>", "At", "as", "öz", "leri", " geçmiş", "ten", " gün", "ümü", "ze", " kadar", " ", "ula", "ş", "an", " anlam", "ı", " bakımından", " me", "ca", "z", "lı", " bir", " mana", " kaz", "anan", " kal", "ı", "pla", "ş", "mış", " söz", "lerdir", "."]} \\
This tokenizer combines morphologically valid segments like \texttt{"bakımından"} and \texttt{"kazanan"} with fragmented roots like \texttt{"At", "as", "öz"}, creating inconsistency in morpheme alignment.

\vspace{1em}

\textbf{YTU Turkish GPT-2:} \\
The tokenizer \texttt{ytu-ce-cosmos/turkish-gpt2-large-750m-instruct-v0.1}, trained on Turkish corpora, yields: \\
\texttt{["At", "as", "öz", "leri", " geçmişten", " günümüze", " kadar", " ulaşan", " anlamı", " bakımından", " mec", "az", "lı", " bir", " mana", " kazanan", " kalıp", "laşmış", " söz", "lerdir", "."]} \\
Although it still segments \texttt{"atasözleri"} incorrectly, it performs well with forms like \texttt{"geçmişten"}, \texttt{"günümüze"}, and \texttt{"bakımından"}, showing the advantage of Turkish-specific pretraining.

\vspace{1em}

\textbf{GPT-4o:} \\
The tokenizer \texttt{gpt-4o-o200k\_base} generates: \\
\texttt{["At", "as", "öz", "leri", " geçmiş", "ten", " gün", "ümü", "ze", " kadar", " ulaş", "an", " anlam", "ı", " bakım", "ından", " mec", "az", "lı", " bir", " mana", " kaz", "anan", " kal", "ı", "pla", "ş", "mış", " söz", "ler", "dir", "."]} \\
Its segmentation strategy is similar to LLaMA and Qwen-partially aware of Turkish morphemes but limited by frequent over-segmentation of compound and derived forms.

\vspace{1em}

The results presented in this section provide strong empirical support for the hypothesis introduced in the introduction: tokenizers that explicitly incorporate morphological and phonological knowledge of Turkish can outperform general-purpose models in both segmentation accuracy and linguistic coherence. While most state-of-the-art tokenizers struggle with root-fragmentation, over-segmentation, and inconsistent affix treatment, the proposed hybrid tokenizer consistently identifies morpheme boundaries, preserves semantically meaningful units, and reduces vocabulary redundancy. These findings validate the motivation behind this work: morphologically informed tokenization is essential for robust and interpretable NLP in agglutinative languages like Turkish. The qualitative comparisons presented here illustrate not only the performance gap between general and language-specific tokenizers, but also the need for tokenizer architectures that respect language-internal rules.

\subsection{Downstream Task Evaluation}
\label{subsec:downstream}

To assess the impact of morphologically informed tokenization on downstream model performance, we evaluated the embeddings produced by models initialized with different tokenizers using three benchmarks: STSb-TR, MTEB-TR, and TurBLiMP. All models were initialized randomly to isolate the effect of tokenization structure from pre-training data.

For STS, we use the Turkish STSb-TR benchmark (\texttt{figenfikri/stsb\_tr}) consisting of sentence pairs with human similarity ratings on a 0--5 scale \cite{beken_fikri_semantic_2021}. Each model encodes both sentences, we compute cosine similarity between the resulting sentence embeddings, and we report Pearson and Spearman correlation with the normalized gold scores. Throughout this section, correlations are presented as percentages ($\times 100$) for readability.

We evaluated the models on the Turkish STS benchmark (stsb-tr) in a zero-shot setting. The proposed \texttt{mft-random-init} model achieved a significantly higher correlation with human judgments compared to other randomly initialized baselines, demonstrating that its structural prior provides a better starting point for capturing semantic similarity.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/sts_benchmark_chart_test.png}
    \caption{Test split performance on STS Benchmark comparing MFT against Tabi, Cosmos, and Mursit baselines.}
    \label{fig:sts_chart}
\end{figure}

The \texttt{mft-random-init} model achieved a Pearson correlation of \textbf{50.37\%} and Spearman correlation of \textbf{49.35\%}, consistently outperforming all baselines. In comparison, the \texttt{newmindaiMursit-random-init} model achieved a Spearman correlation of 43.75\%, \texttt{cosmosGPT2-random-init} reached 38.02\%, and \texttt{tabi-random-init} scored 33.24\%. These results highlight the advantage of morphological segmentation in preserving semantic density, even without extensive pre-training.

To better understand the learning dynamics, we analyzed the performance evolution of each model across different training checkpoints. Figure \ref{fig:version_history_pearson} and Figure \ref{fig:version_history_spearman} illustrate the Pearson and Spearman correlations respectively at various stages of training (if applicable) or across collected versions. The x-axis represents sequential checkpoints ordered by date.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/version_history_pearson.png}
    \caption{Pearson correlation history comparing model performance across versions.}
    \label{fig:version_history_pearson}
\end{figure}

Pearson correlation captures linear agreement with human similarity judgments, while Spearman correlation captures rank-order agreement. Reporting both is important in STS, since models may preserve relative similarity ordering even when the mapping is not perfectly linear, and conversely small linear gains may not reflect better ranking behavior.

In our version tracking, both correlations show the same qualitative trend: MFT remains ahead of the strongest baselines across revisions, indicating that the downstream improvement is stable rather than a single-run artifact.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/version_history_spearman.png}
    \caption{Spearman correlation history comparing model performance across versions.}
    \label{fig:version_history_spearman}
\end{figure}

\FloatBarrier

On the comprehensive MTEB suite, which covers retrieval, classification, clustering, and pair classification tasks, the MFT-based model consistently outperformed other baselines.

\begin{table}[H]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lrrrr}
\toprule
\textbf{Task} & \textbf{MFT} & \textbf{Cosmos} & \textbf{Mursit} & \textbf{Tabi} \\
\midrule
\multicolumn{5}{l}{\textit{BitextMining}} \\
WMT16BitextMining & \textbf{1.53} & 1.08 & 1.26 & 1.39 \\
\multicolumn{5}{l}{\textit{Classification}} \\
THYSentimentClassification & 51.48 & 49.74 & \textbf{52.73} & 43.02 \\
TSTimelineNewsCategoryClassification & \textbf{50.06} & 43.09 & 44.33 & 44.09 \\
Turkish75NewsClassification & 73.33 & 78.67 & 74.67 & \textbf{79.33} \\
TurkishIronyClassification & 51.25 & 50.83 & \textbf{53.33} & 52.50 \\
TurkishMovieSentimentClassification & \textbf{54.74} & 54.37 & 54.57 & 53.84 \\
TurkishNewsCategoryClassification & \textbf{85.40} & 83.32 & 80.52 & 79.08 \\
TurkishOffensiveLanguageClassification & \textbf{49.87} & 48.74 & 49.71 & 48.22 \\
TurkishProductSentimentClassification & \textbf{54.34} & 51.39 & 51.85 & 52.10 \\
\multicolumn{5}{l}{\textit{Clustering}} \\
TurkishColumnWritingClustering & 66.30 & \textbf{66.45} & 65.39 & 65.71 \\
\multicolumn{5}{l}{\textit{Other}} \\
ArguAnaTR & \textbf{7.62} & 2.96 & 3.53 & 2.56 \\
FiQA2018TR & \textbf{6.74} & 1.53 & 2.78 & 2.38 \\
SCIDOCSTR & 0.47 & 0.27 & 0.27 & \textbf{0.74} \\
\multicolumn{5}{l}{\textit{Pair Classification}} \\
MnliTr & \textbf{48.46} & 45.32 & 45.67 & 44.98 \\
SnliTr & \textbf{44.73} & 40.29 & 40.29 & 40.04 \\
XNLI & 57.55 & \textbf{58.21} & 54.72 & 57.28 \\
\multicolumn{5}{l}{\textit{Retrieval}} \\
CQADupstackGamingRetrievalTR & \textbf{13.00} & 6.84 & 7.04 & 6.73 \\
MSMarcoTRRetrieval & \textbf{12.84} & 6.07 & 6.13 & 4.83 \\
NFCorpusTR & \textbf{1.22} & 0.40 & 0.51 & 0.73 \\
QuoraRetrievalTR & \textbf{63.01} & 46.44 & 49.24 & 46.98 \\
SciFactTR & \textbf{25.64} & 16.33 & 20.54 & 15.16 \\
SquadTRRetrieval & \textbf{16.53} & 8.07 & 8.93 & 6.14 \\
TQuadRetrieval & \textbf{43.46} & 29.97 & 29.48 & 26.30 \\
TurkishAbstractCorpusClustering & \textbf{47.46} & 43.63 & 42.81 & 39.69 \\
XQuADRetrieval & \textbf{37.33} & 23.16 & 25.38 & 19.54 \\
\multicolumn{5}{l}{\textit{STS}} \\
STSbTR & \textbf{49.36} & 38.04 & 43.75 & 33.24 \\
\bottomrule
\end{tabular}
}
\caption{Detailed MTEB-TR performance comparison across all tasks. Best scores in bold.}
\label{tab:mteb_detailed}
\end{table}

As shown in Table \ref{tab:mteb_detailed}, the MFT-based model achieved an average score of \textbf{38.99\%} across 26 tasks, surpassing Mursit (34.98\%), Cosmos (34.43\%), and Tabi (33.33\%). The advantages are particularly pronounced in Retrieval and Pair Classification tasks, where morphological coherence aids in matching semantic intent.

TurBLiMP evaluates the model's sensitivity to specific linguistic phenomena such as case marking, agreement, and word order. Table \ref{tab:turblimp_detailed} presents the accuracy of each model in distinguishing grammatical from ungrammatical sentences across various linguistic categories.

\begin{table}[H]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lrrrr}
\toprule
\textbf{Linguistic Phenomenon} & \textbf{Mursit-Random} & \textbf{MFT-Random} & \textbf{Cosmos-Random} & \textbf{Tabi-Random} \\
\midrule
Ellipsis & 98.0\% & \textbf{99.2\%} & 97.7\% & 93.0\% \\
Scrambling & 97.7\% & \textbf{98.0\%} & 97.3\% & \textbf{98.0\%} \\
Determiners & 94.9\% & \textbf{96.5\%} & 93.4\% & 94.1\% \\
Quantifiers & 90.2\% & 90.2\% & 86.3\% & \textbf{94.1\%} \\
Suspended Affixation & 89.5\% & \textbf{93.4\%} & 83.2\% & 91.0\% \\
Relative Clauses & 89.1\% & \textbf{96.1\%} & 86.3\% & 93.0\% \\
Binding & 88.7\% & 89.5\% & 85.5\% & \textbf{94.5\%} \\
Anaphor Agreement & 87.9\% & \textbf{92.6\%} & 84.8\% & \textbf{92.6\%} \\
Npi Licensing & 87.5\% & 87.5\% & 82.4\% & \textbf{89.5\%} \\
Irregular Forms & 87.1\% & 90.6\% & 80.1\% & \textbf{92.6\%} \\
Argument Structure Ditransitive & 86.3\% & 93.4\% & 80.9\% & \textbf{93.8\%} \\
Subject Verb Agreement & 84.8\% & \textbf{89.5\%} & 79.7\% & 86.3\% \\
Nominalization & 82.8\% & 87.5\% & 79.3\% & \textbf{89.8\%} \\
Argument Structure Transitive & 81.6\% & 90.6\% & 76.2\% & \textbf{91.0\%} \\
Passives & 81.6\% & 85.5\% & 79.3\% & \textbf{90.6\%} \\
Island Effects & 79.7\% & \textbf{84.0\%} & 76.2\% & \textbf{84.0\%} \\
\bottomrule
\end{tabular}
}
\caption{Detailed TurBLiMP sensitivity scores comparison across all models.}
\label{tab:turblimp_detailed}
\end{table}

The MFT tokenizer demonstrates superior handling of complex morphological features such as \textit{Scrambling} (98.0\%), \textit{Relative Clauses} (96.1\%), and \textit{Anaphor Agreement} (92.6\%). These results confirm that explicit morphological modeling allows the model to better generalize over complex syntactic dependencies.

Finally, we tracked STS performance across multiple experiment iterations and code revisions to ensure the observed gains are not a one-off artifact; the best observed revision reached 76.10\% Pearson on STSb-TR, with results remaining stable across recent runs.

\FloatBarrier
