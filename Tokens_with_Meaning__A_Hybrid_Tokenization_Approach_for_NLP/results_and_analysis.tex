\section{Results and Analysis}
\label{sec:results}

The performance of the proposed morphological tokenizer was evaluated using the TR-MMLU benchmark dataset, which comprises over 1.6 million characters and approximately 200,000 words curated specifically for Turkish \cite{bayram_setting_2025}. This dataset is designed to reflect the linguistic complexity of Turkish, including its rich morphology, agglutinative structures, and diverse syntactic constructions. As such, it provides a rigorous basis for assessing tokenization quality in morphologically complex languages.

The evaluation compared five different tokenizers: \texttt{google/gemma-2-9b}, \texttt{meta-llama/Llama-3.2-3B}, \texttt{Qwen/Qwen2.5-7B-Instruct}, \texttt{CohereForAI/aya-expanse-8b}, and the proposed \texttt{turkish\_tokenizer}. Each tokenizer was assessed using a consistent set of linguistic and computational metrics introduced in \cite{bayram_tokenization_2025}. These metrics include total token count, vocabulary size, number of unique tokens, Turkish Token Percentage (TR~\%), and Pure Token Percentage (Pure~\%). TR~\% quantifies the proportion of tokens that correspond to valid Turkish words or morphemes, while Pure~\% measures the proportion of tokens that fully align with unambiguous root or affix boundaries, thus reflecting morphological integrity.

\begin{table}[htbp]
\centering
\caption{Performance of the proposed \texttt{turkish\_tokenizer} on the TR-MMLU dataset.}
\label{tab:turkish_tokenizer_results}
\begin{tabular}{|l|c|}
\hline
\rowcolor[HTML]{DDDDDD} 
\textbf{Metric} & \textbf{Value} \\ \hline
Vocabulary Size & 32,768 \\ \hline
Total Token Count & 707,727 \\ \hline
Processing Time (s) & 0.6714 \\ \hline
Unique Token Count & 11,144 \\ \hline
Turkish Token Count & 10,062 \\ \hline
Turkish Token Percentage (TR \%) & 90.29\% \\ \hline
Pure Token Count & 9,562 \\ \hline
Pure Token Percentage (Pure \%) & 85.80\% \\ \hline
\end{tabular}
\end{table}

The proposed \texttt{turkish\_tokenizer} demonstrated the highest linguistic alignment across all evaluated metrics. It achieved a TR~\% of 90.29\% and a Pure~\% of 85.80\%, substantially outperforming all competing tokenizers. In comparison, \texttt{google/gemma-2-9b} reached a TR~\% of only 40.96\% and a Pure~\% of 28.49\%, indicating that the majority of its tokens do not represent full morphemes. Similarly, \texttt{meta-llama/Llama-3.2-3B} produced a TR~\% of 45.77\% and a Pure~\% of 31.45\%, while \texttt{Qwen2.5} and \texttt{aya-expanse} achieved TR~\% values of 40.39\% and 53.48\%, respectively.

Despite employing significantly smaller vocabulary sizes, the proposed tokenizer demonstrated better linguistic segmentation. With a vocabulary of 32,768 tokens and 11,144 unique tokens used during evaluation, it balanced generalization and expressiveness more effectively than models such as \texttt{gemma-2-9b} and \texttt{aya-expanse}, which rely on vocabularies of over 255,000 tokens. These large-vocabulary tokenizers, rooted in frequency-based subword segmentation, tend to fragment morphologically rich expressions and introduce ambiguity in downstream tasks. In contrast, the morphological awareness of the \texttt{turkish\_tokenizer} enables semantically coherent token formation and more consistent syntactic parsing.

Although the total token count generated by the proposed tokenizer (707,727) exceeds those of the other models-for instance, \texttt{aya-expanse} produced 434,526 tokens-this increase is offset by gains in interpretability and linguistic fidelity. High TR~\% and Pure~\% scores suggest reduced reliance on spurious subword splits and improved preservation of morphosyntactic structure. This is particularly beneficial for tasks such as syntactic parsing, translation, summarization, and question answering, where semantic consistency across tokens is essential.

These findings support the hypothesis introduced in \cite{bayram_tokenization_2025}, which argues that high linguistic alignment in tokenization correlates strongly with downstream model performance in morphologically rich and low-resource languages. While conventional subword tokenizers may suffice for high-resource languages like English, they exhibit clear limitations in Turkish unless informed by morphological structure. The results presented here highlight the effectiveness of combining rule-based linguistic analysis with subword strategies to produce tokenizers that are both accurate and efficient in morphologically complex settings.

To illustrate the linguistic fidelity of different tokenization strategies, we present a qualitative comparison using the Turkish sentence:

\textit{"Atasözleri geçmişten günümüze kadar ulaşan anlamı bakımından mecazlı bir mana kazanan kalıplaşmış sözlerdir."} \\
(“Proverbs are fixed expressions passed down from the past to the present that acquire a metaphorical meaning in terms of their significance.”)

This sentence contains a wide range of morphological features, including compound words, multiple derivational and inflectional suffixes, and root forms that undergo phonological alternations. These properties make it an ideal test case for evaluating the morphological sensitivity of different tokenizers.

\vspace{1em}

\textbf{Proposed Hybrid Tokenizer:} \\
The hybrid morphological tokenizer segments the sentence into linguistically meaningful units with high fidelity. It produces: \\
\texttt{["<uppercase>", " atasöz", "ler", "i", " geçmiş", "ten", " günü", "m", "üz", "e", " kadar", " ulaş", "an", " anlam", "ı", " bakım", "ın", "dan", " mecaz", "lı", " bir", " mana", " kazan", "an", " kalıp", "laş", "mış", " sözle", "r", "dir", "."]} \\
It correctly separates suffixes (\texttt{"ler", "i", "ın", "dan", "lı", "an", "mış", "dir"}), extracts root forms such as \texttt{"atasöz", "gün", "mana"} with leading spaces to mark word boundaries, and employs the \texttt{"<uppercase>"} token to preserve orthographic case.

\vspace{1em}

\textbf{Tested newmindaiMursit Tokenizer:} \\
The hybrid morphological tokenizer segments the sentence into linguistically meaningful units with high fidelity. It produces: \\
\texttt{["At", "asöz", "leri", " geçmişten", " günümüze", " kadar", " ulaşan", " anlamı", " bakımından", " mec", "azlı", " bir", " man", "a", " kazanan", " kalıp", "laşmış", " söz", "lerdir", "."]} \\
Although it trained with Turkish text still does not capture most of suffixes, it fragments common roots (\texttt{"At", "asöz"} instead of \texttt{"atasöz"}) and fails to isolate inner morphemes in forms such as \texttt{"lerdir"} and \texttt{"kazanan"}, limiting morphological interpretability.

\vspace{1em}

\textbf{Tested cosmosGPT2 Tokenizer:} \\
The hybrid morphological tokenizer segments the sentence into linguistically meaningful units with high fidelity. It produces: \\
\texttt{["At", "as", "öz", "leri", " geçmişten", " günümüze", " kadar", " ulaşan", " anlamı", " bakımından", " mec", "az", "lı", " bir", " man", "a", " kazanan", " kalıp", "laşmış", " söz", "lerdir", "."]} \\
Although it trained with Turkish text still does not capture most of suffixes, it fragments common roots (\texttt{"At", "as", "öz"} instead of \texttt{"atasöz"}) and fails to isolate inner morphemes in forms such as \texttt{"lerdir"} and \texttt{"kazanan"}, limiting morphological interpretability.

\vspace{1em}

\textbf{Tested tabi Tokenizer:} \\
The hybrid morphological tokenizer segments the sentence into linguistically meaningful units with high fidelity. It produces: \\
\texttt{["A", "tasöz", "leri ", "geçmişten ", "günümüze kadar ", "ulaşan ", "anlamı ", "bakımından ", "mec", "az", "lı bir ", "mana ", "kazanan ", "kalıp", "laşmış", " söz", "lerdir", "."]} \\
Although it trained with Turkish text still does not capture most of suffixes, it fragments common roots (\texttt{"A", "tasöz"} instead of \texttt{"atasöz"}) and fails to isolate inner morphemes in forms such as \texttt{"lerdir"} and \texttt{"kazanan"}, limiting morphological interpretability.

\vspace{1em}

\textbf{Gemma-3:} \\
The tokenizer \texttt{google/gemma-3} segments the sentence as: \\
\texttt{["<bos>", "At", "as", "öz", "leri", " geçmiş", "ten", " gün", "ümü", "ze", " kadar", " ulaş", "an", " anlam", "ı", " bakım", "ından", " mec", "az", "lı", " bir", " mana", " kaz", "anan", " kal", "ı", "pla", "ş", "mış", " söz", "lerdir", "."]} \\
Although it captures some suffixes like \texttt{"ten"} and \texttt{"ından"}, it fragments common roots (\texttt{"At", "as", "öz"} instead of \texttt{"atasöz"}) and fails to isolate inner morphemes in forms such as \texttt{"lerdir"} and \texttt{"kazanan"}, limiting morphological interpretability.

\vspace{1em}

\textbf{LLaMA-3.2:} \\
The tokenizer \texttt{meta-llama/Llama-3.2-3B} yields: \\
\texttt{["<|begin\_of\_text|>", "At", "as", "öz", "leri", " geçmiş", "ten", " gün", "ümü", "ze", " kadar", " ", "ula", "ş", "an", " anlam", "ı", " bakımından", " me", "ca", "z", "lı", " bir", " mana", " kaz", "anan", " kal", "ı", "pla", "ş", "mış", " söz", "lerdir", "."]} \\
This tokenizer combines morphologically valid segments like \texttt{"bakımından"} and \texttt{"kazanan"} with fragmented roots like \texttt{"At", "as", "öz"}, creating inconsistency in morpheme alignment.

\vspace{1em}

\textbf{YTU Turkish GPT-2 (without pruned vocab to 32k):} \\
The tokenizer \texttt{ytu-ce-cosmos/turkish-gpt2-large-750m-instruct-v0.1}, trained on Turkish corpora, yields: \\
\texttt{["At", "as", "öz", "leri", " geçmişten", " günümüze", " kadar", " ulaşan", " anlamı", " bakımından", " mec", "az", "lı", " bir", " mana", " kazanan", " kalıp", "laşmış", " söz", "lerdir", "."]} \\
Although it still segments \texttt{"atasözleri"} incorrectly, it performs well with forms like \texttt{"geçmişten"}, \texttt{"günümüze"}, and \texttt{"bakımından"}, showing the advantage of Turkish-specific pretraining.

\vspace{1em}

\textbf{GPT-4o:} \\
The tokenizer \texttt{gpt-4o-o200k\_base} generates: \\
\texttt{["At", "as", "öz", "leri", " geçmiş", "ten", " gün", "ümü", "ze", " kadar", " ulaş", "an", " anlam", "ı", " bakım", "ından", " mec", "az", "lı", " bir", " mana", " kaz", "anan", " kal", "ı", "pla", "ş", "mış", " söz", "ler", "dir", "."]} \\
Its segmentation strategy is similar to LLaMA and Qwen-partially aware of Turkish morphemes but limited by frequent over-segmentation of compound and derived forms.

\vspace{1em}

The results presented in this section provide strong empirical support for the hypothesis introduced in the introduction: tokenizers that explicitly incorporate morphological and phonological knowledge of Turkish can outperform general-purpose models in both segmentation accuracy and linguistic coherence. While most state-of-the-art tokenizers struggle with root-fragmentation, over-segmentation, and inconsistent affix treatment, the proposed hybrid tokenizer consistently identifies morpheme boundaries, preserves semantically meaningful units, and reduces vocabulary redundancy. These findings validate the motivation behind this work: morphologically informed tokenization is essential for robust and interpretable NLP in agglutinative languages like Turkish. The qualitative comparisons presented here illustrate not only the performance gap between general and language-specific tokenizers, but also the need for tokenizer architectures that respect language-internal rules.

\textbf{Downstream Task Evaluation.}

To assess the impact of morphologically informed tokenization on downstream model performance, we evaluated the embeddings produced by models initialized with different tokenizers using three benchmarks: STSb-TR, MTEB-TR, and TurBLiMP. All models were initialized randomly to isolate the effect of tokenization structure from pre-training data.

Concretely, we construct four sentence embedding models that share the same encoder architecture (\texttt{google/embeddinggemma-300m}) and vocabulary size (32,768). Each model is randomly initialized with a fixed seed (42) and trained under an identical embedding-distillation objective; the only difference between models is the tokenizer and the corresponding pre-encoded \texttt{input\_ids} column used during training. We refer to these models as \texttt{*-random-init} to emphasize that they start from random weights rather than a pretrained checkpoint, so downstream differences primarily reflect inductive bias introduced by tokenization and decoding choices under a controlled training budget.

Training data comes from a Turkish text corpus with pre-computed teacher embeddings (\texttt{alibayram/cosmos-corpus-0-05-with-embeddings}). We prepare a unified dataset (\texttt{alibayram/cosmos-corpus-encoded}) that stores token ID sequences for all compared tokenizers (\texttt{mft\_input\_ids}, \texttt{tabi\_input\_ids}, \texttt{cosmos\_input\_ids}, \texttt{mursit\_input\_ids}). To ensure an apples-to-apples comparison under a fixed context window, we discard any sample for which \emph{any} tokenizer produces a sequence longer than 2048 tokens, so that no model benefits from truncation artifacts or sees different content due to length differences.

Because MFT is implemented as a custom Python tokenizer rather than a standard HuggingFace tokenizer, we use pre-encoded \texttt{input\_ids} and a lightweight ``tokenizer bypass'' patch in the \texttt{sentence-transformers} training stack so the same trainer loop can consume all four token streams without rewriting the model code. All downstream evaluations are run with fixed scripts: \texttt{evaluate\_sts\_tr.py} for STS, \texttt{mteb-tr/mteb\_tr\_cli.py} for MTEB-TR, and \texttt{evaluate\_turblimp.py} for TurBLiMP.

\begin{table}[H]
\centering
\caption{Embedding distillation experiment setup.}
\label{tab:distillation_setup}
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
Student architecture & \texttt{google/embeddinggemma-300m} (SentenceTransformer) \\
Initialization & Random weights with fixed seed 42; vocab resized to 32,768 \\
Training objective & Cosine embedding loss against teacher vectors \\
Teacher model & \texttt{intfloat/multilingual-e5-large-instruct} \\
Training corpus & \texttt{alibayram/cosmos-corpus-0-05-with-embeddings} \\
Training dataset & \texttt{alibayram/cosmos-corpus-encoded} with 4 \texttt{input\_ids} columns \\
Context length & 2048; samples dropped if any tokenizer exceeds the limit \\
Batch size / LR & 256 / $5 \times 10^{-5}$ \\
Schedule & Two-phase: 100-step warmup then 1 full epoch \\
Precision & BF16; gradient checkpointing enabled \\
Hardware & NVIDIA H100 80GB (single node) \\
\bottomrule
\end{tabular}
\end{table}

For STS, we use the Turkish STSb-TR benchmark (\texttt{figenfikri/stsb\_tr}) consisting of sentence pairs with human similarity ratings on a 0--5 scale \cite{beken_fikri_semantic_2021}. Each model encodes both sentences, we compute cosine similarity between the resulting sentence embeddings, and we report Pearson and Spearman correlation with the normalized gold scores. Throughout this section, correlations are presented as percentages ($\times 100$) for readability.

We evaluated the models on the Turkish STS benchmark (stsb-tr) without task-specific fine-tuning. The proposed \texttt{mft-random-init} model achieved a significantly higher correlation with human judgments compared to other randomly initialized baselines, demonstrating that its structural prior provides a better starting point for capturing semantic similarity.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/sts_benchmark_chart_test.png}
    \caption{STS benchmark test split performance.}
    \label{fig:sts_chart}
\end{figure}

On the STSb-TR test split ($n=1379$), the \texttt{mft-random-init} model achieved a Pearson correlation of \textbf{50.37\%} and Spearman correlation of \textbf{49.35\%}, consistently outperforming all baselines. In comparison, \texttt{newmindaiMursit-random-init} achieved 43.94\% Pearson / 43.75\% Spearman, \texttt{cosmosGPT2-random-init} reached 38.90\% Pearson / 38.02\% Spearman, and \texttt{tabi-random-init} scored 33.58\% Pearson / 33.24\% Spearman. Using a Fisher transform, the 95\% confidence interval for MFT test Pearson is [47.5, 53.1], compared to [30.9, 36.2] for Tabi, indicating that the gain is statistically robust at this sample size. On the training split ($n=5749$), MFT also remains ahead (Table~\ref{tab:sts_results}).

\begin{table}[H]
\centering
\caption{STS benchmark correlations across train and test splits.}
\label{tab:sts_results}
\begin{tabular}{llrrr}
\toprule
\textbf{Model} & \textbf{Split} & \textbf{Pearson} & \textbf{Spearman} & \textbf{Time (s)} \\
\midrule
MFT & test ($n=1379$) & \textbf{50.37} & \textbf{49.35} & 10.68 \\
Mursit & test ($n=1379$) & 43.94 & 43.75 & 7.67 \\
Cosmos & test ($n=1379$) & 38.90 & 38.02 & 7.52 \\
Tabi & test ($n=1379$) & 33.58 & 33.24 & 8.18 \\
\midrule
MFT & train ($n=5749$) & \textbf{53.34} & \textbf{51.31} & 47.48 \\
Mursit & train ($n=5749$) & 45.51 & 44.38 & 33.39 \\
Cosmos & train ($n=5749$) & 40.48 & 39.89 & 32.22 \\
Tabi & train ($n=5749$) & 38.20 & 37.44 & 36.31 \\
\bottomrule
\end{tabular}
\end{table}

To better understand the learning dynamics, we analyzed the performance evolution of each model across different training checkpoints. Figure \ref{fig:version_history_pearson} and Figure \ref{fig:version_history_spearman} illustrate the Pearson and Spearman correlations respectively at various stages of training (if applicable) or across collected versions. The x-axis represents sequential checkpoints ordered by date.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/version_history_pearson.png}
    \caption{Pearson correlation across model revisions.}
    \label{fig:version_history_pearson}
\end{figure}

Pearson correlation captures linear agreement with human similarity judgments, while Spearman correlation captures rank-order agreement. Reporting both is important in STS, since models may preserve relative similarity ordering even when the mapping is not perfectly linear, and conversely small linear gains may not reflect better ranking behavior.

In our version tracking, both correlations show the same qualitative trend: MFT remains ahead of the strongest baselines across revisions, indicating that the downstream improvement is stable rather than a single-run artifact. Importantly, all three baseline tokenizers (Mursit, Cosmos, Tabi) show consistent relative ordering, which suggests that the observed performance differences reflect genuine tokenizer-induced inductive bias rather than training variance. While we train with a single seed (42), the consistent MFT advantage across four independently tokenized model variants provides evidence of robustness.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/version_history_spearman.png}
    \caption{Spearman correlation across model revisions.}
    \label{fig:version_history_spearman}
\end{figure}

On the comprehensive MTEB suite, which covers retrieval, classification, clustering, and pair classification tasks, the MFT-based model achieves the strongest overall average among the compared random-initialized baselines.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.82\linewidth]{figures/mteb_average_scores.png}
    \caption{Average MTEB-TR scores.}
    \label{fig:mteb_average}
\end{figure}

\begin{table}[H]
\centering
\caption{MTEB-TR category averages.}
\label{tab:mteb_category_averages}
\begin{tabular}{lrrrr}
\toprule
\textbf{Category} & \textbf{MFT} & \textbf{Cosmos} & \textbf{Mursit} & \textbf{Tabi} \\
\midrule
BitextMining & \textbf{1.53} & 1.08 & 1.26 & 1.39 \\
Classification & \textbf{58.81} & 57.52 & 57.71 & 56.52 \\
Clustering & 66.30 & \textbf{66.45} & 65.39 & 65.71 \\
Other & \textbf{4.94} & 1.58 & 2.19 & 1.89 \\
Pair Classification & \textbf{50.25} & 47.94 & 46.89 & 47.43 \\
Retrieval & \textbf{28.94} & 20.10 & 21.12 & 18.46 \\
STS & \textbf{49.36} & 38.04 & 43.75 & 33.24 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Detailed MTEB-TR task-level performance.}
\label{tab:mteb_detailed}
\resizebox{\linewidth}{!}{
\begin{tabular}{lrrrr}
\toprule
\textbf{Task} & \textbf{MFT} & \textbf{Cosmos} & \textbf{Mursit} & \textbf{Tabi} \\
\midrule
\multicolumn{5}{l}{\textit{BitextMining}} \\
WMT16BitextMining & \textbf{1.53} & 1.08 & 1.26 & 1.39 \\
\multicolumn{5}{l}{\textit{Classification}} \\
THYSentimentClassification & 51.48 & 49.74 & \textbf{52.73} & 43.02 \\
TSTimelineNewsCategoryClassification & \textbf{50.06} & 43.09 & 44.33 & 44.09 \\
Turkish75NewsClassification & 73.33 & 78.67 & 74.67 & \textbf{79.33} \\
TurkishIronyClassification & 51.25 & 50.83 & \textbf{53.33} & 52.50 \\
TurkishMovieSentimentClassification & \textbf{54.74} & 54.37 & 54.57 & 53.84 \\
TurkishNewsCategoryClassification & \textbf{85.40} & 83.32 & 80.52 & 79.08 \\
TurkishOffensiveLanguageClassification & \textbf{49.87} & 48.74 & 49.71 & 48.22 \\
TurkishProductSentimentClassification & \textbf{54.34} & 51.39 & 51.85 & 52.10 \\
\multicolumn{5}{l}{\textit{Clustering}} \\
TurkishColumnWritingClustering & 66.30 & \textbf{66.45} & 65.39 & 65.71 \\
\multicolumn{5}{l}{\textit{Other}} \\
ArguAnaTR & \textbf{7.62} & 2.96 & 3.53 & 2.56 \\
FiQA2018TR & \textbf{6.74} & 1.53 & 2.78 & 2.38 \\
SCIDOCSTR & 0.47 & 0.27 & 0.27 & \textbf{0.74} \\
\multicolumn{5}{l}{\textit{Pair Classification}} \\
MnliTr & \textbf{48.46} & 45.32 & 45.67 & 44.98 \\
SnliTr & \textbf{44.73} & 40.29 & 40.29 & 40.04 \\
XNLI & 57.55 & \textbf{58.21} & 54.72 & 57.28 \\
\multicolumn{5}{l}{\textit{Retrieval}} \\
CQADupstackGamingRetrievalTR & \textbf{13.00} & 6.84 & 7.04 & 6.73 \\
MSMarcoTRRetrieval & \textbf{12.84} & 6.07 & 6.13 & 4.83 \\
NFCorpusTR & \textbf{1.22} & 0.40 & 0.51 & 0.73 \\
QuoraRetrievalTR & \textbf{63.01} & 46.44 & 49.24 & 46.98 \\
SciFactTR & \textbf{25.64} & 16.33 & 20.54 & 15.16 \\
SquadTRRetrieval & \textbf{16.53} & 8.07 & 8.93 & 6.14 \\
TQuadRetrieval & \textbf{43.46} & 29.97 & 29.48 & 26.30 \\
TurkishAbstractCorpusClustering & \textbf{47.46} & 43.63 & 42.81 & 39.69 \\
XQuADRetrieval & \textbf{37.33} & 23.16 & 25.38 & 19.54 \\
\multicolumn{5}{l}{\textit{STS}} \\
STSbTR & \textbf{49.36} & 38.04 & 43.75 & 33.24 \\
\bottomrule
\end{tabular}
}
\end{table}

As shown in Table \ref{tab:mteb_detailed}, the MFT-based model achieved an average score of \textbf{38.99\%} across 26 tasks, surpassing Mursit (34.98\%), Cosmos (34.43\%), and Tabi (33.33\%). Analyzing performance by category reveals distinct trade-offs. MFT demonstrates substantial advantages in \textit{STS} and \textit{Retrieval} tasks (e.g., TQuadRetrieval: 43.46\% vs 26.30\% for Tabi), which aligns with our hypothesis that morphology-aware segmentation improves the semantic quality of embeddings for similarity and search. At the same time, the Tabi baseline remains competitive or superior in specific \textit{Classification} tasks (e.g., Turkish75NewsClassification: 79.33\% vs 73.33\% for MFT) and \textit{BitextMining}, suggesting that different tokenization priors can favor different downstream regimes even under matched architecture and training protocol.


TurBLiMP provides Turkish minimal pairs designed to probe specific linguistic phenomena (e.g., agreement, scrambling, nominalization). Since our models are sentence embedding encoders (rather than generative language models), we evaluate an embedding-based proxy: for each minimal pair, we embed the grammatical and ungrammatical sentence and compute cosine similarity. Each category contains 1,000 minimal pairs in our evaluation. We report the average cosine similarity per phenomenon (shown as a percentage). Higher values indicate that the embedding representation is more invariant to these grammatical perturbations; this should not be interpreted as direct grammaticality classification accuracy.

\begin{table}[H]
\centering
\caption{TurBLiMP sensitivity scores by linguistic phenomenon.}
\label{tab:turblimp_detailed}
\resizebox{\linewidth}{!}{
\begin{tabular}{lrrrr}
\toprule
\textbf{Linguistic Phenomenon} & \textbf{Mursit} & \textbf{MFT} & \textbf{Cosmos} & \textbf{Tabi} \\
\midrule
Ellipsis & 98.0\% & \textbf{99.2\%} & 97.7\% & 93.0\% \\
Scrambling & 97.7\% & \textbf{98.0\%} & 97.3\% & \textbf{98.0\%} \\
Determiners & 94.9\% & \textbf{96.5\%} & 93.4\% & 94.1\% \\
Quantifiers & 90.2\% & 90.2\% & 86.3\% & \textbf{94.1\%} \\
Suspended Affixation & 89.5\% & \textbf{93.4\%} & 83.2\% & 91.0\% \\
Relative Clauses & 89.1\% & \textbf{96.1\%} & 86.3\% & 93.0\% \\
Binding & 88.7\% & 89.5\% & 85.5\% & \textbf{94.5\%} \\
Anaphor Agreement & 87.9\% & \textbf{92.6\%} & 84.8\% & \textbf{92.6\%} \\
Npi Licensing & 87.5\% & 87.5\% & 82.4\% & \textbf{89.5\%} \\
Irregular Forms & 87.1\% & 90.6\% & 80.1\% & \textbf{92.6\%} \\
Argument Structure Ditransitive & 86.3\% & 93.4\% & 80.9\% & \textbf{93.8\%} \\
Subject Verb Agreement & 84.8\% & \textbf{89.5\%} & 79.7\% & 86.3\% \\
Nominalization & 82.8\% & 87.5\% & 79.3\% & \textbf{89.8\%} \\
Argument Structure Transitive & 81.6\% & 90.6\% & 76.2\% & \textbf{91.0\%} \\
Passives & 81.6\% & 85.5\% & 79.3\% & \textbf{90.6\%} \\
Island Effects & 79.7\% & \textbf{84.0\%} & 76.2\% & \textbf{84.0\%} \\
\bottomrule
\end{tabular}
}
\end{table}

Across many categories, MFT yields higher similarity between minimal pairs than the general-purpose baselines (e.g., \textit{Relative Clauses} 96.1\% vs.\ 86.3\% for Cosmos). Under this embedding-based proxy, this suggests that morphology-first segmentation yields more stable semantic representations even when surface form is perturbed by controlled grammatical manipulations. A complementary evaluation of true grammatical sensitivity would require scoring minimal pairs with language model likelihood or a supervised acceptability classifier, which we leave for future work.

Finally, we tracked STS performance across multiple code revisions to ensure the observed gains are not a one-off artifact. Across all evaluated revisions, MFT remains ahead on average, and the best observed MFT revision reaches 50.45\% Pearson / 49.44\% Spearman on STSb-TR.

\FloatBarrier
