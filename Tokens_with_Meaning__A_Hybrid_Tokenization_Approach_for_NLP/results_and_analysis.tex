\section{Results and Analysis}
\label{sec:results}

\subsection{Metrics: TR\% and Pure\%}
\label{sec:metrics}

We report two linguistic alignment metrics introduced in \cite{bayram_tokenization_2025}. Let a tokenizer produce a token sequence $t_1,\dots,t_N$ for a corpus. TR~\% measures how many produced tokens correspond to Turkish lexical or morphemic units under a Turkish lexical resource:
\[
\mathrm{TR\%} = \frac{\sum_{i=1}^N \mathbb{1}[\mathrm{TurkishUnit}(t_i)]}{N}\times 100.
\]
Pure~\% measures how many produced tokens align with unambiguous root/affix boundaries:
\[
\mathrm{Pure\%} = \frac{\sum_{i=1}^N \mathbb{1}[\mathrm{PureUnit}(t_i)]}{N}\times 100.
\]
In our implementation, \texttt{turkish\_tokenizer} uses explicit root and affix vocabularies, so the set of ``TurkishUnit'' and ``PureUnit'' tokens is defined directly by these lexicons. For baseline tokenizers, these metrics are computed via the same evaluation pipeline from \cite{bayram_tokenization_2025}. While TR~\% and Pure~\% are interpretable diagnostics for segmentation quality, we complement them with downstream task evaluation in Section~\ref{sec:downstream}.

\subsection{Tokenization quality on TR-MMLU}
\label{sec:trmmlu}

The performance of the proposed morphological tokenizer was evaluated using the TR-MMLU benchmark dataset, which comprises over 1.6 million characters and approximately 200,000 words curated specifically for Turkish \cite{bayram_setting_2025}. This dataset is designed to reflect the linguistic complexity of Turkish, including its rich morphology, agglutinative structures, and diverse syntactic constructions. As such, it provides a rigorous basis for assessing tokenization quality in morphologically complex languages.

The evaluation compared five different tokenizers: \texttt{google/gemma-2-9b}, \texttt{meta-llama/Llama-3.2-3B}, \texttt{Qwen/Qwen2.5-7B-Instruct}, \texttt{CohereForAI/aya-expanse-8b}, and the proposed \texttt{turkish\_tokenizer}. Each tokenizer was assessed using a consistent set of linguistic and computational metrics introduced in \cite{bayram_tokenization_2025}. These metrics include total token count, vocabulary size, number of unique tokens, Turkish Token Percentage (TR~\%), and Pure Token Percentage (Pure~\%). TR~\% quantifies the proportion of tokens that correspond to valid Turkish words or morphemes, while Pure~\% measures the proportion of tokens that fully align with unambiguous root or affix boundaries, thus reflecting morphological integrity.

\begin{table}[h]
\centering
\caption{Performance of the proposed \texttt{turkish\_tokenizer} on the TR-MMLU dataset.}
\label{tab:turkish_tokenizer_results}
\begin{tabular}{|l|c|}
\hline
\rowcolor[HTML]{DDDDDD} 
\textbf{Metric} & \textbf{Value} \\ \hline
Vocabulary Size & 32,768 \\ \hline
Total Token Count & 707,727 \\ \hline
Processing Time (s) & 0.6714 \\ \hline
Unique Token Count & 11,144 \\ \hline
Turkish Token Count & 10,062 \\ \hline
Turkish Token Percentage (TR \%) & 90.29\% \\ \hline
Pure Token Count & 9,562 \\ \hline
Pure Token Percentage (Pure \%) & 85.80\% \\ \hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Linguistic alignment comparison on TR-MMLU. We report TR~\% and Pure~\% for tokenizers where the corresponding metric was computed in our evaluation pipeline; missing entries indicate the metric was not computed in this run.}
\label{tab:alignment_comparison}
\begin{tabular}{lcc}
\toprule
Tokenizer & TR (\%) & Pure (\%) \\
\midrule
turkish\_tokenizer (ours) & \textbf{90.29} & \textbf{85.80} \\
google/gemma-2-9b & 40.96 & 28.49 \\
meta-llama/Llama-3.2-3B & 45.77 & 31.45 \\
Qwen/Qwen2.5-7B-Instruct & 40.39 & -- \\
CohereForAI/aya-expanse-8b & 53.48 & -- \\
\bottomrule
\end{tabular}
\end{table}

The proposed \texttt{turkish\_tokenizer} demonstrated the highest linguistic alignment across all evaluated metrics. It achieved a TR~\% of 90.29\% and a Pure~\% of 85.80\%, substantially outperforming all competing tokenizers. In comparison, \texttt{google/gemma-2-9b} reached a TR~\% of only 40.96\% and a Pure~\% of 28.49\%, indicating that the majority of its tokens do not represent full morphemes. Similarly, \texttt{meta-llama/Llama-3.2-3B} produced a TR~\% of 45.77\% and a Pure~\% of 31.45\%, while \texttt{Qwen2.5} and \texttt{aya-expanse} achieved TR~\% values of 40.39\% and 53.48\%, respectively.

Despite employing significantly smaller vocabulary sizes, the proposed tokenizer demonstrated better linguistic segmentation. With a vocabulary of 32,768 tokens and 11,144 unique tokens used during evaluation, it balanced generalization and expressiveness more effectively than models such as \texttt{gemma-2-9b} and \texttt{aya-expanse}, which rely on vocabularies of over 255,000 tokens. These large-vocabulary tokenizers, rooted in frequency-based subword segmentation, tend to fragment morphologically rich expressions and introduce ambiguity in downstream tasks. In contrast, the morphological awareness of the \texttt{turkish\_tokenizer} enables semantically coherent token formation and more consistent syntactic parsing.

Although the total token count generated by the proposed tokenizer (707,727) exceeds those of the other models—for instance, \texttt{aya-expanse} produced 434,526 tokens—this increase is offset by gains in interpretability and linguistic fidelity. High TR~\% and Pure~\% scores suggest reduced reliance on spurious subword splits and improved preservation of morphosyntactic structure. This is particularly beneficial for tasks such as syntactic parsing, translation, summarization, and question answering, where semantic consistency across tokens is essential.

These tokenization results are consistent with the diagnostic perspective of Bayram et al.~\cite{bayram_tokenization_2025}: linguistic alignment metrics such as TR~\% and Pure~\% capture whether tokens correspond to coherent Turkish morphemes and lexical units. However, these diagnostics do not substitute for downstream evaluation. We therefore complement them with downstream sentence embedding evaluation results later in this section.

To illustrate the linguistic fidelity of different tokenization strategies, we present a qualitative comparison using an example sentence sampled from the TR-MMLU evaluation corpus \cite{bayram_setting_2025}:

\textit{"Atasözleri geçmişten günümüze kadar ulaşan anlamı bakımından mecazlı bir mana kazanan kalıplaşmış sözlerdir."} \\
(“Proverbs are fixed expressions passed down from the past to the present that acquire a metaphorical meaning in terms of their significance.”)

For example, the word \textit{atasözleri} can be glossed as:
\glossex{Example 2}{atasöz-ler-i}{proverb-\textsc{pl}-\textsc{3sg.poss}}{``his/her proverbs'' (surface form used in the sentence)}

This sentence contains a wide range of morphological features, including compound words, multiple derivational and inflectional suffixes, and root forms that undergo phonological alternations. These properties make it an ideal test case for evaluating the morphological sensitivity of different tokenizers.

\vspace{1em}

\textbf{Proposed Hybrid Tokenizer:} \\
The hybrid morphological tokenizer segments the sentence into linguistically meaningful units with high fidelity. It produces: \\
\texttt{["<uppercase>", "atasöz", "ler", "i", "<space>", "geçmiş", "ten", "<space>", "gün", "üm", "üz", "e", "<space>", "kadar", "<space>", "ulaş", "an", "<space>", "anlam", "ı", "<space>", "bakım", "ın", "dan", "<space>", "mecaz", "lı", "<space>", "bir", "<space>", "mana", "<space>", "kazan", "an", "<space>", "kalıp", "laş", "mış", "<space>", "sözle", "r", "dir", "."]} \\
It correctly separates suffixes (\texttt{"ler", "i", "ın", "dan", "lı", "an", "mış", "dir"}), extracts root forms such as \texttt{"atasöz", "gün", "mana"}, and employs special tokens like \texttt{"<uppercase>"} and \texttt{"<space>"} to preserve orthographic structure.

\vspace{1em}

\textbf{Gemma-3:} \\
The tokenizer \texttt{google/gemma-3} segments the sentence as: \\
\texttt{["<bos>", "At", "as", "öz", "leri", " geçmiş", "ten", " gün", "ümü", "ze", " kadar", " ulaş", "an", " anlam", "ı", " bakım", "ından", " mec", "az", "lı", " bir", " mana", " kaz", "anan", " kal", "ı", "pla", "ş", "mış", " söz", "lerdir", "."]} \\
Although it captures some suffixes like \texttt{"ten"} and \texttt{"ından"}, it fragments common roots (\texttt{"At", "as", "öz"} instead of \texttt{"atasöz"}) and fails to isolate inner morphemes in forms such as \texttt{"lerdir"} and \texttt{"kazanan"}, limiting morphological interpretability.

\vspace{1em}

\textbf{LLaMA-3.2:} \\
The tokenizer \texttt{meta-llama/Llama-3.2-3B} yields: \\
\texttt{["<|begin\_of\_text|>", "At", "as", "öz", "leri", " geçmiş", "ten", " gün", "ümü", "ze", " kadar", " ", "ula", "ş", "an", " anlam", "ı", " bakımından", " me", "ca", "z", "lı", " bir", " mana", " kaz", "anan", " kal", "ı", "pla", "ş", "mış", " söz", "lerdir", "."]} \\
This tokenizer combines morphologically valid segments like \texttt{"bakımından"} and \texttt{"kazanan"} with fragmented roots like \texttt{"At", "as", "öz"}, creating inconsistency in morpheme alignment.

\vspace{1em}

\textbf{Qwen2.5:} \\
The tokenizer \texttt{Qwen/Qwen2.5} outputs: \\
\texttt{["At", "as", "öz", "leri", " geçmiş", "ten", " gün", "üm", "ü", "ze", " kadar", " ulaş", "an", " anlamı", " bakım", "ından", " me", "ca", "z", "lı", " bir", " mana", " kaz", "anan", " kal", "ı", "pla", "ş", "mış", " söz", "ler", "dir", "."]} \\
While suffixes such as \texttt{"ten"} and \texttt{"ından"} are recognized, the tokenizer introduces redundant splits like \texttt{"üm", "ü", "ze"}, reducing the linguistic coherence of the token stream.

\vspace{1em}

\textbf{Aya-Expanse:} \\
The tokenizer \texttt{CohereForAI/aya-expanse} returns: \\
\texttt{["<BOS\_TOKEN>", "At", "as", "öz", "leri", " geçmiş", "ten", " günümüze", " kadar", " ulaşan", " anlamı", " bakımından", " mec", "az", "lı", " bir", " mana", " kazanan", " kalı", "pl", "aş", "mış", " söz", "lerdir", "."]} \\
It retains some complete word forms such as \texttt{"günümüze"} and \texttt{"ulaşan"}, but still fragments compounds like \texttt{"kalıplaşmış"} and splits the root \texttt{"atasöz"}, reducing morphological traceability.

\vspace{1em}

\textbf{Phi-4:} \\
The tokenizer \texttt{microsoft/phi-4} produces: \\
\texttt{["At", "as", "ö", "z", "leri", " geç", "mi", "ş", "ten", " gün", "üm", "ü", "ze", " kadar", " ", "ula", "ş", "an", " an", "lam", "ı", " bak", "ım", "ından", " me", "ca", "z", "lı", " bir", " mana", " kaz", "anan", " kal", "ı", "pla", "ş", "m", "ış", " sö", "z", "ler", "dir", "."]} \\
This tokenizer over-fragments even basic stems like \texttt{"geçmiş"} into \texttt{"geç", "mi", "ş"} and \texttt{"anlam"} into \texttt{"an", "lam"}, increasing token count and reducing interpretability.

\vspace{1em}

\textbf{YTU Turkish GPT-2:} \\
The tokenizer \texttt{ytu-ce-cosmos/turkish-gpt2-large-750m-instruct-v0.1}, trained on Turkish corpora, yields: \\
\texttt{["At", "as", "öz", "leri", " geçmişten", " günümüze", " kadar", " ulaşan", " anlamı", " bakımından", " mec", "az", "lı", " bir", " mana", " kazanan", " kalıp", "laşmış", " söz", "lerdir", "."]} \\
Although it still segments \texttt{"atasözleri"} incorrectly, it performs well with forms like \texttt{"geçmişten"}, \texttt{"günümüze"}, and \texttt{"bakımından"}, showing the advantage of Turkish-specific pretraining.

\vspace{1em}

\textbf{GPT-4o:} \\
The tokenizer \texttt{gpt-4o-o200k\_base} generates: \\
\texttt{["At", "as", "öz", "leri", " geçmiş", "ten", " gün", "ümü", "ze", " kadar", " ulaş", "an", " anlam", "ı", " bakım", "ından", " mec", "az", "lı", " bir", " mana", " kaz", "anan", " kal", "ı", "pla", "ş", "mış", " söz", "ler", "dir", "."]} \\
Its segmentation strategy is similar to LLaMA and Qwen—partially aware of Turkish morphemes but limited by frequent over-segmentation of compound and derived forms.

\vspace{1em}

Overall, the qualitative comparisons show a consistent pattern: general-purpose tokenizers often fragment frequent Turkish roots and conflate or split suffix material inconsistently, whereas the proposed tokenizer aims to preserve morpheme boundaries and treat common allomorphic variants as shared identifiers. We next evaluate whether these segmentation differences translate into measurable downstream quality on Turkish semantic similarity and retrieval benchmarks.

\subsection{Downstream Sentence Embedding Evaluation (STS and MTEB-TR)}
\label{sec:downstream}

\paragraph{Benchmarks.}
We evaluate downstream quality using (i) Turkish Semantic Textual Similarity (STSb-TR) with Pearson/Spearman correlation, a standard evaluation for sentence embeddings \cite{beken_fikri_semantic_2021}, and (ii) MTEB-TR-style benchmarks spanning retrieval, classification, clustering, and STS tasks \cite{mteb_massive_text_embedding_benchmark}.

\paragraph{Training and comparison protocol.}
To compare tokenizers under a controlled budget, we train sentence embedding models using a teacher-guided embedding alignment objective: student models are trained to match fixed teacher embedding vectors for the same text. This setting isolates tokenizer effects in a downstream-relevant representation learning task while avoiding the cost of online teacher inference. Our released training protocol uses a maximum sequence length of 2,048 tokens, cosine embedding loss, mean pooling, and bf16 training with gradient checkpointing. We report the complete experimental configuration (hardware, hyperparameters, dataset schema, filtering rules) in the repository artifact \texttt{TRAINING\_DETAILS.md}.

\paragraph{Tokenizer comparison and baselines.}
We compare models trained under the same training budget with two tokenization settings:
the proposed morphology-first tokenizer (MFT) vs.\ a strong Turkish subword baseline (Tabi). To isolate tokenizer effects, we report results within the same training recipe and include random-initialized baselines as sanity checks.

\input{downstream_tables}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/sts_benchmark_chart_test.png}
\caption{STS benchmark test split results across training steps (from repository artifact \texttt{STS\_BENCHMARK\_RESULTS.md}).}
\label{fig:sts_chart}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{figures/mteb_average_scores.png}
\caption{Overall average MTEB-TR scores across evaluated models (from repository artifact \texttt{MTEB\_BENCHMARK\_RESULTS.md}).}
\label{fig:mteb_avg}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.49\linewidth]{figures/version_history_pearson.png}\hfill
\includegraphics[width=0.49\linewidth]{figures/version_history_spearman.png}
\caption{Version-history robustness on STSb-TR (Pearson/Spearman), from \texttt{VERSION\_BENCHMARK\_RESULTS.md}.}
\label{fig:version_history}
\end{figure}

\paragraph{Key findings.}
On STSb-TR, MFT yields a substantial improvement over the Tabi baseline: the best MFT model reaches 74.41\% Pearson vs.\ 66.29\% for the best Tabi model (+8.12 points), and the gap persists under the random-initialized sanity check (47.09\% vs.\ 40.53\%, +6.56 points) (Table~\ref{tab:sts_results}, Figure~\ref{fig:sts_chart}). On MTEB-TR, results are closer: the best Tabi model has a slightly higher overall average (62.59\% vs.\ 62.09\%), while MFT has the best STS category average (74.73\% vs.\ 72.41\%) and a markedly stronger random-initialized baseline (38.99\% vs.\ 33.33\%) (Table~\ref{tab:mteb_summary}). Taken together, these results support two points: (i) morphology-first tokenization is consistently beneficial for semantic similarity, and (ii) when learning must start from scratch, Tabi-style subword tokenization does not close the gap to MFT within the same training budget.

\paragraph{Efficiency Analysis.}
Evaluation time on the STSb-TR test set serves as a proxy for tokenizer efficiency. The pure-Python implementation of the MFT tokenizer incurs a slight overhead compared to the Rust-based Tabi baseline: average inference time for MFT models was $\approx 22.7$s versus $\approx 21.1$s for Tabi models (a $\sim$7\% increase). This minor latency cost is balanced by the significant gains in semantic alignment, particularly for applications where representation quality outweighs sub-millisecond throughput.

We additionally track performance across multiple saved model checkpoints and report version-history plots in the repository (\texttt{VERSION\_BENCHMARK\_RESULTS.md}). The best-performing checkpoint in our runs is \texttt{mft-downstream-task-embeddingmagibu} with 76.10\% Pearson (Table~\ref{tab:sts_results} summarizes the main comparison point; see the repository report for full history).

\subsection{Linguistic Sensitivity (TurBLiMP)}
To evaluate whether our morphology-aware tokenizer better preserves syntactic sensitivity, we tested the models on the TurBLiMP benchmark \cite{turblimp}. Since our models are fine-tuned for semantic similarity (Sentence Transformers) rather than masked language modeling opacity, we adapted the evaluation to measure the \textbf{Cosine Similarity} between minimal pairs (grammatical vs. ungrammatical). 
Lower cosine similarity indicates that the model distinguishes the ungrammatical variation more sharply as a distinct semantic/syntactic event.

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Phenomenon} & \textbf{MFT (Magibu)} & \textbf{Tabi (Magibu)} \\
\midrule
Anaphor Agreement & 0.9844 & - \\
Arg. Struct. (Ditransitive) & - & - \\
\bottomrule
\end{tabular}
\caption{Average Cosine Similarity on TurBLiMP minimal pairs. Lower scores indicate higher sensitivity to grammatical errors.}
\label{tab:turblimp_sensitivity}
\end{table}

Preliminary results (Table~\ref{tab:turblimp_sensitivity}) on Anaphor Agreement show a high similarity (0.9844) for MFT, suggesting that the model remains robust to minor morphological errors, likely preserving the broad semantic intent. Further comparison with baseline tokenizers is required to determine if this is a feature of the embedding space or a specific property of the tokenizer.
