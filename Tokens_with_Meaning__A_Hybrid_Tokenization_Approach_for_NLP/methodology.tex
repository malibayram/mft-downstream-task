\section{Methodology}
\label{sec:methodology}

Our tokenizer is designed for morphologically rich, predominantly concatenative systems (e.g., Turkish) where many grammatical functions are expressed through productive suffixation and morphophonological alternations. The key design choice is \emph{morphology-first tokenization}: we prioritize roots and affixes as atomic units when possible, while using subwords only as a fallback for coverage.

\subsection{Resources: Roots, Affixes, and Subwords}
\label{sec:resources}

The tokenizer vocabulary is the union of three components:
\begin{itemize}
  \item \textbf{Root lexicon} (\texttt{kokler.json}): a curated list of Turkish roots and high-frequency lexicalized forms (including selected compounds). Each root may have multiple surface variants mapped to a shared identifier to control vocabulary growth.
  \item \textbf{Affix lexicon} (\texttt{ekler.json}): a curated list of productive suffixes and function markers. Allomorphic variants (e.g., vowel harmony and consonant alternations) are grouped to share identifiers.
  \item \textbf{Subword fallback} (\texttt{bpe\_tokenler.json}): a list of frequent subword units used only when dictionary segmentation fails. In our release, these units are derived from a Turkish subword training run and exported as a lookup table.\footnote{The training corpora include public Turkish corpora hosted on HuggingFace (e.g., \cite{hf_umarigan_turkish_corpus_small, hf_kadirnar_combined_turkish_datasets_v4}).}
\end{itemize}

We additionally include special tokens for whitespace and case handling, notably \texttt{<uppercase>} and an explicit space token.

\paragraph{Identifier ranges and allomorph sets.}
Our released vocabulary uses disjoint identifier ranges for clarity:
(i) roots: ids 0--19999, (ii) affixes: ids 20000--20071, and (iii) subwords: ids 20072--32767.
Importantly, the lexicons map multiple surface strings to shared identifiers to encode allomorph equivalence classes.
In the current release, \texttt{kokler.json} contains 22,231 surface strings mapped to 20,000 root identifiers, and \texttt{ekler.json} contains 177 surface strings mapped to 72 affix identifiers (a direct measure of allomorph unification). The subword fallback consists of 12,696 distinct subword units.

\begin{table}[t]
\centering
\caption{Vocabulary composition in the released Turkish tokenizer. ``Strings'' counts surface forms in JSON files; ``IDs'' counts unique identifiers (i.e., equivalence classes).}
\label{tab:vocab_comp}
\begin{tabular}{lrrr}
\toprule
Component & Strings & IDs & ID range \\
\midrule
Roots (\texttt{kokler.json}) & 22{,}231 & 20{,}000 & 0--19{,}999 \\
Affixes (\texttt{ekler.json}) & 177 & 72 & 20{,}000--20{,}071 \\
Subwords (\texttt{bpe\_tokenler.json}) & 12{,}696 & 12{,}696 & 20{,}072--32{,}767 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Dictionary Construction and Curation}
\label{sec:dict_construction}

The root lexicon is designed to cover high-frequency lexical material while keeping identifiers stable across surface variants. In practice, this involves (i) collecting high-frequency Turkish word types from large corpora, (ii) filtering and normalizing candidates (e.g., handling Turkish dotted/dotless \textit{i}), and (iii) optionally adding lexicalized multiword compounds as atomic entries when they behave as stable meaning units in the target corpus. This curation improves robustness and interpretability but introduces a key trade-off: the lexicon is language-specific and requires maintenance and domain adaptation.

The affix lexicon encodes productive suffixes and function markers. Each affix identifier can correspond to multiple surface allomorphs. For example, vowel harmony yields alternations over \{a, e\} and \{ı, i, u, ü\}, and consonant hardness can yield alternations such as \textit{d/t} or \textit{k/ğ} (e.g., \textit{kitap} vs.\ \textit{kitabı}; \textit{çocuk} vs.\ \textit{çocuğu}). Instead of treating each surface form as a distinct token, we group them into equivalence classes and select the appropriate surface form at decode time based on phonological context.

\subsection{Normalization and Allomorph Unification}
\label{sec:normalization}

Turkish suffixes and some roots exhibit systematic surface variation driven by vowel harmony and consonant alternations. Our vocabulary therefore supports \emph{many-to-one} mappings: multiple surface strings can map to the same token identifier. During decoding, the appropriate surface allomorph is selected based on phonological context (front/back and rounded/unrounded vowel harmony; consonant hardness).

We explicitly model common Turkish vowel harmony patterns for suffix selection, including alternations over \{a, e\} and \{ı, i, u, ü\}. We also treat \textit{y} as a \emph{vowel or semivowel} in relevant boundary contexts (e.g., buffer consonant behavior), following standard Turkish morphophonology.

\subsection{Encoding Algorithm}
\label{sec:encoding}

The encoder operates by longest-prefix matching against the three vocabularies in a fixed order (roots $\rightarrow$ affixes $\rightarrow$ subwords). It also inserts explicit markers for whitespace and capitalization.

\begin{center}
\fbox{\begin{minipage}{0.96\linewidth}
\textbf{Algorithm 1: Morphology-first hybrid encoding}\\[0.25em]
\textbf{Input:} text string $x$\\
\textbf{Output:} token id sequence $y$\\[0.25em]
1. Split $x$ into whitespace-separated segments; represent spaces explicitly.\\
2. For each segment $w$:
   (a) Split camelCase/PascalCase boundaries (e.g., \texttt{HTTPServer} $\rightarrow$ \texttt{H T T P Server}).\\
   (b) For each subsegment starting with an uppercase letter, emit \texttt{<uppercase>} and lowercase the subsegment.\\
   (c) Scan left-to-right; at each position, emit the longest matching prefix from (roots, then affixes, then subwords).\\
   (d) If no match exists, emit \texttt{<unknown>} and advance by one character.\\
3. Return the concatenated ids.
\end{minipage}}
\end{center}

This procedure is deterministic and designed to be reproducible from the released lookup tables. We use a longest-match heuristic over the \emph{surface form}. This choice improves robustness in practice, but it can diverge from lexical (underlying) analyses in ambiguous cases; we discuss this limitation in Section~\ref{sec:future_work}.

\begin{figure}[H]
\centering
\resizebox{\textwidth}{0.88\textheight}{% Reduced height from full to 88%
\begin{tikzpicture}[node distance=1.3cm and 1.2cm, every node/.style={transform shape, scale=0.85}]

\tikzstyle{process} = [rectangle, minimum width=2.8cm, text width=2.8cm, minimum height=1.2cm, text centered, draw=black, fill=blue!10]
\tikzstyle{decision} = [diamond, aspect=2, text width=3cm, text centered, draw=black, fill=yellow!20]
\tikzstyle{arrow} = [thick,->,>=stealth]

% Nodes
\node[process] (input_node) {Input};
\node[decision, below=of input_node] (special_check) {Is there any special tokens?};
\node[process, below left=1.3cm and 0.5cm of special_check] (add_special) {Add special token to token list};
\node[process, below right=1.3cm and 0.5cm of special_check] (process_word) {Process word segment};
\node[decision, below=1.3cm of process_word] (root_check) {Is the word in root list?};
\node[process, below left=1.3cm and 0.5cm of root_check] (add_root_id) {Add ID of root to token list};
\node[process, below right=1.3cm and 0.5cm of root_check] (iterate_root) {Iterate through word to find longest matched root};
\node[decision, below=1.3cm of iterate_root] (root_found_check) {Is the root found?};
\node[process, below left=1.3cm and 0.3cm of root_found_check] (check_suffixes) {Check suffixes};
\node[process, below right=2.5cm and 0.8cm of root_found_check] (try_bpe) {Try BPE segmentation};
\node[decision, below=1.3cm of check_suffixes] (suffixes_found_check) {Are suffixes found?};
\node[process, below left=1.3cm and 0.3cm of suffixes_found_check] (add_root_suffix_ids) {Add IDs of root and suffixes to token list};
\node[decision, below right=1.3cm and 0.3cm of suffixes_found_check] (remainder_root_check) {Is remainder a root?};
\node[process, below left=1.3cm and 0.3cm of remainder_root_check] (add_root_id_rem) {Add ID of root to token list};

% Arrows
\draw[arrow] (input_node) -- (special_check);
\draw[arrow] (special_check) -- node[above left, pos=0.4] {Yes} (add_special);
\draw[arrow] (special_check) -- node[above right, pos=0.4] {No} (process_word);
\draw[arrow] (process_word) -- (root_check);
\draw[arrow] (root_check) -- node[above left, pos=0.4] {Yes} (add_root_id);
\draw[arrow] (root_check) -- node[above right, pos=0.4] {No} (iterate_root);
\draw[arrow] (iterate_root) -- (root_found_check);
\draw[arrow] (root_found_check) -- node[above left, pos=0.4] {Yes} (check_suffixes);
\draw[arrow] (root_found_check.east) -| node[above, pos=0.3] {No} (try_bpe.north);
\draw[arrow] (check_suffixes) -- (suffixes_found_check);
\draw[arrow] (suffixes_found_check) -- node[above left, pos=0.4] {Yes} (add_root_suffix_ids);
\draw[arrow] (suffixes_found_check) -- node[above right, pos=0.4] {No} (remainder_root_check);
\draw[arrow] (remainder_root_check) -- node[above left, pos=0.4] {Yes} (add_root_id_rem);
\draw[arrow] (remainder_root_check) -- node[above right, pos=0.4] {No} (try_bpe);

\end{tikzpicture}%
}
\caption{Tokenization decision flow with root, suffix, and fallback segmentation logic.}
\label{fig:decision_flow}
\end{figure}

\subsection{Decoding and Surface Form Reconstruction}
\label{sec:decoding}

Decoding maps token identifiers back to text using reverse vocabularies and morphophonological rules. If an id corresponds to an allomorph set (multiple surface strings), the decoder selects the surface form based on the preceding lexical material (vowel harmony; consonant hardness). For the \texttt{<uppercase>} marker, the decoder capitalizes the subsequent token.

This design enables near-lossless reconstruction for typical Turkish text where unknown tokens are rare and capitalization is mostly word-initial. However, full losslessness is not guaranteed: unknown tokens are rendered as placeholders, and sequences of all-caps letters (e.g., acronyms) may be imperfectly reconstructed under the current marker design. We treat this as an explicit edge case for future refinement.

\paragraph{Decoder rules (implementation summary).}
Our public decoder (\texttt{turkish\_decoder.py}) implements rule-based selection for affix allomorphs based on the most recent vowel in the preceding token (front/back; rounded/unrounded) and the hardness of the final consonant (e.g., \textit{d/t} alternations). In addition, a small set of root identifiers are associated with multiple surface root forms (e.g., variants triggered by vowel-initial suffixes), and the decoder selects among them based on whether the next token begins with a vowel. These rules are intentionally lightweight: they do not attempt full morphological disambiguation, and they operate on local context rather than syntactic structure.

\paragraph{Complexity.}
The encoder performs longest-prefix matching over bounded prefix lengths, making runtime approximately linear in input length with a constant factor determined by maximum token string lengths in each lexicon. In practice, this enables efficient tokenization while preserving the transparency of a rule-based pipeline.

\subsection{Worked Example with Leipzig-Style Glossing}
\label{sec:glossing}

We illustrate morphology-first segmentation on a multi-morpheme Turkish word:

\glossex{Example 1}{anla-yabil-dik-ler-imiz-den}{understand-\textsc{abil}-\textsc{nmlz}-\textsc{pl}-\textsc{1pl.poss}-\textsc{abl}}{``from what we were able to understand''}

This example highlights two goals: (i) preserve the root as a stable unit, and (ii) isolate productive suffixes so that grammatical meaning is represented compositionally rather than through arbitrary subword fragments.
We follow standard Leipzig-style conventions for interlinear glossing \cite{leipzig_glossing_rules}.

\begin{figure}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[node distance=1.5cm and 0.7cm, every node/.style={transform shape, scale=0.95}]
\tikzset{
  mystep/.style={
    rectangle, rounded corners, draw=black, fill=blue!10,
    minimum height=1.1cm, minimum width=2.8cm,
    text centered, align=center
  },
  arrow/.style={->, thick, >=stealth}
}

\node[mystep] (text) {Input Text \\ \texttt{"Kalktığımızda hep birlikte yürüdük."}};
\node[mystep, below=of text] (tokenized) {Tokenized \\ \texttt{[uppercase], kalk, tığ, ımız, da, [space], hep, [space], birlikte, [space], yürü, dü, k, .}};
\node[mystep, below=of tokenized] (ids) {Token IDs \\ \texttt{0, 1502, 22280, 22285, 22278, 1, 2300, 1, 4803, 1, 2280, 22296, 22617, 22582}};
\node[mystep, below=1.7cm of ids] (reconstruct) {Decoded Tokens};
\node[mystep, below=of reconstruct] (finaltext) {Output Text \\ \texttt{"Kalktığımızda hep birlikte yürüdük."}};

\draw[arrow] (text) -- (tokenized) node[midway, right] {Encoding};
\draw[arrow] (tokenized) -- (ids);
\draw[arrow] (ids) -- (reconstruct) node[midway, right] {Decoding};
\draw[arrow] (reconstruct) -- (finaltext);

\end{tikzpicture}%
}
\caption{Encoding and decoding process for the sentence “Kalktığımızda hep birlikte yürüdük.”}
\label{fig:encoding_process}
\end{figure}
