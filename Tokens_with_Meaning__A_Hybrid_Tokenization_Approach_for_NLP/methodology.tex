\section{Methodology}
\label{sec:methodology}

We propose a hybrid tokenization framework that combines linguistic knowledge with statistical subword segmentation. This approach, the \textit{Morphology-First Tokenizer} (MFT), integrates rule-based morphological analysis with a structured dictionary of roots and affixes while incorporating BPE to handle out-of-vocabulary (OOV) terms. The objective is to create a tokenization system that accurate represents linguistic structures while maintaining computational efficiency.

\begin{figure}[t]
\centering
\resizebox{0.98\linewidth}{!}{%
\begin{tikzpicture}[
  font=\scriptsize,
  node distance=6mm and 8mm,
  every node/.style={align=center, transform shape},
  arrow/.style={thick,->,>=Stealth}
]

\tikzset{
  startstop/.style={ellipse, draw, fill=gray!10, minimum height=6mm, minimum width=15mm},
  process/.style={rounded corners, rectangle, draw, fill=orange!20, minimum height=7mm, text width=3.6cm},
  decision/.style={diamond, draw, fill=green!15, aspect=2.0, text width=3.1cm, inner sep=1pt},
  io/.style={trapezium, trapezium left angle=70, trapezium right angle=110, draw, fill=blue!10, minimum height=7mm, text width=3.6cm}
}

\node[startstop] (start) {Start};
\node[io, below=of start] (scan) {Scan input\\(segments: word / special)};

\node[decision, below=of scan] (special) {Special segment?};
\node[process, left=of special] (emit_special) {Emit special token};

\node[process, below=of special] (prep) {If capitalized: emit \texttt{<uppercase>}\\Lowercase word; normalize variants};

\node[decision, below=of prep] (root) {Root+suffix\\analysis succeeds?};
\node[process, left=of root] (emit_morph) {Emit root ID\\+ suffix IDs};
\node[process, right=of root] (emit_bpe) {BPE fallback\\(else \texttt{<unk>})};

\node[startstop, below=10mm of root] (end) {Next / End};

\draw[arrow] (start) -- (scan);
\draw[arrow] (scan) -- (special);
\draw[arrow] (special) -- node[above, pos=0.5] {Yes} (emit_special);
\draw[arrow] (emit_special) |- (end);
\draw[arrow] (special) -- node[right, pos=0.45] {No} (prep);

\draw[arrow] (prep) -- (root);
\draw[arrow] (root) -- node[above, pos=0.5] {Yes} (emit_morph);
\draw[arrow] (root) -- node[above, pos=0.5] {No} (emit_bpe);
\draw[arrow] (emit_morph) |- (end);
\draw[arrow] (emit_bpe) |- (end);

\end{tikzpicture}%
}
\caption{Simplified algorithmic flow of MFT: segment scan, special/case handling, morphology-first tokenization, and BPE fallback.}
\label{fig:flowchart}
\end{figure}

We provide a Python reference implementation of the tokenizer and release the lexical resources (root and affix inventories) and decoder rules used in our experiments.

\textbf{Dictionary Construction.} The core of MFT is a dual-dictionary system designed to cover the productive morphology of Turkish.

\textbf{Root Dictionary.} The root dictionary is constructed from high-frequency words extracted from large-scale Turkish corpora, comprising approximately 22,000 roots. To address the challenge of phonological alternation, we employ a normalization strategy where surface variants map to a single canonical root ID. For example, consonant alternation causes \textit{kitap} (book) and \textit{kitabı} (its book) to share the same root ID despite the $p \to b$ softening; vowel hiatus maps \textit{oyna} (play) and \textit{oynuyor} (playing, where `a' drops) to a unified token; and haplology treats \textit{alın} (forehead) and \textit{alnı} (his forehead) identically. We also explicitly tokenize frequent compound words (e.g., \textit{akarsu} `stream', \textit{çamaşırhane} `laundromat') as single units to prevent erroneous splitting.

\textbf{Affix Dictionary.} The affix inventory consists of approximately 230 grammatical morphemes (suffixes, prepositions, conjunctions). Similar to roots, we merge allomorphs that serve identical grammatical functions into shared IDs. For instance, the plural suffix \textit{-ler} and its harmonic variant \textit{-lar} are assigned a single token ID (e.g., \textsc{PL}), as are the ablative variants \textit{-den, -dan, -ten, -tan}. This abstraction reduces vocabulary redundancy while preserving the morphosyntactic signal.

\textbf{Input Normalization and Special Tokens.} To ensure robustness across diverse text inputs, we implement strict normalization rules. For case handling, we introduce an \texttt{<uppercase>} token to mark capitalized words, allowing the model to process \textit{Kitap} and \textit{kitap} using the same root embedding and effectively halving the number of required surface forms. For CamelCase splitting, technical terms and code-switching often introduce CamelCase (e.g., `HTTPServer), which we split into constituent parts (`HTTP, `Server) before tokenization to improve subword coverage. For whitespace and punctuation, explicit tokens are used for spaces, tabs, and newlines, ensuring that formatting is lossless and reversible.

\textbf{Encoding Algorithm.} The encoding process (Algorithm \ref{alg:encoding}) follows a ``longest-prefix match'' strategy. For each word, the tokenizer first attempts to identify a valid root from the dictionary. If a root is found, it greedily matches the longest chain of valid suffixes.

\begin{algorithm}
\caption{Morphology-First Tokenization Pipeline}
\label{alg:encoding}
\begin{algorithmic}[1]
\State \textbf{Input:} Raw text string $S$
\State \textbf{Output:} Sequence of Token IDs $T$
\State $S \gets \text{Preprocess}(S)$ \Comment{Insert spaces, split CamelCase}
\For{each word $w$ in $S$}
    \If{$w$ is Space or Punctuation}
        \State $T.\text{append}(\text{GetSpecialID}(w))$
        \State \textbf{continue}
    \EndIf
    \If{$w$ is Capitalized}
        \State $T.\text{append}(\text{ID}_{\text{uppercase}})$
        \State $w \gets w.\text{lower}()$
    \EndIf
    \State $root, suffixes \gets \text{MorphAnalyze}(w)$ \Comment{Greedy Dictionary Search}
    \If{$root \neq \text{None}$}
        \State $T.\text{append}(root.\text{id})$
        \For{$s$ in $suffixes$}
            \State $T.\text{append}(s.\text{id})$
        \EndFor
    \Else
        \State $subwords \gets \text{BPE}(w)$ \Comment{Fallback}
        \State $T.\text{extend}(subwords)$
    \EndIf
\EndFor
\State \textbf{Return} $T$
\end{algorithmic}
\end{algorithm}

If the morphological analyzer fails to cover the word (i.e., no valid root+suffix combination is found), the system falls back to a BPE model. This ensures that the tokenizer remains open-vocabulary and can handle foreign entities or neologisms. The BPE model is trained on a version of the corpus where known morphological segments are masked, focusing its vocabulary (approx. 10,000 tokens) on residual stems and subwords.

\textbf{Example.} Consider the validation sentence: ``\textit{Kalktığımızda hep birlikte yürüdük.}'' (When we stood up, we walked together.)

\glossex{\texttt{<upper>} kalk-tığ-ımız-da \texttt{<space>} hep \texttt{<space>} birlik-te \texttt{<space>} yürü-dü-k .}
{CAPS stand-NMZ-POSS.1PL-LOC \_ always \_ unity-LOC \_ walk-PST-1PL .}
{When we stood up, we walked together.}
{Kalktığımızda hep birlikte yürüdük.}

Here, the tokenizer correctly identifies the root \textit{kalk} (stand) and segments the complex nominalization chain \textit{-tık-ımız-da} (represented by phonologically normalized abstract suffixes).

\textbf{Decoding Algorithm.} Decoding in MFT is non-trivial compared to standard subword tokenizers. Simple concatenation is insufficient due to the normalization of affixes. The \texttt{TurkishDecoder} module applies phonological rules to reconstruct the correct surface form. For vowel harmony, the decoder selects the appropriate vowel for a suffix based on the last vowel of the preceding unit (e.g., outputting \textit{-lar} after \textit{çocuk} but \textit{-ler} after \textit{ev}). For consonant assimilation, suffixes starting with dynamic consonants (e.g., $d/t$ in \textit{-da/-ta}) are adjusted based on whether the preceding sound is voiced or voiceless.

\textbf{Example.} \glossex{\texttt{<upper>} kitap \texttt{<space>} okuma-yı \texttt{<space>} sev-iyor-um .}
{CAPS book \_ reading-ACC \_ love-PROG-1SG .}
{I like reading books.}
{Kitap okumayı seviyorum.}

In this example, the abstract accusative suffix (represented as \textsc{ACC} in the vocabulary) is realized as \textit{-yı} after \textit{okuma} due to buffer consonant insertion rules (`y') and vowel harmony.

